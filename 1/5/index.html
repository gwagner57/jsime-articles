<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8" />
<meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
<title>JSimE 1/5 - Simulation Framework for Asynchronous Iterative
  Methods</title>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<!--Google Scholar-->
<meta name="citation_publisher"
  content="Consortium for True Open Access in Modeling and Simulation" />
<meta name="citation_journal_title" content="Journal of Simulation Engineering" />
<meta name="citation_journal_abbrev" content="JSimE" />
<meta name="citation_issn" content="2569-9466" />
<meta name="citation_volume" content="1" />
<meta name="citation_firstpage" content="5:1" />
<meta name="citation_lastpage" content="5:21" />
<meta name="citation_title"
  content="Simulation Framework for Asynchronous Iterative Methods" />
<meta name="citation_article_type" content="Article" />
<meta name="citation_online_date" content="2018/6/25" />
<meta name="citation_publication_date" content="2018/6/28" />
<meta name="citation_author" content="Evan Christopher Coleman" />
<meta name="citation_author_email" content="evanccoleman@gmail.com" />
<meta name="citation_author_institution"
  content="Old Dominion University, Norfolk, VA, United States" />
<meta name="citation_author" content="Erik Jensen" />
<meta name="citation_author_email" content="ejens005@odu.edu" />
<meta name="citation_author_institution"
  content="Old Dominion University, Norfolk, VA, United States" />
<meta name="citation_author" content="Masha Sosonkina" />
<meta name="citation_author_email" content="msosonki@odu.edu" />
<meta name="citation_author_institution"
  content="Old Dominion University, Norfolk, VA, United States" />
<meta name="citation_abstract_html_url"
  content="https://jsime.org/index.php/jsimeng/article/view/6" />
<meta name="citation_pdf_url"
  content="https://articles.jsime.org/1/jsime-article-1-5.pdf" />
<meta name="citation_fulltext_html_url"
  content="https://articles.jsime.org/1/5/Simulation-Framework-for-Asynchronous-Iterative-Methods" />
<!--Canonical URL-->
<link rel="canonical"
  href="https://articles.jsime.org/1/5/Simulation-Framework-for-Asynchronous-Iterative-Methods" />
<!--Stylesheets-->
<link rel="stylesheet"
  href="https://cdnjs.cloudflare.com/ajax/libs/normalize/7.0.0/normalize.min.css" />
<link rel="stylesheet" href="../../pseudocode.min.css" />
<link rel="stylesheet" href="../../jsime.css" />
<!-- Style -->
<style>
.small-caps {
  font-variant: small-caps;
}

.table-matrix td {
  border-style: hidden;
  text-align: center;
}

.table-figure caption {
  caption-side: bottom;
}

.table-figure td {
  border-style: hidden;
  text-align: center;
  caption-side: bottom;
  padding: 5px;
}

tr.border_top_bottom td {
  border-top: 1pt solid black;
  border-bottom: 1pt solid black;
}

td.border_left_right {
  border-left: 1pt solid black;
  border-right: 1pt solid black;
}
</style>
<!-- Global site tag (gtag.js) - Google Analytics -->
<script type="text/javascript" async="async" src="https://www.google-analytics.com/analytics.js"></script>
<script async="async" src="https://www.googletagmanager.com/gtag/js?id=UA-115543812-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag () {
    dataLayer.push( arguments );
  }
  gtag( 'js', new Date() );
  gtag( 'config', 'UA-115543812-1', {
    'anonymize_ip': true
  } );
</script>
<!--Scripts-->
<script
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=MML_CHTML"></script>
<!--schema.org JSON-LD-->
<script type="application/ld+json">
{
  "@context": {
    "@vocab": "http://schema.org",
    "@base": "https://articles.jsime.org"
  },
  "@graph": [
    {
      "@type": "ScholarlyArticle",
      "name": "Simulation Framework for Asynchronous Iterative Methods",
      "author": [
        { "@id": "/1/5/author-1" },
        { "@id": "/1/5/author-2" },
        { "@id": "/1/5/author-3" }
      ],
      "keywords": [ "Asynchronous Iterative Methods", "Fault Tolerance", "Asynchronous Simulation", "Shared Memory", "Intel Xeon Phi" ],
      "description": "As high-performance computing (HPC) platforms progress towards exascale, computational methods must be revamped to successfully leverage them. In particular, (1) asynchronous methods become of great importance because synchronization becomes prohibitively expensive and (2) resilience of computations must be achieved, e.g., using checkpointing selectively which may otherwise become prohibitively expensive due to the sheer scale of the computing environment. In this work, a simulation framework for asynchronous iterative methods is proposed and tested on HPC accelerator (shared-memory) architecture. The design proposed here offers a lightweight alternative to existing computational frameworks to allow for easy experimentation with various relaxation iterative techniques, solution updating schemes, and predicted performance. The simulation framework is implemented in MATLABÂ® using function handles which offers a modular, easily extensible design. An example of a case study using the simulation framework is presented to examine the efficacy of different checkpointing schemes for asynchronous relaxation methods.",
      "url": "https://articles.jsime.org/1/5/Simulation-Framework-for-Asynchronous-Iterative-Methods",
      "inLanguage": "en-US",
      "license": "https://creativecommons.org/licenses/by/4.0/",
      "copyrightHolder": [
        { "@id": "/1/5/author-1" },
        { "@id": "/1/5/author-2" },
        { "@id": "/1/5/author-3" }
      ],
      "copyrightYear": "2018",
      "dateCreated": "2018-06-25",
      "dateModified": "2018-08-24",
      "datePublished": "2018-06-28",
      "isPartOf": {
        "@type": "PublicationVolume",
        "datePublished": "2018-06-28",
        "volumeNumber": "1",
        "url": "https://articles.jsime.org/1/",
        "isPartOf": {
          "@type": "Periodical",
          "name": "Journal of Simulation Engineering",
          "issn": "2569-9466",
          "url": "https://jsime.org",
          "publisher": {
            "@type": "Organization",
            "name": "Consortium for True Open Access in Modeling and Simulation",
            "url": "https://articles.jsime.org/consortium-for-true-open-access"
          },
          "publishingPrinciples": "http://publicationethics.org/files/Code_of_conduct_for_journal_editors.pdf"
        }
      },
      "about": [
        {
          "@type": "CategoryCode",
          "identifier": "10010583.10010750.10010751",
          "codeValue": "Hardware~Fault tolerance",
          "inCodeSet": "http://totem.semedica.com/taxonomy/The%20ACM%20Computing%20Classification%20System%20(CCS)"
        },
        {
          "@type": "CategoryCode",
          "identifier": "10010147.10010148.10010149.10010158",
          "codeValue": "Computing methodologies~Linear algebra algorithms",
          "inCodeSet": "http://totem.semedica.com/taxonomy/The%20ACM%20Computing%20Classification%20System%20(CCS)"
        },
        {
          "@type": "CategoryCode",
          "identifier": "10010405.10010432.10010442",
          "codeValue": "Applied computing~Mathematics and statistics",
          "inCodeSet": "http://totem.semedica.com/taxonomy/The%20ACM%20Computing%20Classification%20System%20(CCS)"
        }
      ]
    },
    {
      "@id": "/1/5/author-1",
      "@type": "Person",
      "name": "Evan Christopher Coleman",
      "email": "evanccoleman@gmail.com",
      "affiliation": [
        { "@id": "/1/5/affiliation-1" },
        { "@id": "/1/5/affiliation-2" }
      ]
    },
    {
      "@id": "/1/5/author-2",
      "@type": "Person",
      "name": "Erik Jensen",
      "email": "ejens005@odu.edu",
      "affiliation": [
        { "@id": "/1/5/affiliation-2" }
      ]
    },
    {
      "@id": "/1/5/author-3",
      "@type": "Person",
      "name": "Masha Sosonkina",
      "email": "msosonki@odu.edu",
      "affiliation": [
        { "@id": "/1/5/affiliation-2" }
      ]
    },
    {
      "@id": "/1/5/affiliation-1",
      "@type": "Organization",
      "name": "Naval Surface Warfare Center",
      "department": "Dahlgren Division",
      "address": {
        "addressLocality": "Dahlgren",
        "addressRegion": "Virginia",
        "addressCountry": "United States"
      }
    },
    {
      "@id": "/1/5/affiliation-2",
      "@type": "Organization",
      "name": "Old Dominion University",
      "address": {
        "addressLocality": "Norfolk",
        "addressRegion": "Virginia",
        "addressCountry": "United States"
      }
    }
  ]
}
</script>
</head>
<body vocab="http://schema.org/">
  <header>
    <small><a href="https://jsime.org"><img src="../../JSimE.svg"
        style="height: 24px; margin-right: 0.7em;" /></a><i>Journal of Simulation
      Engineering</i>, Volume 1 (2018). Article URL: <a
      href="https://articles.jsime.org/1/5/Simulation-Framework-for-Asynchronous-Iterative-Methods"
      id="articleURL">https://articles.jsime.org/1/5</a><a id="PDF"
      class="suppressInPDF"
      href="https://jsime.org/index.php/jsimeng/article/view/6"
      style="position: relative; left: 1em"><img src="../../icons/pdf.png" /></a></small>
  </header>
  <main>
  <div id="front-matter">
    <div id="article-number">5</div>
    <h1 id="article-title">Simulation Framework for Asynchronous Iterative Methods</h1>
    <div id="article-title-abbr">Simulation Framework for Asynchronous Iterative Methods</div>
    <div id="authors">
      <address typeof="Person">
        <div property="name">
          Evan C. Coleman<sup>1,2<img src="../../icons/envelope.png"></sup>
        </div>
        <div property="email">
          <a href="mailto:evanccoleman@gmail.com">evanccoleman@gmail.com</a>
        </div>
      </address>
      <address typeof="Person">
        <div property="name">
          Erik Jensen<sup>2</sup>
        </div>
        <div property="email">
          <a href="mailto:ejens005@odu.edu">ejens005@odu.edu</a>
        </div>
      </address>
      <address style="display: block; margin: 0.5em auto" typeof="Person">
        <div property="name">
          Masha Sosonkina<sup>2</sup>
        </div>
        <div property="email">
          <a href="mailto:msosonki@odu.edu">msosonki@odu.edu</a>
        </div>
      </address>
      <div class="affiliation">
        <sup>1</sup> Naval Surface Warfare Center, Dahlgren Division, Dahlgren,
        VA, United States
      </div>
      <div class="affiliation">
        <sup>2</sup> Old Dominion University, Norfolk, VA, United States
      </div>
    </div>
    <div id="acm-subject-categories">
      <h1>ACM Subject Categories</h1>
      <ul>
        <li><code>Hardware~Fault tolerance</code></li>
        <li><code>Computing methodologies~Linear algebra algorithms</code></li>
        <li><code>Applied computing~Mathematics and statistics</code></li>
      </ul>
    </div>
    <div id="keywords">
      <h1>Keywords</h1>
      <ul class="list-inline comma-separated">
        <li>Asynchronous Iterative Methods</li>
        <li>Fault Tolerance</li>
        <li>Asynchronous Simulation</li>
        <li>Shared Memory</li>
        <li>Intel<sup>Â®</sup> Xeon Phi<sup>â¢</sup></li>
      </ul>
    </div>
  </div>

  <section id="sect0" role="doc-abstract">
    <h1>Abstract</h1>
    <p>
      As high-performance computing (HPC) platforms progress towards exascale,
      computational methods must be revamped to successfully leverage them. In
      particular, (1) asynchronous methods become of great importance because
      synchronization becomes prohibitively expensive and (2) resilience of
      computations must be achieved, e.g., using checkpointing selectively which
      may otherwise become prohibitively expensive due to the sheer scale of the
      computing environment. In this work, a simulation framework for
      asynchronous iterative methods is proposed and tested on HPC accelerator
      (shared-memory) architecture. The design proposed here offers a
      lightweight alternative to existing computational frameworks to allow for
      easy experimentation with various relaxation iterative techniques,
      solution updating schemes, and predicted performance. The simulation
      framework is implemented in MATLAB<sup>Â®</sup> using function handles,
      which offers a modular and easily extensible design. An example of a case
      study using the simulation framework is presented to examine the efficacy
      of different checkpointing schemes for asynchronous relaxation methods.
    </p>
  </section>

  <section id="sect1">
    <h1>Introduction</h1>
    <p>
      Asynchronous iterative methods are increasing in popularity recently due
      to their ability to be parallelized naturally on modern co-processors such
      as GPUs and Intel<sup>Â®</sup> Xeon Phi<sup>â¢</sup>. Many examples of
      recent work using fine-grained parallel methods are available (see <a
        role="doc-biblioref" href="#ADQO16">Anzt, Dongarra, &amp;
        Quintana-OrtÃ­, 2016</a>, <a role="doc-biblioref" href="#Anz12">Anzt,
        2012</a>, <a role="doc-biblioref" href="#CAD15">Chow, Anzt, &amp;
        Dongarra, 2015</a>, <a role="doc-biblioref" href="#CP15">Chow &amp;
        Patel, 2015</a>, <a role="doc-biblioref" href="#ACD15">Anzt, Chow, &amp;
        Dongarra, 2015</a> and many others in <a href="#sect2">Section 2</a>). A
      specific area of interest is on techniques that utilize fixed point
      iteration, i.e., those techniques that solve equations of the form
    </p>
    <div class="equation">
      <math display="block" xmlns="http://www.w3.org/1998/Math/MathML">
        <mtable>
          <mtr>
            <mtd>
              <mrow>
                <mi>x</mi>
                <mo>=</mo>
                <mi>G</mi>
                <mo maxsize="1">(</mo>
                <mi>x</mi>
                <mo maxsize="1">)</mo>
              </mrow>
            </mtd>
          </mtr>
        </mtable>
      </math>
    </div>
    <p>
      for some vector <span class="math-span">ð¥ â ð·</span> and some map <span
        class="math-span">ðº : ð· â ð·</span>. These techniques are well suited
      for fine-grained computation and they can be executed either synchronously
      or asynchronously, which helps tolerate latency in high-performance
      computing (HPC) environments. Looking forward to the future of HPC, it is
      important to prioritize the develop of algorithms that are resilient to
      faults since on future platforms, the rate at which faults occur is
      expected to increase dramatically (<a role="doc-biblioref" href="#CGG+09">Cappello
        et al., 2009</a>; <a role="doc-biblioref" href="#CGG+14">Cappello et
        al., 2014</a>; <a role="doc-biblioref" href="#ABC+06">AsanoviÄ et al.,
        2006</a>; <a role="doc-biblioref" href="#GL09">Geist &amp; Lucas, 2009</a>).
    </p>
    <p>
      While many asynchronous methods are designed for shared memory
      architectures and asynchronous iterative methods have gained popularity
      for their efficient use of resources on shared memory accelerators in
      modern HPC environments (<a role="doc-biblioref" href="#VV09">Venkatasubramanian
        &amp; Vuduc, 2009</a>), lately there has been some work done at improving
      the performance of asynchronous iterative methods in distributed memory
      environments. Such works include attempts to implement asynchronous
      iterative methods in MPI-3 using one sided remote memory access (<a
        role="doc-biblioref" href="#GBH14">Gerstenberger, Besta, &amp;
        Hoefler, 2014</a>) as well as efforts to reduce the cost of communication in
      these environments (<a role="doc-biblioref" href="#WPC16">Wolfson-Pou
        &amp; Chow, 2016</a>).
    </p>
    <p>Developing algorithms that are resilient to faults is of paramount
      importance, and fine-grained parallel fixed point methods are no
      exception. In this paper, we propose a simulation framework that can help
      developing algorithms resilient to faults. These types of frameworks allow
      for experimentation that is not specific to any singular platform or
      hardware architectures and allows experiments to simulate performance on
      both current computing environments and look at how those results may
      continue to evolve along with the computer hardware. Hence, they enable
      the possibility to: (1) test and validate different fault-models (which
      are still emerging), (2) experiment with different checkpointing
      libraries/mechanisms, and (3) help in efficiently implementing
      asynchronous iterative methods. Additionally, it can be difficult to
      implement asynchronous iterative methods on a variety of architectures to
      observe performance behavior in different computing environments, and
      having a working simulation framework allows users to conduct extensive
      experiments without any major programming investment.</p>
    <p>This study aims to develop a simulation framework that is focused on
      the performance of asynchronous iterative methods. The goal is to produce
      a lightweight computational framework capable of being used for various
      asynchronous iterative methods, with an emphasis on methods for solving
      linear systems, and simulating the performance of these methods on shared
      memory devices. The contributions of this work are</p>
    <ol>
      <li>the development, testing, and validation of a modular simulation
        framework for asynchronous iterative methods that can be used in the
        creation of new and improved algorithms,</li>
      <li>a process for the generation of time models from HPC
        implementation code, which may be used to initialize the framework,</li>
      <li>a case study on how to use the framework in the development of
        fault tolerant algorithms, and</li>
      <li>a comparison of several implementations of an asynchronous
        iterative relaxation method, used in the proposed framework.</li>
    </ol>
    <p>The simulation framework developed here is capable of predicting
      performance on various HPC system configurations and to show the
      performance of an algorithm subject to resiliency or reproducibility
      requirements.</p>
    <p>
      The rest of this paper is organized as follows. <a href="#sect2">Section
        2</a> provides a brief summary of related studies. <a href="#sect3">Section
        3</a> gives an overview of asynchronous iterative methods, while <a
        href="#sect4">Section 4</a> describes the design and utilization of the
      simulation framework in modeling the behavior of these methods. <a
        href="#sect5">Section 5</a> describes a process for collecting time data
      from HPC implementations and developing time models from the data for use
      in the simulation framework. A comparison of different implementations is
      given in <a href="#sect5.3">Section 5.3</a>, while framework validation is
      considered in <a href="#sect5.4">Section 5.4</a>. <a href="#sect6">Section
        6</a> gives background information related to the creation of efficient
      checkpointing routines and provides a series of numerical results. <a
        href="#sect7">Section 7</a> provides conclusions.
    </p>
  </section>

  <section id="sect2">
    <h1>Related Work</h1>
    <p>
      Development of computational frameworks for the purposes of simulating
      performance has a long history in the literature. Examples of such
      frameworks include SimGrid (<a role="doc-biblioref" href="#Cas01">Casanova,
        2001</a>; <a role="doc-biblioref" href="#CLQ08">Casanova, Legrand, &amp;
        Quinson, 2008</a>) that focuses on distributed experiments, GangSim (<a
        role="doc-biblioref" href="#DF05">Dumitrescu &amp; Foster, 2005</a>) and
      GridSim (<a role="doc-biblioref" href="#BM02">Buyya &amp; Murshed,
        2002</a>) that focus on grid scheduling, and CloudSim (<a
        role="doc-biblioref" href="#CRDRB09">Calheiros, Ranjan, De Rose,
        &amp; Buyya, 2009</a>; <a role="doc-biblioref" href="#CRB+11">Calheiros,
        Ranjan, Beloglazov, De Rose, &amp; Buyya, 2011</a>) that models performance
      of cloud computing environments. These environments focus on specific HPC
      implementation features, such as job scheduling and data movement, and on
      providing a view of how the systems themselves behave in HPC-like
      scenarios. On the other hand, the framework developed here is intended to
      simulate not only the HPC performance but also the algorithmic performance
      of a particular class of problems (e.g., iterative convergence to a linear
      system solution) under various system configurations (e.g., asynchronous
      thread behavior in shared-memory systems) and with various additional
      requirements (e.g., resiliency or reproducibility).
    </p>
    <p>
      The class of problems that the framework proposed in this study addresses
      are stationary solvers, also referred to as relaxation methods. The focus
      is on the behavior of these methods in asynchronous computing
      environments. However, the framework also easily admits synchronous
      updates; the key is the fine-grained nature of the algorithm. Fine-grained
      parallel methods, specifically parallel fixed point methods, are an area
      of increased research activity due to their practical use on HPC
      environments. An initial exploration of fault tolerance for stationary
      iterative linear solvers (i.e., Jacobi) is given in (<a
        role="doc-biblioref" href="#ADQO15">Anzt, Dongarra, &amp;
        Quintana-OrtÃ­, 2015</a>) and expanded on (<a role="doc-biblioref"
        href="#ADQO16">Anzt, Dongarra, &amp; Quintana-OrtÃ­, 2016</a>). The
      general convergence of parallel fixed point methods has been explored
      extensively (<a role="doc-biblioref" href="#AB05">Addou &amp;
        Benahmed, 2005</a>; <a role="doc-biblioref" href="#FS00">Frommer &amp;
        Szyld, 2000</a>; <a role="doc-biblioref" href="#BT89">Bertsekas &amp;
        Tsitsiklis, 1989</a>; <a role="doc-biblioref" href="#OR00">Ortega &amp;
        Rheinboldt, 2000</a>; <a role="doc-biblioref" href="#Bau78">Baudet, 1978</a>;
      <a role="doc-biblioref" href="#Ben07">Benahmed, 2007</a>).
    </p>
    <p>
      Examples of work examining the performance of asynchronous iterative
      methods include an in-depth analysis from the perspective of utilizing a
      system with a co-processor (<a role="doc-biblioref" href="#Anz12">Anzt,
        2012</a>; <a role="doc-biblioref" href="#ADG15">Avron, Druinsky, &amp;
        Gupta, 2015</a>), as well as performance analysis of asynchronous methods (<a
        role="doc-biblioref" href="#BBDH11">Bethune, Bull, Dingle, &amp;
        Higham, 2011</a>; <a role="doc-biblioref" href="#BBDH14">Bethune, Bull,
        Dingle, &amp; Higham, 2014</a>; <a role="doc-biblioref" href="#HD18">Hook,
        &amp; Dingle, 2018</a>). In particular, both (<a role="doc-biblioref"
        href="#BBDH11">Bethune, Bull, Dingle, &amp; Higham, 2011</a>) and (<a
        role="doc-biblioref" href="#BBDH14">Bethune, Bull, Dingle, &amp;
        Higham, 2014</a>) focus on low level analysis of the asynchronous Jacobi
      method, similar to the example problem that is featured here. While many
      recent research results for asynchronous iterative methods are focused on
      implementations that utilize a shared memory architecture, one area of
      asynchronous iterative methods that has seen significant development using
      a distributed memory architecture is optimization (<a role="doc-biblioref"
        href="#CC16">Cheung &amp; Cole, 2016</a>; <a role="doc-biblioref"
        href="#IBCH13">Iutzeler, Bianchi, Ciblat, &amp; Hachem, 2013</a>; <a
        role="doc-biblioref" href="#Hon17">Hong, 2017</a>; <a
        role="doc-biblioref" href="#ZC10">Zhong &amp; Cassandras, 2010</a>; <a
        role="doc-biblioref" href="#SN11">Srivastava &amp; Cassandras, 2011</a>;
      <a role="doc-biblioref" href="#TBA86">Tsitsiklis, Bertsekas, &amp;
        Athans, 1986</a>; <a role="doc-biblioref" href="#BPC+11">Boyd, Parikh,
        Chu, Peleato, &amp; Eckstein, 2011</a>).
    </p>
    <p>
      The use case of the simulation framework that is featured in <a
        href="#sect6">Section 6</a> shows the ability of the framework to be
      used in the development of fault tolerance techniques. The development of
      these techniques is important as HPC platforms continue to become both
      larger and more susceptible to faults. The expected increase in faults for
      future HPC systems is detailed in a variety of different sources. A high
      level article detailing the expected increase in failure rate from a
      reasonably non-technical point of view is available in the various
      versions of the "Monster in the Closet" talk and paper (<a
        role="doc-biblioref" href="#Gei11">Geist, 2011</a>; <a
        role="doc-biblioref" href="#Gei12">Geist, 2012</a>; <a
        role="doc-biblioref" href="#Gei16">Geist, 2016</a>). More technical and
      detailed reports are given in a variety of sources composed of groups of
      different researchers from both academia and industry (<a
        role="doc-biblioref" href="#ABC+06">AsanoviÄ et al., 2006</a>; <a
        role="doc-biblioref" href="#CGG+09">Cappello et al., 2009</a>; <a
        role="doc-biblioref" href="#CGG+14">Cappello et al., 2014</a>; <a
        role="doc-biblioref" href="#SWA+14">Snir et al., 2014</a>; <a
        role="doc-biblioref" href="#GL09">Geist &amp; Lucas, 2009</a>).
      Additionally, the Department of Energy has commissioned two very detailed
      reports about the progression towards exascale level computing; one from a
      general computing standpoint (<a role="doc-biblioref" href="#ABC+10">Ashby
        et al., 2010</a>), and a report aimed specifically at applied mathematics
      for exascale computing (<a role="doc-biblioref" href="#DHB+14">Dongarra
        et al., 2014</a>). Fault tolerance for fine-grained asynchronous iterative
      methods has been studied at a fundamental level (<a role="doc-biblioref"
        href="#GÃ¤r99">GÃ¤rtner, 1999</a>; <a role="doc-biblioref" href="#CS17">Coleman
        &amp; Sosonkina, 2017</a>), as well as made specific to certain algorithms (<a
        role="doc-biblioref" href="#CSC17">Coleman, Sosonkina, &amp; Chow,
        2017</a>; <a role="doc-biblioref" href="#CS18">Coleman, &amp; Sosonkina,
        2018</a>; <a role="doc-biblioref" href="#ADQO15">Anzt, Dongarra, &amp;
        Quintana-OrtÃ­, 2015</a>; <a role="doc-biblioref" href="#ADQO16">Anzt,
        Dongarra, &amp; Quintana-OrtÃ­, 2016</a>). Fault tolerance for synchronous
      fixed point algorithms from a numerical analysis has been investigated in
      (<a role="doc-biblioref" href="#SW15">Stoyanov &amp; Webster, 2015</a>).
      Error correction for GPU based oriented asynchronous methods were
      investigated in (<a role="doc-biblioref" href="#ALDH12">Anzt,
        Luszczek, Dongarra, &amp; Heuveline, 2012</a>).
    </p>
    <p>
      Several numerically based fault models similar to the one that is used in
      this study have been developed recently, and are used as a basis for the
      generalized fault simulation that is developed here. These include a
      "numerical" fault model that is predicated on shuffling the components of
      an important data structure (<a role="doc-biblioref" href="#EHM15">Elliott,
        Hoemmen, &amp; Mueller, 2015</a>), and a perturbation based model put forth
      in (<a role="doc-biblioref" href="#SW15">Stoyanov &amp; Webster, 2015</a>)
      and (<a role="doc-biblioref" href="#CS16b">Coleman &amp; Sosonkina,
        2016b</a>). Other models that are not based upon directly injecting a bit
      flip, such as inducing a small shift to a single component of a vector
      have been considered as well (<a role="doc-biblioref" href="#HH11">Hoemmen
        &amp; Heroux, 2011</a>; <a role="doc-biblioref" href="#BFHH12">Bridges,
        Ferreira, Heroux, &amp; Hoemmen, 2012</a>). Comparisons between various
      numerical soft fault models have been made in (<a role="doc-biblioref"
        href="#CS16a">Coleman &amp; Sosonkina, 2016a</a>; <a
        role="doc-biblioref" href="#CJB+18">Coleman, Jamal, Baboulin,
        Khabou, &amp; Sosonkina, 2018</a>).
    </p>
  </section>

  <section id="sect3">
    <h1>Asynchronous Iterative Methods</h1>
    <p>
      In fine-grained parallel computation, each component of the problem (i.e.,
      a matrix or vector entry) is updated in a manner that does not require
      information from the computations involving other components while the
      update is being made. This allows for each computing element (e.g., for a
      single processor, CUDA core, or Xeon Phi<sup>â¢</sup> core) to act
      independently from all other computing elements. Depending on the size of
      both the problem and the computing environment, each computing element may
      be responsible for updating a single entry to update, or may be assigned a
      block that contains multiple components. The generalized mathematical
      model that is used throughout this paper comes from (<a
        role="doc-biblioref" href="#FS00">Frommer &amp; Szyld, 2000</a>), which
      in turn comes from (<a role="doc-biblioref" href="#CM69">Chazan &amp;
        Miranker, 1969</a>; <a role="doc-biblioref" href="#Bau78">Baudet, 1978</a>;
      <a role="doc-biblioref" href="#Szy98">Szyld, 1998</a>) among many others.
    </p>
    <p>
      To keep the mathematical model as general as possible, consider a
      function, <span class="math-span">ðº : ð· â ð·</span>, where <span
        class="math-span">ð·</span> is a domain that represents a product space
      <span class="math-span">ð· = ð·<sub>1</sub> Ã ð·<sub>2</sub> Ã â¦ Ã
        ð·<sub>2</sub></span>. The goal is to find a fixed point of the function <span
        class="math-span">ðº</span> inside of the domain <span class="math-span">ð·</span>.
      To this end, a fixed point iteration is performed, such that
    </p>
    <div class="equation">
      <math display="block" xmlns="http://www.w3.org/1998/Math/MathML">
        <mtable>
          <mtr>
            <mtd>
              <mrow>
                <msup>
                  <mrow>
                    <mi>x</mi>
                  </mrow>
                  <mrow>
                    <mi>k</mi>
                    <mo>+</mo>
                    <mn>1</mn>
                  </mrow>
                </msup>
                <mo>=</mo>
                <mi>G</mi>
                <mo maxsize="1">(</mo>
                <msup>
                  <mrow>
                    <mi>x</mi>
                  </mrow>
                  <mrow>
                    <mi>k</mi>
                  </mrow>
                </msup>
                <mo maxsize="1">)</mo>
                <mtext maxsize="1" mathvariant="normal">,</mtext>
              </mrow>
            </mtd>
          </mtr>
        </mtable>
      </math>
    </div>
    <p>
      and a fixed point is declared if <span class="math-span">ð¥<sup>ð+1</sup>
        &asymp; ð¥<sup>ð</sup></span>. Note that the function <span class="math-span">ðº</span>
      has internal component functions <span class="math-span">ðº<sub>ð</sub></span>
      for each sub-domain, <span class="math-span">ð·<sub>ð</sub></span>, in
      the product space, <span class="math-span">ð·</span>. In particular, <span
        class="math-span">ðº<sub>ð</sub> : ð· â ð·<sub>ð</sub></span>, which
      gives that
    </p>
    <div class="equation">
      <math display="block" xmlns="http://www.w3.org/1998/Math/MathML">
        <mtable columnalign="right left" columnspacing="0.28em"
          displaystyle="true">
          <mtr>
            <mtd>
              <mi>x</mi>
            </mtd>
            <mtd>
              <mo>=</mo>
              <mfenced open="(" close=")">
              <msub>
              <mi>x</mi>
              <mn>1</mn>
              </msub>
              <msub>
              <mi>x</mi>
              <mn>2</mn>
              </msub>
              <mo>â¦</mo>
              <msub>
              <mi>x</mi>
              <mi>m</mi>
              </msub>
              </mfenced>
              <mo>â</mo>
              <mi>D</mi>
              <mo>â</mo>
            </mtd>
          </mtr>
          <mtr>
            <mtd>
              <mi>G</mi>
              <mo maxsize="1">(</mo>
              <mi>x</mi>
              <mo maxsize="1">)</mo>
            </mtd>
            <mtd>
              <mo>=</mo>
              <mi>G</mi>
              <mo maxsize="1">(</mo>
              <msub>
              <mi>x</mi>
              <mn>1</mn>
              </msub>
              <mo separator="true">,</mo>
              <msub>
              <mi>x</mi>
              <mn>2</mn>
              </msub>
              <mo separator="true">,</mo>
              <mo>â¦</mo>
              <mo separator="true">,</mo>
              <msub>
              <mi>x</mi>
              <mi>m</mi>
              </msub>
              <mo maxsize="1">)</mo>
            </mtd>
          </mtr>
          <mtr>
            <mtd>
            </mtd>
            <mtd>
              <mo>=</mo>
              <mfenced open="(" close=")">
              <mrow>
              <msub>
              <mi>G</mi>
              <mn>1</mn>
              </msub>
              <mo maxsize="1">(</mo>
              <mi>x</mi>
              <mo maxsize="1">)</mo>
              </mrow>
              <mrow>
              <msub>
              <mi>G</mi>
              <mn>2</mn>
              </msub>
              <mo maxsize="1">(</mo>
              <mi>x</mi>
              <mo maxsize="1">)</mo>
              </mrow>
              <mo>â¦</mo>
              <mrow>
              <msub>
              <mi>G</mi>
              <mi>m</mi>
              </msub>
              <mo maxsize="1">(</mo>
              <mi>x</mi>
              <mo maxsize="1">)</mo>
              </mrow>
              </mfenced>
              <mo>â</mo>
              <mi>D</mi>
              <mo separator="true">.</mo>
            </mtd>
          </mtr>
        </mtable>
      </math>
    </div>
    <p>
      As a concrete example, let each <span class="math-span">ð·<sub>ð</sub>
        = â
      </span>. Forming the product space of each of these <span class="math-span">ð·<sub>ð</sub></span>'s
      gives that <span class="math-span">ð· = â<sup>ð</sup>
      </span>. This leads to the more formal functional mapping, <span
        class="math-span">ð : â<sup>ð</sup> â â<sup>ð</sup>
      </span>. Additionally, let
      <math xmlns="http://www.w3.org/1998/Math/MathML">
        <mi>f</mi>
        <mo maxsize="1">(</mo>
      <mover accent="true">
      <mi>x</mi>
      <mo style="font-size: small;">â</mo>
      </mover>
      <mo maxsize="1">)</mo>
      <mo>=</mo>
      <mn>2</mn>
      <mover accent="true">
      <mi>x</mi>
      <mo style="font-size: small;">â</mo>
      </mover>
    </math>
      . In this case, each of the individual <span class="math-span">ð<sub>ð</sub></span>
      component functions is defined by
      <math xmlns="http://www.w3.org/1998/Math/MathML">
        <mi>f</mi>
        <mo maxsize="1">(</mo>
      <mover accent="true">
      <mi>x</mi>
      <mo style="font-size: small;">â</mo>
      </mover>
      <mo maxsize="1">)</mo>
      <mo>=</mo>
      <mn>2</mn>
      <msub>
      <mi>x</mi>
      <mi>i</mi>
      </msub>
    </math>
      . Note that each component functions operates on all of the vector
      <math xmlns="http://www.w3.org/1998/Math/MathML">
      <mover accent="true">
      <mi>x</mi>
      <mo style="font-size: small;">â</mo>
      </mover>
      </math>
      even if the individual function definition does not require all of the
      components of
      <math xmlns="http://www.w3.org/1998/Math/MathML">
      <mover accent="true">
      <mi>x</mi>
      <mo style="font-size: small;">â</mo>
      </mover>
      </math>
      to perform its specific update.
    </p>
    <p>
      The assumption is also made that there is some finite number of processing
      elements <span class="math-span">ð<sub>1</sub>, ð<sub>2</sub>, â¦,
        ð<sub>ð</sub></span> each of which is assigned to a block <span
        class="math-span">ðµ</span> of components <span class="math-span">ðµ<sub>1</sub>,
        ðµ<sub>2</sub>, â¦, ðµ<sub>ð</sub></span> to update. Note that the number <span
        class="math-span">ð</span> of processing elements will typically be
      significantly smaller than the number <span class="math-span">ð</span> of
      blocks to update. With these assumptions, the computational model can be
      stated in <a href="#alg1">Algorithm 1</a>.
    </p>
    <div id="alg1" class="algorithm column-top">
      <div class="ps-root">
        <div class="ps-algorithm with-caption">
          <p class="ps-line" style="text-indent: -1.2em; padding-left: 1.2em;">
            <span class="ps-keyword">Algorithm 1 </span>General computational
            model
          </p>
          <div class="ps-algorithmic with-linenum">
            <div class="ps-block" style="margin-left: 1.7999999999999998em;">
              <p class="ps-line ps-code">
                <span class="ps-linenum" style="left: 0em;">1:</span><span
                  class="ps-keyword">for </span>each processing element
                <math xmlns="http://www.w3.org/1998/Math/MathML">
                    <msub>
                    <mi>P</mi>
                    <mi>l</mi></msub>
                  </math>
                <span class="ps-keyword"> do</span>
              </p>
              <div class="ps-block" style="margin-left: 1.2em;">
                <p class="ps-line ps-code">
                  <span class="ps-linenum" style="left: -1.5em;">2:</span><span
                    class="ps-keyword">for </span>
                  <math xmlns="http://www.w3.org/1998/Math/MathML">
                    <mi>i</mi>
                    <mo>=</mo>
                    <mn>1</mn>
                    <mo separator="true">,</mo>
                    <mn>2</mn>
                    <mo separator="true">,</mo>
                    <mo>â¦</mo>
                  </math>
                  until convergence<span class="ps-keyword"> do</span>
                </p>
                <div class="ps-block" style="margin-left: 1.2em;">
                  <p class="ps-line ps-code">
                    <span class="ps-linenum" style="left: -3em;">3:</span>Read
                    <math xmlns="http://www.w3.org/1998/Math/MathML">
                      <mi>x</mi>
                    </math>
                    from common memory
                  </p>
                  <p class="ps-line ps-code">
                    <span class="ps-linenum" style="left: -3em;">4:</span>Compute
                    <math xmlns="http://www.w3.org/1998/Math/MathML">
                      <msubsup>
                      <mi>x</mi>
                      <mi>j</mi>
                      <mrow>
                      <mi>i</mi>
                      <mo>+</mo>
                      <mn>1</mn>
                      </mrow>
                      </msubsup>
                      <mo>=</mo>
                      <msub>
                      <mi>G</mi>
                      <mi>j</mi>
                      </msub>
                      <mo maxsize="1">(</mo>
                      <mi>x</mi>
                      <mo maxsize="1">)</mo>
                    </math>
                    for all
                    <math xmlns="http://www.w3.org/1998/Math/MathML">
                      <mi>j</mi>
                      <mo>â</mo>
                      <msub>
                      <mi>ðµ</mi>
                      <mi>l</mi>
                      </msub>
                    </math>
                  </p>
                  <p class="ps-line ps-code">
                    <span class="ps-linenum" style="left: -3em;">5:</span>Update
                    <math xmlns="http://www.w3.org/1998/Math/MathML">
                      <msub>
                      <mi>x</mi>
                      <mi>j</mi></msub>
                    </math>
                    in common memory with
                    <math xmlns="http://www.w3.org/1998/Math/MathML">
                      <msubsup>
                      <mi>x</mi>
                      <mi>j</mi>
                      <mrow>
                      <mi>i</mi>
                      <mo>+</mo>
                      <mn>1</mn>
                      </mrow>
                      </msubsup>
                    </math>
                    for all
                    <math xmlns="http://www.w3.org/1998/Math/MathML">
                      <mi>j</mi>
                      <mo>â</mo>
                      <msub>
                      <mi>ðµ</mi>
                      <mi>l</mi>
                      </msub>
                    </math>
                  </p>
                </div>
                <p class="ps-line ps-code">
                  <span class="ps-linenum" style="left: -1.5em;">6:</span><span
                    class="ps-keyword">end for</span>
                </p>
              </div>
              <p class="ps-line ps-code">
                <span class="ps-linenum" style="left: 0em;">7:</span><span
                  class="ps-keyword">end for</span>
              </p>
            </div>
          </div>
        </div>
      </div>
    </div>
    <p>
      This computational model has each processing element read all pertinent
      data from global memory that is accessible by each of the processors,
      update the pieces of data specific to the component functions that it has
      been assigned, and update those components in the global memory. Note that
      the computational model presented in <a href="#alg1">Algorithm 1</a>
      allows for either synchronous or asynchronous computation; it only
      prescribes that an update has to be made as an "atomic" operation (in line
      5), i.e., without interleaving of its result. If each processing element <span
        class="math-span">ð<sub>ð</sub></span> is to wait for the other
      processors to finish each update, then the model describes a parallel
      synchronous form of computation. On the other hand, if no order is
      established for <span class="math-span">ð<sub>ð</sub></span>s, then an
      asynchronous form of computation arises.
    </p>
    <p>
      To continue formalizing this computational model a few more definitions
      are necessary. First, set a global iteration counter <span
        class="math-span">ð</span> that increases <em>every</em> time any
      processor reads
      <math xmlns="http://www.w3.org/1998/Math/MathML">
      <mover accent="true">
      <mi>x</mi>
      <mo style="font-size: small;">â</mo>
      </mover>
      </math>
      from common memory. At the end of the work done by any individual
      processor, <span class="math-span">ð</span>, the components associated
      with the block <span class="math-span">ðµ<sub>ð</sub></span> will be
      updated. This results in a vector,
      <math xmlns="http://www.w3.org/1998/Math/MathML">
      <mover accent="true">
      <mi>x</mi>
      <mo style="font-size: small;">â</mo>
      </mover>
      <mo>=</mo>
      <mfenced open="(" close=")">
      <msubsup>
      <mi>x</mi>
      <mn>1</mn>
      <mrow>
      <msub>
      <mi>s</mi>
      <mn>1</mn>
      </msub>
      <mo maxsize="1">(</mo>
      <mo>x</mo>
      <mo maxsize="1">)</mo>
      </mrow>
      </msubsup>
      <msubsup>
      <mi>x</mi>
      <mn>2</mn>
      <mrow>
      <msub>
      <mi>s</mi>
      <mn>2</mn>
      </msub>
      <mo maxsize="1">(</mo>
      <mo>x</mo>
      <mo maxsize="1">)</mo>
      </mrow>
      </msubsup>
      <mo>â¦</mo>
      <msubsup>
      <mi>x</mi>
      <mi>m</mi>
      <mrow>
      <msub>
      <mi>s</mi>
      <mi>m</mi>
      </msub>
      <mo maxsize="1">(</mo>
      <mo>x</mo>
      <mo maxsize="1">)</mo>
      </mrow>
      </msubsup>
      </mfenced>
      </math>
      where the function <span class="math-span">ð <sub>ð</sub>(ð)
      </span> indicates how many times an specific component has been updated. Finally,
      a set of individual components can be grouped into a set, <span
        class="math-span">ð¼<sup>ð</sup>
      </span> , that contains all of the components that were updated on the <span
        class="math-span">ð<sup>th</sup></span> iteration. Given these basic
      definitions, the three following conditions (along with the model
      presented in <a href="#alg1">Algorithm 1</a>) provide a working
      mathematical framework for fine-grained asynchronous computation.
    </p>
    <div id="def1" class="definition">
      <p>
        <strong>Definition 1.</strong> If the following three conditions hold
      </p>
      <ol>
        <li><math xmlns="http://www.w3.org/1998/Math/MathML">
      <msub>
      <mi>s</mi>
      <mi>i</mi>
      </msub>
      <mo maxsize="1">(</mo>
      <mi>k</mi>
      <mo maxsize="1">)</mo>
      <mo>â¤</mo>
      <mi>k</mi>
      <mo>â</mo>
      <mn>1</mn>
      </math>, i.e., only components that have finished computing are used in the
          current approximation.</li>
        <li><math xmlns="http://www.w3.org/1998/Math/MathML">
      <msub>
      <mtext>lim</mtext>
      <mrow>
      <mi>k</mi>
      <mo>â</mo>
      <mn>â</mn>
      </mrow>
      </msub>
      <msub>
      <mi>s</mi>
      <mi>i</mi>
      </msub>
      <mo maxsize="1">(</mo>
      <mi>k</mi>
      <mo maxsize="1">)</mo>
      <mo>=</mo>
      <mn>â</mn>
      </math>, i.e., the newest updates for each component are used.</li>
        <li><math xmlns="http://www.w3.org/1998/Math/MathML">
      <mfenced open="|" close="|">
      <mrow>
      <mi>k</mi>
      <mo>â</mo>
      <mo>â</mo>
      <mo>:</mo>
      <mi>i</mi>
      <mo>â</mo>
      <msup>
      <mi>I</mi>
      <mi>k</mi>
      </msup>
      </mrow>
      </mfenced>
      <mo>=</mo>
      <mn>â</mn>
      </math>, i.e., all components will continue to be updated.</li>
      </ol>
      <p>
        Then given an initial
        <math xmlns="http://www.w3.org/1998/Math/MathML">
          <msup>
          <mover accent="true">
          <mi>x</mi>
          <mo style="font-size: small;">â</mo>
          </mover>
          <mn>0</mn>
          </msup>
          <mo>â</mo>
          <mi>D</mi>
          </math>
        , the iterative update process defined by
      </p>
      <math display="block" xmlns="http://www.w3.org/1998/Math/MathML">
          <msubsup>
          <mi>x</mi>
          <mi>i</mi>
          <mi>k</mi>
          </msubsup>
          <mo>=</mo>
          <mrow>
          <mo>{</mo>
          <mtable>
          <mtr>
            <mtd>
            <msubsup>
            <mi>x</mi>
            <mi>i</mi>
            <mrow>
            <mi>k</mi>
            <mo>â</mo>
            <mn>1</mn>
            </mrow>
            </msubsup>
            </mtd>
            <mtd>
            <mi>i</mi>
            <mo>â</mo>
            <msub>
            <mi>I</mi>
            <mi>k</mi>
            </msub>
            </mtd>
          </mtr>
          <mtr>
            <mtd>
            <msub>
            <mi>G</mi>
            <mi>i</mi>
            </msub>
            <mo maxsize="1">(</mo>
            <mover accent="true">
            <mi>x</mi>
            <mo style="font-size: small;">â</mo>
            </mover>
            <mo maxsize="1">)</mo>
            </mtd>
            <mtd>
            <mi>i</mi>
            <mo>â</mo>
            <msub>
            <mi>I</mi>
            <mi>k</mi>
            </msub>
            </mtd>
          </mtr>
          </mtable>
          </mrow>
        </math>
      <p>
        where the function
        <math xmlns="http://www.w3.org/1998/Math/MathML">
      <msub>
      <mi>G</mi>
      <mi>i</mi>
      </msub>
      <mo maxsize="1">(</mo>
      <mover accent="true">
      <mi>x</mi>
      <mo style="font-size: small;">â</mo>
      </mover>
      <mo maxsize="1">)</mo>
      </math>
        uses the latest updates available is called an asynchronous iteration.
      </p>
    </div>
    <p>
      This basic computational model (i.e., the combination of <a href="#alg1">Algorithm
        1</a> and <a href="#def1">Definition 1</a> together) allows for many
      different results on fine-grained iterative methods that are both
      synchronous and asynchronous, though the three conditions given in <a
        href="#def1">Definition 1</a> are unnecessary in the synchronous case.
    </p>

    <section id="sect3.1">
      <h1>Asynchronous Relaxation Methods</h1>

      <p>
        Relaxation methods have been the focus of many of the works mentioned in
        <a href="#sect2">Section 2</a> such as (<a role="doc-biblioref"
          href="#CM69">Chazan &amp; Miranker, 1969</a>) and (<a
          role="doc-biblioref" href="#Bau78">Baudet, 1978</a>); a much more
        detailed description can be found in (<a role="doc-biblioref"
          href="#BT89">Bertsekas &amp; Tsitsiklis, 1989</a>) among many other
        sources. This section provides an introduction that will serve as a
        reference for the later work in this study.
      </p>
      <p>Relaxation methods can be expressed as general fixed point
        iterations of the form</p>
      <div id="eq1" class="equation">
        <math display="block" xmlns="http://www.w3.org/1998/Math/MathML">
            <mtable>
              <mlabeledtr>
                <mtd>
                  <mtext>(1)</mtext>
                </mtd>
                <mtd>
                  <mrow>
                  <msup>
                  <mi>x</mi>
                  <mrow>
                  <mi>k</mi>
                  <mo>+</mo>
                  <mn>1</mn>
                  </mrow>
                  </msup>
                  <mo>=</mo>
                  <mi>C</mi>
                  <msup>
                  <mi>x</mi>
                  <mi>k</mi>
                  </msup>
                  <mo>+</mo>
                  <mi>d</mi>
                  <mo separator="true">,</mo>
                  </mrow>
                </mtd>
              </mlabeledtr>
            </mtable>
          </math>
      </div>
      <p>
        where <span class="math-span">ð¶</span> is the <span class="math-span">ð
          Ã ð</span> iteration matrix, <span class="math-span">ð¥</span> is an <span
          class="math-span">ð¥-dimensional</span> vector that represents the
        solution, and <span class="math-span">ð</span> is another <span
          class="math-span">ð¥-dimensional</span> vector that can be used to
        help define the particular problem at hand.
      </p>
      <p>The Jacobi method is an asynchronous relaxation method built for
        solving linear systems of the form</p>
      <div class="equation">
        <math display="block" xmlns="http://www.w3.org/1998/Math/MathML">
            <mtable>
              <mtr>
                <mtd>
                  <mrow>
                  <mi>A</mi>
                  <mi>x</mi>
                  <mo>=</mo>
                  <mi>b</mi>
                  </mrow>
                </mtd>
              </mtr>
            </mtable>
          </math>
      </div>
      <p>
        and following the methodology put forth in (<a role="doc-biblioref"
          href="#BT89">Bertsekas &amp; Tsitsiklis, 1989</a>), this can be broken
        down to view a specific row â say the <span class="math-span">ð<sup>th</sup></span>
        â of the matrix <span class="math-span">ð´</span>,
      </p>
      <div class="equation">
        <math display="block" xmlns="http://www.w3.org/1998/Math/MathML">
          <mtable>
            <mtr>
              <mtd>
                <mrow>
                <mstyle displaystyle="true">
                <munderover>
                <mo>â</mo>
                <mrow>
                <mi>j</mi>
                <mo>=</mo>
                <mn>1</mn>
                </mrow>
                <mi>n</mi>
                </munderover>
                </mstyle>
                <msub>
                <mi>a</mi>
                <mrow>
                <mi>i</mi>
                <mi>j</mi>
                </mrow>
                </msub>
                <msub>
                <mi>x</mi>
                <mi>j</mi>
                </msub>
                <mo>=</mo>
                <msub>
                <mi>b</mi>
                <mi>i</mi>
                </msub>
                </mrow>
              </mtd>
            </mtr>
          </mtable>
        </math>
      </div>
      <p>
        and this equation can be solved for the <span class="math-span">ð<sup>th</sup></span>
        component of the solution, <span class="math-span">ð¥<sub>ð</sub></span>,
        to give,
      </p>
      <div id="eq2" class="equation">
        <math display="block" xmlns="http://www.w3.org/1998/Math/MathML">
          <mtable>
            <mlabeledtr>
              <mtd>
                <mtext>(2)</mtext>
              </mtd>
              <mtd columnalign="center">
                <mrow>
                <msub>
                <mi>x</mi>
                <mi>i</mi>
                </msub>
                <mo>=</mo>
                <mfrac>
                <mn>â1</mn>
                <mrow>
                <msub>
                <mi>a</mi>
                <mrow>
                <mi>i</mi>
                <mi>i</mi>
                </mrow>
                </msub>
                </mrow>
                </mfrac>
                <mfenced open="[" close="]">
                <mrow>
                <msub>
                <mo>â</mo>
                <mrow>
                <mi>j</mi>
                <mo>â </mo>
                <mi>i</mi>
                </mrow>
                </msub>
                <msub>
                <mi>a</mi>
                <mrow>
                <mi>i</mi>
                <mi>j</mi>
                </mrow>
                </msub>
                <msub>
                <mi>x</mi>
                <mi>j</mi>
                </msub>
                <mo>â</mo>
                <msub>
                <mi>b</mi>
                <mi>i</mi>
                </msub>
                </mrow>
                </mfenced>
                <mtext maxsize="1" mathvariant="normal">.</mtext>
                </mrow>
              </mtd>
            </mlabeledtr>
          </mtable>
        </math>
      </div>
      <p>
        This equation can then be computed in an iterative manner in order to
        give successive updates to the solution vector. In synchronous computing
        environments, each update to an element of the solution vector, <span
          class="math-span">ð¥<sub>ð</sub></span> is computed sequentially
        using the same data for the other components of the solution vector
        (i.e., the <span class="math-span">ð¥<sub>ð</sub></span>, in <a
          href="#eq2">Equation (2)</a>. Conversely, in an asynchronous computing
        environment, each update to an element of the solution vector occurs
        when the computing element responsible for updating that component is
        ready to write the update to memory and the other components used are
        simply the latest ones available to the computing element.
      </p>
      <p>
        Expressing <a href="#eq2">Equation (2)</a> in a block matrix form more
        similar to the original form of the iteration expressed in <a
          href="#eq1">Equation (1)</a>,
      </p>
      <div class="equation">
        <math display="block" xmlns="http://www.w3.org/1998/Math/MathML">
          <mtable columnalign="right left" columnspacing="0.28em"
            displaystyle="true">
            <mtr>
              <mtd>
                <mi>x</mi>
              </mtd>
              <mtd>
                <mo>=</mo>
                <mo>â</mo>
                <msup>
                <mi>D</mi>
                <mn>â1</mn>
                </msup>
                <mfenced open="(" close=")">
                <mrow>
                <mfenced open="(" close=")">
                <mrow>
                <mi>L</mi>
                <mo>+</mo>
                <mi>U</mi>
                </mrow>
                </mfenced>
                <mi>x</mi>
                <mo>â</mo>
                <mi>b</mi>
                </mrow>
                </mfenced>
              </mtd>
            </mtr>
            <mtr>
              <mtd>
                <mi>x</mi>
              </mtd>
              <mtd>
                <mo>=</mo>
                <mo>â</mo>
                <msup>
                <mi>D</mi>
                <mn>â1</mn>
                </msup>
                <mfenced open="(" close=")">
                <mrow>
                <mi>L</mi>
                <mo>+</mo>
                <mi>U</mi>
                </mrow>
                </mfenced>
                <mi>x</mi>
                <mo>+</mo>
                <msup>
                <mi>D</mi>
                <mn>â1</mn>
                </msup>
                <mi>b</mi>
                <mo separator="true">.</mo>
              </mtd>
            </mtr>
          </mtable>
        </math>
      </div>
      <p>
        where <span class="math-span">ð·</span> is the diagonal portion of <span
          class="math-span">ð´</span>, and <span class="math-span">ð¿</span> and
        <span class="math-span">ð</span> are the strictly lower and upper
        triangular portions of <span class="math-span">ð´</span> respectively.
        This gives an iteration matrix of <span class="math-span">ð¶ =
          -ð·<sup>â1</sup>(ð¿ + ð)
        </span>.
      </p>
      <p>
        Convergence of asynchronous fixed point methods of the form presented in
        <a href="#eq1">Equation (1)</a> is determined by the spectral radius of
        the iteration matrix, <span class="math-span">ð¶</span>, and dates back
        to the pioneering work done by both (<a role="doc-biblioref"
          href="#CM69">Chazan &amp; Miranker, 1969</a>) and (<a
          role="doc-biblioref" href="#Bau78">Baudet, 1978</a>):
      </p>
      <p id="theo1" class="theorem">
        <strong>Theorem 1.</strong> <em>For a fixed point iteration of the
          form given in <a href="#eq1">Equation (1)</a> that adheres to the
          asynchronous computational model provided by <a href="#alg1">Algorithm
            1</a> and <a href="#def1">Definition 1</a>, if the spectral radius of <span
          class="math-span">ð¶</span>, <span class="math-span">Ï(|ð¶|)</span>,
          is less than one, then the iterative method will converge to the fixed
          point solution.
        </em>
      </p>
      <p>
        As noted in (<a role="doc-biblioref" href="#WPC16">Wolfson-Pou &amp;
          Chow, 2016</a>), the iteration matrix <span class="math-span">ð¶</span>
        that is used in the Jacobi relaxation method serves as a worst case for
        relaxation methods of the form discussed here. However, because of the
        ubiquitous use of the Jacobi method in parallel solutions of large
        problems in many different domains in science and engineering we use the
        Asynchronous (Block) Jacobi method predominantly throughout the
        remainder of this study. Note that many of the concepts and ideas
        expressed in this paper can be easily adapted to more complex
        algorithms.
      </p>
    </section>
  </section>

  <section id="sect4">
    <h1>Design of Simulation Framework</h1>
    <p>
      The simulation framework proposed here is designed to simulate the
      performance of an asynchronous iterative method operating on multiple
      computing elements using a single processing element. In this simulation
      framework, the emphasis is on fixed-point iterations<span class="fn"><sup><a
          href="#ftn1" id="fn_pointer_ftn1" role="doc-noteref"
          title="Footnote 1: Throughout the text, vector notation is occasionally adopted to emphasize when functions take all components of x as opposed to a single component, such as x1.">1</a></sup></span>
    </p>
    <div class="equation">
      <math display="block" xmlns="http://www.w3.org/1998/Math/MathML">
        <mtable>
          <mtr>
            <mtd>
              <mrow>
              <mover accent="true">
              <mi>x</mi>
              <mo style="font-size: small;">â</mo>
              </mover>
              <mo>=</mo>
              <mi>G</mi>
              <mo maxsize="1">(</mo>
              <mrow>
              <mover accent="true">
              <mi>x</mi>
              <mo style="font-size: small;">â</mo>
              </mover>
              </mrow>
              <mo maxsize="1">)</mo>
              </mrow>
            </mtd>
          </mtr>
        </mtable>
      </math>
    </div>
    <p>
      for some
      <math xmlns="http://www.w3.org/1998/Math/MathML">
      <mover accent="true">
      <mi>x</mi>
      <mo style="font-size: small;">â</mo>
      </mover>
      <mo>â</mo>
      <msup>
      <mi>â</mi>
      <mi>n</mi>
      </msup>
      </math>
      . In the framework, certain components are assigned (possibly distinct)
      times for performing an update to their components, and the effects of
      various delay structures can be examined.
    </p>
    <p>
      The development of the present computational framework is shown by the
      flow diagram in <a href="#fig1">Figure 1</a>, which is typical for
      computation frameworks, except for the third Timing Distributions stage. A
      mathematical formulation of a problem (e.g., as a set of equations) is
      presented first (Mathematical Model stage). The mathematical model is then
      implemented in an HPC environment (Parallel Implementation stage). Timing
      and algorithm-performance data (e.g., iterations to convergence) are
      collected from parallel executions on a subset of configurations and
      problem sizes, such that, in the proposed framework, timing distributions
      may be constructed (Timing Distributions stage) and used to simulate the
      performance of the mathematical model for target configurations and
      requirements. Since such simulations are faster and less-cumbersome to
      set-up, they allow for easy experimenting with variations of the
      underlying mathematical model, parallel implementation type and
      environment, or, eventually, in the expected performance.
    </p>
    <figure id="fig1" class="top">
      <img src="./img/image1.png" />
      <figcaption>
        <strong>Figure 1.</strong> Stages in the proposed framework development.
      </figcaption>
    </figure>
    <p>The simulation framework developed here works to simulate the
      performance of generic asynchronous relaxation methods in shared memory
      environments. The simulation framework can then be modified to reflect
      changes in the environment, or else can be utilized to demonstrate the
      effectiveness of algorithmic modifications.</p>
    <p>
      As a simple example, take <span class="math-span">ð = 2</span>. Then
      <math xmlns="http://www.w3.org/1998/Math/MathML">
      <mover accent="true">
      <mi>x</mi>
      <mo style="font-size: small;">â</mo>
      </mover>
      <mo>=</mo>
      <mfenced open="(" close=")">
      <msub>
      <mo>x</mo>
      <mn>1</mn>
      </msub>
      <msub>
      <mo>x</mo>
      <mn>2</mn>
      </msub>
      </mfenced>
      <mo>â</mo>
      <msup>
      <mi>â</mi>
      <mn>2</mn>
      </msup>
      </math>
      and, using the terminology of <a href="#sect3">Section 3</a>,
    </p>
    <div class="equation">
      <math display="block" xmlns="http://www.w3.org/1998/Math/MathML">
        <mtable columnalign="right left" columnspacing="0.28em"
          displaystyle="true">
          <mtr>
            <mtd>
              <msub>
              <mi>x</mi>
              <mn>1</mn>
              </msub>
            </mtd>
            <mtd>
              <mo>=</mo>
              <msub>
              <mi>G</mi>
              <mn>1</mn>
              </msub>
              <mo maxsize="1">(</mo>
              <mover accent="true">
              <mi>x</mi>
              <mo style="font-size: small;">â</mo>
              </mover>
              <mo maxsize="1">)</mo>
              <mo>=</mo>
              <msub>
              <mi>G</mi>
              <mn>1</mn>
              </msub>
              <mo maxsize="1">(</mo>
              <msub>
              <mi>x</mi>
              <mn>1</mn>
              </msub>
              <mo separator="true">,</mo>
              <msub>
              <mi>x</mi>
              <mn>2</mn>
              </msub>
              <mo maxsize="1">)</mo>
              <mo separator="true">,</mo>
            </mtd>
          </mtr>
          <mtr>
            <mtd>
              <msub>
              <mi>x</mi>
              <mn>2</mn>
              </msub>
            </mtd>
            <mtd>
              <mo>=</mo>
              <msub>
              <mi>G</mi>
              <mn>2</mn>
              </msub>
              <mo maxsize="1">(</mo>
              <mover accent="true">
              <mi>x</mi>
              <mo style="font-size: small;">â</mo>
              </mover>
              <mo maxsize="1">)</mo>
              <mo>=</mo>
              <msub>
              <mi>G</mi>
              <mn>2</mn>
              </msub>
              <mo maxsize="1">(</mo>
              <msub>
              <mi>x</mi>
              <mn>1</mn>
              </msub>
              <mo separator="true">,</mo>
              <msub>
              <mi>x</mi>
              <mn>2</mn>
              </msub>
              <mo maxsize="1">)</mo>
              <mtext maxsize="1" mathvariant="normal">.</mtext>
            </mtd>
          </mtr>
        </mtable>
      </math>
    </div>
    <p>
      In a traditional fully synchronous environment, both functions, <span
        class="math-span">ðº<sub>1</sub></span> and <span class="math-span">ðº<sub>2</sub></span>,
      would be called simultaneously and no subsequent calls would be executed
      until both functions had returned and <em>synchronized</em> all results.
      In a fully asynchronous environment, both functions would be allowed to
      execute again immediately upon their own return, leading to a case where
      one of <span class="math-span">ð¥<sub>1</sub></span> or <span
        class="math-span">ð¥<sub>2</sub></span> may be updated more frequently
      than the other. Per <a href="#def1">Definition 1</a>, both functions use
      the latest values of
      <math xmlns="http://www.w3.org/1998/Math/MathML">
      <mover accent="true">
      <mi>x</mi>
      <mo style="font-size: small;">â</mo>
      </mover>
    </math>
      that are available to them when the function call is initiated. For
      instance, if the processing element that was assigned to update the
      component <span class="math-span">ð¥<sub>1</sub></span> was ten times as
      fast as the processing element assigned to update <span class="math-span">ð¥<sub>2</sub></span>,
      then in the amount of time needed to update <span class="math-span">ð¥<sub>2</sub></span>
      once, the component <span class="math-span">ð¥<sub>1</sub></span> will
      have been updated ten times, and when <span class="math-span">ðº<sub>2</sub></span>
      is called for the second time it will be called using the latest component
      of <span class="math-span">ð¥<sub>1</sub></span> (which has been updated
      10 times), and the latest component of <span class="math-span">ð¥<sub>2</sub></span>
      (which has only been updated once).
    </p>
    <p>
      A block diagram showing the flow of the simulation framework is provided
      in <a href="#fig2">Figure 2</a>. The framework models the performance of
      methods that solve the linear system
    </p>
    <div class="equation">
      <math display="block" xmlns="http://www.w3.org/1998/Math/MathML">
        <mtable>
          <mtr>
            <mtd>
              <mrow>
              <mi>A</mi>
              <mi>x</mi>
              <mo>=</mo>
              <mi>b</mi>
              </mrow>
            </mtd>
          </mtr>
        </mtable>
      </math>
    </div>
    <p>using relaxation methods in either a synchronous or asynchronous
      manner.</p>
    <figure id="fig2" class="top">
      <img src="./img/image2.png" />
      <figcaption>
        <strong>Figure 2.</strong> Block diagram of the simulation framework.
      </figcaption>
    </figure>
    <p>
      The simulation requires as input the matrix <span class="math-span">ð´</span>,
      the right hand side <span class="math-span">ð</span> and an initial guess
      at the solution, <span class="math-span">ð¥<sub>0</sub></span>. The
      important pieces of the simulation are all passed as functions to the
      tool. There are three functions required:
    </p>
    <ol>
      <li>An update function that specifies how to perform the relaxation.
        A common technique for this is given by <a href="#eq2">Equation (2)</a>.
        It is certainly possible to modify this equation to obtain different
        updates, as described, e.g., in (<a role="doc-biblioref" href="#Saa03">Saad,
          2003</a>).
      </li>
      <li>An update pattern function that determines which elements of the
        matrix <span class="math-span">ð´</span> are assigned to each simulated
        processor. A common technique for this assignment, is to evenly divide
        the work among all of the available processors, however other patterns
        are also possible. For example, the use of randomization in the solution
        of linear systems via relaxation methods has gained some popularity in
        the fields of optimization and machine learning (see, e.g., (<a
        role="doc-biblioref" href="#ADG15">Avron, Druinsky, &amp; Gupta,
          2015</a>) and references therein) and update patterns such as this are
        easy to implement inside of this framework.
      </li>
      <li>An update time function that captures the empirical information
        that was captured from parallel performance runs on the HPC hardware.
        This function will typically be used to sample from the timing
        distribution that was generated beforehand. Note that, since each
        simulated processor makes calls to this function independently, the
        simulated performance will be asynchronous so long as the function
        returns different values upon different calls. Defining an update time
        function that has constant return (or constant return for every
        processor) provides a means to show synchronous performance.</li>
    </ol>
    <p>
      By varying the three functions that are passed to the framework, not only
      can the HPC performance be predicted by making changes to the update time
      function, but various modifications to the basic algorithm can be quickly
      and easily compared in a manner that reflects real world asynchronous
      performance. With the renewed research interest in asynchronous iterative
      methods that perform relaxation updates, oftentimes performance between
      new variants and existing algorithms is <em>only</em> compared in simple
      synchronous experiments; the simulation framework proposed here allows for
      a more meaningful comparison between methods that does not require
      development of parallel implementations of all the methods or algorithm
      variations that are involved.
    </p>
    <p>
      The simulation framework requires some data that specifies parameters
      concerning the particular run of the simulation such as the desired
      tolerance, the number of processors to simulate, and a computational scale
      factor. The framework itself is developed in MATLAB<sup>Â®</sup> and the
      three required functions are passed as function handles.
    </p>
    <p>
      The simulation itself (see <em>Simulation</em> block in <a href="#fig2">Figure
        2</a>) progresses by reading in the user provided input data, assigning an
      initial update pattern and time to each processor, and then beginning the
      main loop. Inside of the main loop, the time increments and a check is
      performed to see if the current time matches with the scheduled update
      time for any of the processors, if so, the update function is called and
      then a time for the next update is assigned to the processor that just
      updated and (if desired) the update pattern for the current processor is
      changed. After this, a check is performed on the size of the residual to
      determine if the exit criteria is met before the time is incremented again
      and the loop starts over. A pseudocode representation of the simulation
      framework for simulated asynchronous Jacobi is given in <a href="#alg2">Algorithm
        2</a>.
    </p>
    <div id="alg2" class="algorithm column-top">
      <div class="ps-root">
        <div class="ps-algorithm with-caption">
          <p class="ps-line" style="text-indent: -1.2em; padding-left: 1.2em;">
            <span class="ps-keyword">Algorithm 2 </span>Asynchronous Jacobi
            simulation
          </p>
          <div class="ps-algorithmic with-linenum">
            <div class="ps-block" style="margin-left: 1.7999999999999998em;">
              <p class="ps-line ps-code">
                <span class="ps-linenum" style="left: 0em;">1:</span><span
                  style="font-weight: bold;">Input:</span>
                <math xmlns="http://www.w3.org/1998/Math/MathML">
                    <msub>
                    <mi>a</mi>
                    <mrow>
                    <mi>i</mi>
                    <mi>j</mi>
                    </mrow>
                    </msub>
                    <mo>â</mo>
                    <mi>A</mi>
                  </math>
                , initial guess for
                <math xmlns="http://www.w3.org/1998/Math/MathML">
                    <msub>
                    <mi>x</mi>
                    <mn>0</mn>
                    </msub>
                  </math>
                , a number of processing elements
                <math xmlns="http://www.w3.org/1998/Math/MathML">
                  <mi>p</mi>
                </math>
                , an input random number distribution
              </p>
              <p class="ps-line ps-code">
                <span class="ps-linenum" style="left: 0em;">2:</span><span
                  style="font-weight: bold;">Output:</span> Solution vector
                <math xmlns="http://www.w3.org/1998/Math/MathML">
                  <mi>x</mi>
                </math>
              </p>
              <p class="ps-line ps-code">
                <span class="ps-linenum" style="left: 0em;">3:</span>Assign
                processor update times.
                <math xmlns="http://www.w3.org/1998/Math/MathML">
                  <msub>
                  <mi>Ï</mi>
                  <mn>1</mn></msub>
                  <mo separator="true">,</mo>
                  <msub>
                  <mi>Ï</mi>
                  <mn>2</mn></msub>
                  <mo separator="true">,</mo>
                  <mo>â¦</mo>
                  <mo separator="true">,</mo>
                  <msub>
                  <mi>Ï</mi>
                  <mi>p</mi>
                  </msub>
                </math>
                by sampling from an appropriate random number distribution
              </p>
              <p class="ps-line ps-code">
                <span class="ps-linenum" style="left: 0em;">4:</span>Assign
                elements
                <math xmlns="http://www.w3.org/1998/Math/MathML">
                  <msub>
                  <mi>x</mi>
                  <mi>i</mi>
                  </msub>
                  <mo>â</mo>
                  <mi>x</mi>
                </math>
                to each simulated processing element
              </p>
              <p class="ps-line ps-code">
                <span class="ps-linenum" style="left: 0em;">5:</span><span
                  class="ps-keyword">for </span>
                <math xmlns="http://www.w3.org/1998/Math/MathML">
                    <mi>t</mi>
                    <mo>=</mo>
                    <mn>1</mn>
                    <mo separator="true">,</mo>
                    <mn>2</mn>
                    <mo separator="true">,</mo>
                    <mo>â¦</mo>
                  </math>
                until convergence<span class="ps-keyword"> do</span>
              </p>
              <div class="ps-block" style="margin-left: 1.2em;">
                <p class="ps-line ps-code">
                  <span class="ps-linenum" style="left: -1.5em;">6:</span><span
                    class="ps-keyword">for </span>each processing element
                  <math xmlns="http://www.w3.org/1998/Math/MathML">
                      <msub>
                      <mi>P</mi>
                      <mi>l</mi>
                      </msub>
                    </math>
                  <span class="ps-keyword"> do</span>
                </p>
                <div class="ps-block" style="margin-left: 1.2em;">
                  <p class="ps-line ps-code">
                    <span class="ps-linenum" style="left: -3em;">7:</span><span
                      class="ps-keyword">if </span>
                    <math xmlns="http://www.w3.org/1998/Math/MathML">
                        <msub>
                        <mi>Ï</mi>
                        <mi>l</mi>
                        </msub>
                        <mo>=</mo>
                        <mi>t</mi>
                      </math>
                    <span class="ps-keyword">then</span>
                  </p>
                  <div class="ps-block" style="margin-left: 1.2em;">
                    <p class="ps-line ps-code">
                      <span class="ps-linenum" style="left: -4.5em;">8:</span><span
                        class="ps-keyword">for </span>each element
                      <math xmlns="http://www.w3.org/1998/Math/MathML">
                          <msub>
                          <mi>x</mi>
                          <mi>i</mi>
                          </msub>
                          <mo>â</mo>
                          <mi>x</mi>
                        </math>
                      assigned to
                      <math xmlns="http://www.w3.org/1998/Math/MathML">
                        <msub>
                        <mi>P</mi>
                        <mi>l</mi>
                        </msub>
                    </math>
                      <span class="ps-keyword"> do</span>
                    </p>
                    <div class="ps-block" style="margin-left: 1.2em;">
                      <p class="ps-line ps-code">
                        <span class="ps-linenum" style="left: -6em;">9:</span>
                        <math xmlns="http://www.w3.org/1998/Math/MathML">
                          <msub>
                          <mi>x</mi>
                          <mi>i</mi>
                          </msub>
                          <mo>=</mo>
                          <mfrac>
                          <mrow>
                          <mo>â</mo>
                          <mn>1</mn>
                          </mrow>
                          <msub>
                          <mi>a</mi>
                          <mrow>
                          <mi>i</mi>
                          <mi>i</mi>
                          </mrow>
                          </msub>
                          </mfrac>
                          <mrow>
                          <mfenced open="[" close="]">
                          <mrow>
                          <msub>
                          <mo>â</mo>
                          <mrow>
                          <mi>j</mi>
                          <mo>â </mo>
                          <mi>i</mi>
                          </mrow>
                          </msub>
                          <msub>
                          <mi>a</mi>
                          <mrow>
                          <mi>i</mi>
                          <mi>j</mi>
                          </mrow>
                          </msub>
                          <msub>
                          <mi>x</mi>
                          <mi>j</mi></msub>
                          <mo>â</mo>
                          <msub>
                          <mi>b</mi>
                          <mi>i</mi>
                          </msub>
                          </mrow>
                          </mfenced>
                          </mrow>
                        </math>
                      </p>
                    </div>
                    <p class="ps-line ps-code">
                      <span class="ps-linenum" style="left: -4.5em;">10:</span><span
                        class="ps-keyword">end for</span>
                    </p>
                    <p class="ps-line ps-code">
                      <span class="ps-linenum" style="left: -4.5em;">11:</span>Retrieve
                      a new update time
                      <math xmlns="http://www.w3.org/1998/Math/MathML">
                        <msub>
                        <mi>Ï</mi>
                        <mi>l</mi>
                        </msub>
                    </math>
                      by sampling from the input distribution
                    </p>
                  </div>
                  <p class="ps-line ps-code">
                    <span class="ps-linenum" style="left: -3em;">12:</span><span
                      class="ps-keyword">end if</span>
                  </p>
                </div>
                <p class="ps-line ps-code">
                  <span class="ps-linenum" style="left: -1.5em;">13:</span><span
                    class="ps-keyword">end for</span>
                </p>
                <p class="ps-line ps-code">
                  <span class="ps-linenum" style="left: -1.5em;">14:</span>Calculate
                  the residual as in <a href="#eq3">Equation (3)</a> and check
                  termination conditions
                </p>
              </div>
              <p class="ps-line ps-code">
                <span class="ps-linenum" style="left: 0em;">15:</span><span
                  class="ps-keyword">end for</span>
              </p>
            </div>
          </div>
        </div>
      </div>
    </div>
    <p>
      <a href="#alg2">Algorithm 2</a>, a given update time <span
        class="math-span">Ï<sub>ð</sub></span> will often not be sampled as an
      integer. The simulation adjusts for this by scaling the number that is
      sampled by the appropriate order of magnitude, adjusting the maximum value
      allowed for <span class="math-span">ð¡</span> accordingly, and then
      scaling back the final time calculated by the simulation. For example, if
      the desired time precision is hundredths of a second, and the time
      resulting for the first sampling of <span class="math-span">Ï<sub>ð</sub></span>
      was 1.234<span class="math-span">ð </span>, then the simulation would
      perform the following steps:
    </p>
    <ol>
      <li><math xmlns="http://www.w3.org/1998/Math/MathML">
      <msubsup>
      <mi>Ï</mi>
      <mi>l</mi>
      <mtext>new</mtext>
      </msubsup>
      <mo>=</mo>
      <mi>s</mi>
      <mo>Ã</mo>
      <msubsup>
      <mi>Ï</mi>
      <mi>l</mi>
      <mtext>old</mtext>
      </msubsup>
      </math></li>
      <li><math xmlns="http://www.w3.org/1998/Math/MathML">
      <msubsup>
      <mi>Ï</mi>
      <mtext>max</mtext>
      <mtext>new</mtext>
      </msubsup>
      <mo>=</mo>
      <mi>s</mi>
      <mo>Ã</mo>
      <msubsup>
      <mi>Ï</mi>
      <mtext>max</mtext>
      <mtext>old</mtext>
      </msubsup>
      </math></li>
      <li><math xmlns="http://www.w3.org/1998/Math/MathML">
      <msubsup>
      <mi>Ï</mi>
      <mtext>final</mtext>
      <mtext>new</mtext>
      </msubsup>
      <mo>=</mo>
      <mfenced open="(" close=")">
      <mfrac>
      <mn>1</mn>
      <mi>s</mi>
      </mfrac>
      </mfenced>
      <mo>Ã</mo>
      <msubsup>
      <mi>Ï</mi>
      <mtext>final</mtext>
      <mtext>old</mtext>
      </msubsup>
      <mtext maxsize="1" mathvariant="normal">,</mtext>
      </math></li>
    </ol>
    <p>
      where <span class="math-span">ð </span> referenced is the <q>scale_factor</q>
      defined in the block diagram given by <a href="#fig2">Figure 2</a>. For
      example, if the desired precision is hundredths of a second, <span
        class="math-span">ð  = 10<sup>2</sup></span>, and the sampled value <span
        class="math-span">Ï<sub>ð</sub></span> becomes
    </p>
    <div class="equation">
      <math display="block" xmlns="http://www.w3.org/1998/Math/MathML">
        <mtable columnalign="left">
        <mtr>
          <mtd>
          <msub>
          <mi>Ï</mi>
          <mi>l</mi>
          </msub>
          <mo>=</mo>
          <mn>1.234</mn>
          <mtext mathvariant="normal"> â initial sample</mtext>
          </mtd>
        </mtr>
        <mtr>
          <mtd>
          <msub>
          <mi>Ï</mi>
          <mi>l</mi>
          </msub>
          <mo>=</mo>
          <mn>123.4</mn>
          <mtext mathvariant="normal"> â apply scale factor</mtext>
          </mtd>
        </mtr>
        <mtr>
          <mtd>
          <msub>
          <mi>Ï</mi>
          <mi>l</mi>
          </msub>
          <mo>=</mo>
          <mn>123</mn>
          <mtext mathvariant="normal"> â round to the nearest integer.</mtext>
          </mtd>
        </mtr>
        </mtable>
      </math>
    </div>
    <p>
      Inside of the simulation framework, time is abstracted away to <q>units
        of time</q>, and then the final time is scaled back into the appropriate
      units. This allows the framework to be adapted to future HPC environments,
      as well as examining the impact of the standard variance of single core
      performance on multi-core hardware elements if the method that is used is
      tuned to be completely asynchronous. It should be possible â by adding or
      removing appropriate communication penalties â to simulate the performance
      of different memory architectures (e.g., distributed or cloud computing
      environments). This is left as future work although the method for doing
      this inside of the proposed framework is straightforward.
    </p>

    <section id="sect4.1">
      <h1>Sample Use-Cases for the Framework</h1>
      <p>
        Let the matrix <span class="math-span">ð´</span> result from a simple
        two dimensional finite-difference discretization of the Laplacian over a
        <span class="math-span">10 Ã 10</span> grid, resulting in a <span
          class="math-span">100 Ã 100</span> matrix with an average of 4.6
        non-zero entries per row. The Laplacian
      </p>
      <div class="equation">
        <math display="block" xmlns="http://www.w3.org/1998/Math/MathML">
          <mtable columnalign="right left" columnspacing="0.28em"
            displaystyle="true">
            <mtr>
              <mtd>
                <mo>Î</mo>
                <mi>u</mi>
              </mtd>
              <mtd>
                <mo>=</mo>
                <mi>g</mi>
                <mtext maxsize="1" mathvariant="normal">,</mtext>
              </mtd>
            </mtr>
            <mtr>
              <mtd>
                <msub>
                <mi>u</mi>
                <mrow>
                <mi>x</mi>
                <mi>x</mi>
                </mrow>
                </msub>
                <mo>+</mo>
                <msub>
                <mi>u</mi>
                <mrow>
                <mi>y</mi>
                <mi>y</mi>
                </mrow>
                </msub>
              </mtd>
              <mtd>
                <mo>=</mo>
                <mi>g</mi>
              </mtd>
            </mtr>
          </mtable>
        </math>
      </div>
      <p>is a partial differential equation (PDE) commonly found in both
        science and engineering. The example problems taken in this study can be
        thought of as simulating the diffusion of heat across a two dimensional
        surface given some heat source along the boundary of the problem.</p>
      <p>Once the PDE is discretized over the desired grid using finite
        differences, typically central finite differences, the linear system</p>
      <div class="equation">
        <math display="block" xmlns="http://www.w3.org/1998/Math/MathML">
          <mtable>
            <mtr>
              <mtd>
                <mrow>
                <mi>A</mi>
                <mi>x</mi>
                <mo>=</mo>
                <mi>b</mi>
                </mrow>
              </mtd>
            </mtr>
          </mtable>
        </math>
      </div>
      <p>is set up to be solved for a random right-hand side b that
        represents the desired boundary conditions. All problems considered in
        this study use Dirichlet boundary conditions. For the examples in this
        particular subsection, the righthand side is generated by taking each
        component sampled as a uniform random number between â0:5 and 0:5, and
        then normalizing the resultant vector. The iterative Jacobi method
        proceeds until the residual</p>
      <div id="eq3" class="equation">
        <math display="block" xmlns="http://www.w3.org/1998/Math/MathML">
          <mtable>
            <mlabeledtr>
              <mtd>
                <mtext>(3)</mtext>
              </mtd>
              <mtd columnalign="center">
                <mrow>
                <mi>r</mi>
                <mo>=</mo>
                <mi>b</mi>
                <mo>â</mo>
                <mi>A</mi>
                <mi>x</mi>
                </mrow>
              </mtd>
            </mlabeledtr>
          </mtable>
        </math>
      </div>
      <p>is reduced past some desired threshold.</p>
      <p>
        To begin with, an example of nominal performance of the solution of the
        two dimensional Laplacian in a synchronous environment is provided by <a
          href="#fig3">Figure 3</a>.
      </p>
      <figure id="fig3">
        <img src="./img/image3.png" />
        <figcaption class="multi-line-pdf">
          <strong>Figure 3.</strong> Example of nominal performance of the
          synchronous Jacobi iteration.
        </figcaption>
      </figure>
      <p>
        Next, consider the same problem from above, but in two slightly more
        complicated scenarios. In <a href="#fig4">Figure 4</a> one of the ten
        processors involved in updating blocks of components of <span
          class="math-span">ð¥</span> is provided updates more slowly than the
        other processors. This could reflect the scenario where updates are
        either performed synchronously or asynchronously where the effect of
        variance in performance is negligible, and a single processor has
        degraded performance. This can also be viewed as a look at the impact of
        asynchronous behavior on the Jacobi algorithm. Each curve shows the
        progression of the (global) residual subject to having a single slower
        processor with different degrees of slowdown (from zero to 11x).
      </p>
      <figure id="fig4">
        <img src="./img/image4.png" />
        <figcaption class="multi-line-pdf">
          <strong>Figure 4.</strong> Example of experiments within the
          simulation framework; single processor slowdown.
        </figcaption>
      </figure>
      <p>
        In <a href="#fig5">Figure 5</a> the processor updates are not restricted
        to occur synchronously. Instead, the processors are assumed to have
        similar performance and perform their updates in time <span
          class="math-span">ð¡<sub>ð</sub> â¼ ð(u, Ï<sup>2</sup>)
        </span> where the mean is set to 10 units of time and the variance is different
        for each curve depicted in the plot. An increase in the variance of
        processor performance, regardless of the timing distribution, could come
        about for a variety of reasons; an example of a scenario in the future
        could be having chips with more cores and lower voltage that are
        designed to address the challenges in creating very large scale HPC
        environments.
      </p>
      <figure id="fig5">
        <img src="./img/image5.png" />
        <figcaption class="multi-line-pdf">
          <strong>Figure 5.</strong> Example of experiments within the
          simulation framework; effect of variance.
        </figcaption>
      </figure>
    </section>
  </section>

  <section id="sect5">
    <h1>Asynchronous Jacobi Implementations for the Framework</h1>
    <p>
      <a href="#fig4">Figure 4</a> and <a href="#fig5">Figure 5</a> show
      relative differences in compute times among sharedmemory computing
      elements for a specific problem and a specific asynchronous iterative
      method. A more general simulation framework, which can be used for
      modeling and testing any synchronous or asynchronous iterative relaxation
      method, is presented here. Baseline, non-resilient method behavior may be
      reproduced in the framework; further, the user may also investigate fault
      injection and checkpointing.
    </p>
    <p>
      The user decomposes the method according to the input parameters required
      by the simulation framework. The update function that performs the
      relaxation has an associated operational time, both of which are defined
      by the user. Functionality within the relaxation may be isolated into
      discrete operations with corresponding time information; the level of
      granularity is decided by the user. For example, time to complete an
      operation in the simulation framework may be modeled with a probability
      density function derived from empirical data. To model time to perform
      specific operations or calculations during method execution, data is
      collected from the application during execution. In the implementation
      code, operations are enclosed within calls to time functions, which
      measure time to perform the operations. In this work the OpenMP<sup>Â®</sup>
      library function
      <code>omp_get_wtime()</code>
      is used to measure wall time. For HPC implementations that use MPI,
      <code>MPI_Wtime()</code>
      may be used to measure wall time. Fine-grained operations in the code
      should not overlap such that measurements overlap, i.e., for one
      operation, do not measure time function calls of another operation. After
      taking sufficient measurements, an operation is modeled by fitting a
      probability density function to a normalized histogram of the time data.
      This function may be included as part of the input to the framework. Note
      that when comparing simulated run times with HPC run times, it may be
      preferable to use an unmodified version of the HPC implementation code
      that does not have time function calls and mechanisms for storing or
      printing times. These functions and activities may increase run time and
      provide an inaccurate metric for comparison.
    </p>
    <p>This section describes two asynchronous relaxation method
      implementations and two corresponding use cases of the simulation
      framework. For both implementations, the test problem is a two dimensional
      discretization of the Laplacian</p>
    <div class="equation">
      <math display="block" xmlns="http://www.w3.org/1998/Math/MathML">
        <mtable>
          <mtr>
            <mtd>
              <mrow>
              <mo>Î</mo>
              <mi>u</mi>
              <mo>=</mo>
              <mi>b</mi>
              <mtext maxsize="1" mathvariant="normal">,</mtext>
              </mrow>
            </mtd>
          </mtr>
        </mtable>
      </math>
    </div>
    <p>
      where the right-hand side is initialized with Dirichlet boundary
      conditions. Both implementations use OpenMP<sup>Â®</sup> for shared-memory
      parallelism and are executed on the shared-memory computing platform
      nicknamed Rulfo, which is an Intel<sup>Â®</sup> Xeon Phi<sup>â¢</sup>
      Knight's Landing<span class="fn"><sup><a href="#ftn2"
          id="fn_pointer_ftn2" role="doc-noteref"
          title="Footnote 2: Rulfo is a part of computing resources of the Department of Modeling, Simulation and Visualization Engineering at Old Dominion University.">2</a></sup></span>
      having 7210 model processor with 64 cores. Each core may optimally execute
      4 threads for 256 threads total, and runs at 1.30 GHz. The simulation
      framework and experiments were implemented in MATLAB<sup>Â®</sup> R2018a,
      while the Jacobi implementations were written in C/C++ using the Intel C
      compiler version 17.04 and OpenMP<sup>Â®</sup> version 4.5.
    </p>

    <section id="sect5.1">
      <h1>Implementation 1: General Jacobi Solver</h1>
      <p>
        In this case, the heat problem is represented mathematically by a sparse
        matrix, which is solved by an asynchronous general Jacobi method. The
        Laplacian is generated over a <span class="math-span">100 Ã 100</span>
        grid resulting in a matrix of size <span class="math-span">10,000
          Ã 10,000</span> with 49,600 non-zeros with an average of 4.96 non-zeros per
        row. The vector b from the resulting linear system,
      </p>
      <div class="equation">
        <math display="block" xmlns="http://www.w3.org/1998/Math/MathML">
          <mtable>
            <mtr>
              <mtd>
                <mrow>
                <mi>A</mi>
                <mi>x</mi>
                <mo>=</mo>
                <mi>b</mi>
                </mrow>
              </mtd>
            </mtr>
          </mtable>
        </math>
      </div>
      <p>
        is initialized such that the final solution vector has <span
          class="math-span">ð¥<sub>ð</sub> = 1
        </span> for all <span class="math-span">ð</span>. The initial guess <span
          class="math-span">ð¥<sub>0</sub></span> is all zeros.
      </p>
      <p>
        In this implementation, all threads but one perform relaxations on
        assigned components, and a dedicated thread computes the global residual
        norm value <span class="math-span">ðâð´ð¥<sup>(ð¡)</sup></span> that
        determines satisfactory convergence. Each thread retrieves the data it
        needs from shared memory, performs the necessary computations, and, in
        the case of the relaxation threads, writes the result back to shared
        memory. Synchronous shared-memory implementations of all classes of
        algorithms commonly use mutex locks to avoid race conditions with read
        and write operations. However, this type of asynchronous relaxation
        method may be less dependent on these safeguards for two reasons: (1)
        iterative methods can correct some errors with more iterations, if
        necessary, and (2) threads executing operations in asynchronous
        iterative methods are more likely to be at different stages of the
        iterative cycle, meaning fewer threads may be writing to and reading
        from the same memory location concurrently. This general Jacobi solver
        has two varieties: (a) <span class="small-caps">Safe</span> which uses
        mutex locks to avoid race conditions, and (b) <span class="small-caps">Race</span>
        which permits race conditions. <span class="small-caps">Safe</span> uses
        OpenMP<sup>Â®</sup> locks to copy <span class="math-span">ð¥<sup>(ð¡)</sup></span>
        safely from shared memory and to update <span class="math-span">ð¥<sup>(ð¡+1)</sup></span>.
        Pseudocode for this process is given in <a href="#alg3">Algorithm 3</a>,
        where bold upper-case text indicates that OpenMP<sup>Â®</sup> locks are
        employed. The algorithm for <span class="small-caps">Race</span> is
        identical to <a href="#alg3">Algorithm 3</a>, with the exception that
        locks are omitted.
      </p>
      <div id="alg3" class="algorithm column-top">
        <div class="ps-root">
          <div class="ps-algorithm with-caption">
            <p class="ps-line" style="text-indent: -1.2em; padding-left: 1.2em;">
              <span class="ps-keyword">Algorithm 3 </span>OpenMP Implementation
              1 (a) <span style="font-style: normal; font-variant: small-caps;">Safe</span>
            </p>
            <div class="ps-algorithmic with-linenum">
              <div class="ps-block" style="margin-left: 1.7999999999999998em;">
                <p class="ps-line ps-code">
                  <span class="ps-linenum" style="left: 0em;">1:</span><span
                    style="font-weight: bold;">Input:</span>
                  <math xmlns="http://www.w3.org/1998/Math/MathML">
                    <msub>
                    <mi>a</mi>
                    <mrow>
                    <mi>i</mi>
                    <mi>j</mi>
                    </mrow>
                    </msub>
                    <mo>â</mo>
                    <mi>A</mi>
                  </math>
                  ,
                  <math xmlns="http://www.w3.org/1998/Math/MathML">
                    <mi>b</mi>
                  </math>
                  , initial guess for
                  <math xmlns="http://www.w3.org/1998/Math/MathML">
                    <msub>
                    <mi>X</mi>
                    <mn>0</mn>
                    </msub>
                  </math>
                  ,
                  <math xmlns="http://www.w3.org/1998/Math/MathML">
                    <mi>n</mi>
                  </math>
                  , processing elements
                  <math xmlns="http://www.w3.org/1998/Math/MathML">
                    <mi>p</mi>
                   </math>
                </p>
                <p class="ps-line ps-code">
                  <span class="ps-linenum" style="left: 0em;">2:</span><span
                    style="font-weight: bold;">Output:</span> Solution vector
                  <math xmlns="http://www.w3.org/1998/Math/MathML">
                      <mi>X</mi>
                   </math>
                </p>
                <p class="ps-line ps-code">
                  <span class="ps-linenum" style="left: 0em;">3:</span>Assign
                  elements
                  <math xmlns="http://www.w3.org/1998/Math/MathML">
                    <msub>
                    <mi>X</mi>
                    <mi>i</mi>
                    </msub>
                    <mo>â</mo>
                    <mi>X</mi>
                  </math>
                  to
                  <math xmlns="http://www.w3.org/1998/Math/MathML">
                    <mi>n</mi>
                    <mo>â</mo>
                    <mn>1</mn>
                  </math>
                  processing elements,
                  <math xmlns="http://www.w3.org/1998/Math/MathML">
                    <mi>i</mi>
                    <mo>=</mo>
                    <mfenced open="[" close="]">
                    <mi>Î±</mi>
                    <mi>Ï</mi>
                    </mfenced>
                  </math>
                </p>
                <p class="ps-line ps-code">
                  <span class="ps-linenum" style="left: 0em;">4:</span><span
                    class="ps-keyword">for </span><span
                    style="font-weight: bold;">parallel</span> each processing
                  element in
                  <math xmlns="http://www.w3.org/1998/Math/MathML">
                    <msub>
                    <mi>p</mi>
                    <mn>1</mn>
                    </msub>
                    <mo>â¦</mo>
                    <msub>
                    <mi>p</mi>
                    <mi>n</mi>
                    </msub>
                  </math>
                  <span class="ps-keyword"> do</span>
                </p>
                <div class="ps-block" style="margin-left: 1.2em;">
                  <p class="ps-line ps-code">
                    <span class="ps-linenum" style="left: -1.5em;">5:</span><span
                      class="ps-keyword">while </span>residual norm
                    <math xmlns="http://www.w3.org/1998/Math/MathML">
                      <mo>&gt;</mo>
                    </math>
                    tolerance <span class="ps-keyword">do</span>
                  </p>
                  <div class="ps-block" style="margin-left: 1.2em;">
                    <p class="ps-line ps-code">
                      <span class="ps-linenum" style="left: -3em;">6:</span><span
                        style="font-weight: bold;">COPY</span> global
                      <math xmlns="http://www.w3.org/1998/Math/MathML">
                          <msup>
                          <mi>X</mi>
                          <mrow>
                          <mfenced open="(" close=")">
                          <mi>t</mi>
                          </mfenced>
                          </mrow>
                          </msup>
                        </math>
                      from shared memory to local
                      <math xmlns="http://www.w3.org/1998/Math/MathML">
                          <msup>
                          <mi>x</mi>
                          <mrow>
                          <mfenced open="(" close=")">
                          <mi>t</mi>
                          </mfenced>
                          </mrow>
                          </msup>
                        </math>
                    </p>
                    <p class="ps-line ps-code">
                      <span class="ps-linenum" style="left: -3em;">7:</span><span
                        class="ps-keyword">if </span>
                      <math xmlns="http://www.w3.org/1998/Math/MathML">
                          <msub>
                          <mi>p</mi>
                          <mi>1</mi>
                          </msub>
                        </math>
                      <span class="ps-keyword"> then</span>
                    </p>
                    <div class="ps-block" style="margin-left: 1.2em;">
                      <p class="ps-line ps-code">
                        <span class="ps-linenum" style="left: -4.5em;">8:</span>Compute
                        residual norm
                        <math xmlns="http://www.w3.org/1998/Math/MathML">
                          <mrow>
                          <mfenced open="||" close="||">
                          <mrow>
                          <mi>b</mi>
                          <mo>â</mo>
                          <mi>A</mi>
                          <mi>x</mi>
                          </mrow>
                          </mfenced>
                          </mrow>
                          <mn>2</mn>
                        </math>
                      </p>
                    </div>
                    <p class="ps-line ps-code">
                      <span class="ps-linenum" style="left: -3em;">9:</span><span
                        class="ps-keyword">else if </span>
                      <math xmlns="http://www.w3.org/1998/Math/MathML">
                          <msub>
                          <mi>p</mi>
                          <mn>2</mn>
                          </msub>
                          <mo>â¦</mo>
                          <msub>
                          <mi>p</mi>
                          <mi>n</mi>
                          </msub>
                        </math>
                      <span class="ps-keyword"> then</span>
                    </p>
                    <div class="ps-block" style="margin-left: 1.2em;">
                      <p class="ps-line ps-code">
                        <span class="ps-linenum" style="left: -4.5em;">10:</span><span
                          class="ps-keyword">for </span>
                        <math xmlns="http://www.w3.org/1998/Math/MathML">
                            <mi>x</mi>
                          </math>
                        index
                        <math xmlns="http://www.w3.org/1998/Math/MathML">
                            <mi>i</mi>
                            <mo>=</mo>
                            <mi>Î±</mi>
                            <mo>â¦</mo>
                            <mi>Ï</mi>
                          </math>
                        <span class="ps-keyword"> do</span>
                      </p>
                      <div class="ps-block" style="margin-left: 1.2em;">
                        <p class="ps-line ps-code">
                          <span class="ps-linenum" style="left: -6em;">11:</span>Compute
                          <math xmlns="http://www.w3.org/1998/Math/MathML">
                            <msubsup>
                            <mi>x</mi>
                            <mi>i</mi>
                            <mfenced open="(" close=")">
                            <mrow>
                            <mi>t</mi>
                            <mo>+</mo>
                            <mn>1</mn>
                            </mrow>
                            </mfenced>
                            </msubsup>
                            <mo>=</mo>
                            <mfrac>
                            <mrow>
                            <mo>â</mo>
                            <mn>1</mn>
                            </mrow>
                            <msub>
                            <mi>a</mi>
                            <mrow>
                            <mi>i</mi>
                            <mi>i</mi>
                            </mrow>
                            </msub>
                            </mfrac>
                            <mrow>
                            <mfenced open="[" close="]">
                            <mrow>
                            <msub>
                            <mo>â</mo>
                            <mrow>
                            <mi>j</mi>
                            <mo>â </mo>
                            <mi>i</mi>
                            </mrow>
                            </msub>
                            <msub>
                            <mi>a</mi>
                            <mrow>
                            <mi>i</mi>
                            <mi>j</mi>
                            </mrow>
                            </msub>
                            <msubsup>
                            <mi>x</mi>
                            <mi>j</mi>
                            <mrow>
                            <mfenced open="(" close=")">
                            <mi>t</mi>
                            </mfenced>
                            </mrow>
                            </msubsup>
                            <mo>â</mo>
                            <msub>
                            <mi>b</mi>
                            <mi>i</mi>
                            </msub>
                            </mrow>
                            </mfenced>
                            </mrow>
                          </math>
                        </p>
                      </div>
                      <p class="ps-line ps-code">
                        <span class="ps-linenum" style="left: -4.5em;">12:</span><span
                          class="ps-keyword">end for</span>
                      </p>
                      <p class="ps-line ps-code">
                        <span class="ps-linenum" style="left: -4.5em;">13:</span><span
                          style="font-weight: bold;">UPDATE</span>
                        <math xmlns="http://www.w3.org/1998/Math/MathML">
                            <msubsup>
                            <mi>X</mi>
                            <mi>i</mi>
                            <mrow>
                            <mfenced open="(" close=")">
                            <mrow>
                            <mi>t</mi>
                            <mo>+</mo>
                            <mn>1</mn>
                            </mrow>
                            </mfenced>
                            </mrow>
                            </msubsup>
                          </math>
                        in shared memory with
                        <math xmlns="http://www.w3.org/1998/Math/MathML">
                          <msubsup>
                          <mi>x</mi>
                          <mi>i</mi>
                          <mrow>
                          <mfenced open="(" close=")">
                          <mrow>
                          <mi>t</mi>
                          <mo>+</mo>
                          <mn>1</mn>
                          </mrow>
                          </mfenced>
                          </mrow>
                          </msubsup>
                        </math>
                        for all
                        <math xmlns="http://www.w3.org/1998/Math/MathML">
                          <mi>i</mi>
                        </math>
                        belonging to processing element
                      </p>
                    </div>
                    <p class="ps-line ps-code">
                      <span class="ps-linenum" style="left: -3em;">14:</span><span
                        class="ps-keyword">end if</span>
                    </p>
                  </div>
                  <p class="ps-line ps-code">
                    <span class="ps-linenum" style="left: -1.5em;">15:</span><span
                      class="ps-keyword">end while</span>
                  </p>
                </div>
                <p class="ps-line ps-code">
                  <span class="ps-linenum" style="left: 0em;">16:</span><span
                    class="ps-keyword">end for</span>
                </p>
              </div>
            </div>
          </div>
        </div>
      </div>
      <p>
        <a href="#fig6">Figure 6</a> compares <span class="small-caps">Safe</span>
        and <span class="small-caps">Race</span> calculation times and number of
        iterations. Calculation times and average iteration counts are similar
        for thread counts up to 81, but behavior diverges beyond that. For
        thread counts 101 through 501, <span class="small-caps">Race</span>
        requires more iterations, perhaps to compensate for threads reading and
        computing with inaccurate <span class="math-span">ð¥</span> vectors.
        Despite this, <a href="#fig6a">Figure 6(a)</a> shows that <span
          class="small-caps">Race</span> is still quicker for the largest thread
        counts, perhaps because threads do not use locks to access data and
        eliminate that overhead cost. <a href="#fig6a">Figure 6(a)</a> also
        shows that perhaps locks are not too costly for intermediate thread
        counts 101, 201, and 251, where <span class="small-caps">Safe</span>
        outperforms <span class="small-caps">Race</span> in terms of calculation
        time.
      </p>
      <table id="fig6" class="table-figure top">
        <tbody>
          <tr>
            <td><figure id="fig6a">
                <img src="./img/image6a.png" />
                <figcaption>
                  <strong>(a)</strong> Calculation time.
                </figcaption>
              </figure></td>
            <td><figure id="fig6b">
                <img src="./img/image6b.png" />
                <figcaption>
                  <strong>(b)</strong> Number of iterations, average per thread.
                </figcaption>
              </figure></td>
          </tr>
        </tbody>
        <caption>
          <strong>Figure 6.</strong> Performance variations between <span
            class="small-caps">Safe</span> and <span class="small-caps">Race</span>
          as a function of thread count.
        </caption>
      </table>
      <p>
        Both <span class="small-caps">Safe</span> and <span class="small-caps">Race</span>
        were executed over several trials and varying thread counts on the
        experimental HPC platform. For each trial, the times for a thread to
        access the solution in shared memory (Line 6 of <a href="#alg3">Algorithm
          3</a>), compute the relaxation for the rows assigned to it (Line 11), and
        to update the solution in shared memory (Line 13) were captured. This
        data was used to generate MATLAB<sup>Â®</sup> kernel probability density
        functions for modeling the amount of time a thread takes to complete a
        copy, compute, or update operation. These distributions may be used in
        the simulation framework as an input parameter, for the generation of
        random variables corresponding to key operational times in the HPC
        architecture. <a href="#alg2">Algorithm 2</a> demonstrates the use of a
        time distribution in the framework. Thread counts of 11, 21, 41, 81,
        101, 201, 251, and 401 were used to collect data for the generation of
        distributions, some of which are in <a href="#fig7">Figure 7</a> and <a
          href="#fig8">Figure 8</a>. For 201 threads, <span class="small-caps">Safe</span>
        in <a href="#fig7d">Figure 7(d)</a> and <a href="#fig7f">Figure 7(f)</a>
        shows the tendency of locks to stratify copy and update times, compared
        with <span class="small-caps">Race</span> in <a href="#fig8d">Figure
          8(d)</a> and <a href="#fig8f">Figure 8(f)</a>, which are less uniform.
        These findings are mirrored in <a href="#tab1">Table 1</a>, which
        provides mean times for each of the three operations that were
        benchmarked in this implementation, for <span class="small-caps">Safe</span>
        and <span class="small-caps">Race</span>. <span class="small-caps">Race</span>
        copy and update times are slightly or significantly quicker than
        comparable <span class="small-caps">Safe</span> times. Compute times
        typically dominate total iteration time, except for <span
          class="small-caps">Safe</span> copy and update times for threads 201,
        251, and 401. <a href="#tab1">Table 1</a> shows that increasing the
        number of threads decreases <span class="small-caps">Race</span> copy,
        compute, and update times until cores are sufficiently over-subscribed:
        at 201 threads, these operations have become significantly more costly,
        as compared with 101 threads. This cost may be attributed to thread
        context switching. Compute times for <span class="small-caps">Safe</span>
        do not increase with higher thread counts because thread behavior is
        controlled explicitly using locks. These statistics can be used to
        validate the performance of the time distributions, so that the
        framework provides results comparable to the HPC hardware.
      </p>
      <table id="tab1" class="top">
        <thead>
          <tr>
            <th rowspan="2">Threads</th>
            <th colspan="3"><span class="small-caps">Safe</span></th>
            <th colspan="3"><span class="small-caps">Race</span></th>
          </tr>
          <tr>
            <th>Copy<br /> (10<sup>â5</sup>s)
            </th>
            <th>Compute<br /> (10<sup>â4</sup>s)
            </th>
            <th>Update<br /> (10<sup>â6</sup>s)
            </th>
            <th>Copy<br /> (10<sup>â5</sup>s)
            </th>
            <th>Compute<br /> (10<sup>â4</sup>s)
            </th>
            <th>Update<br /> (10<sup>â6</sup>s)
            </th>
          </tr>
        </thead>
        <tbody style="text-align: right">
          <tr>
            <td style="text-align: center">11</td>
            <td>1.28</td>
            <td>167.00</td>
            <td>7.76</td>
            <td>1.15</td>
            <td>167.00</td>
            <td>2.79</td>
          </tr>
          <tr>
            <td style="text-align: center">21</td>
            <td>1.31</td>
            <td>84.30</td>
            <td>6.98</td>
            <td>1.17</td>
            <td>83.60</td>
            <td>1.96</td>
          </tr>
          <tr>
            <td style="text-align: center">41</td>
            <td>1.38</td>
            <td>43.00</td>
            <td>7.09</td>
            <td>1.23</td>
            <td>43.10</td>
            <td>1.63</td>
          </tr>
          <tr>
            <td style="text-align: center">81</td>
            <td>2.98</td>
            <td>27.30</td>
            <td>20.60</td>
            <td>1.43</td>
            <td>27.20</td>
            <td>1.79</td>
          </tr>
          <tr>
            <td style="text-align: center">101</td>
            <td>36.70</td>
            <td>23.40</td>
            <td>357.00</td>
            <td>1.64</td>
            <td>25.20</td>
            <td>1.79</td>
          </tr>
          <tr>
            <td style="text-align: center">201</td>
            <td>251.00</td>
            <td>15.30</td>
            <td>2500.00</td>
            <td>11.80</td>
            <td>74.30</td>
            <td>4.33</td>
          </tr>
          <tr>
            <td style="text-align: center">251</td>
            <td>345.00</td>
            <td>13.30</td>
            <td>3440.00</td>
            <td>16.60</td>
            <td>90.90</td>
            <td>4.55</td>
          </tr>
          <tr>
            <td style="text-align: center">401</td>
            <td>1880.00</td>
            <td>8.23</td>
            <td>18,700.00</td>
            <td>20.20</td>
            <td>91.60</td>
            <td>4.52</td>
          </tr>
        </tbody>
        <caption>
          <strong>Table 1.</strong> Mean times for copy, compute, and update
          operations.
        </caption>
      </table>
      <table id="fig7" class="table-figure top">
        <tbody>
          <tr>
            <td><figure id="fig7a">
                <img src="./img/image7a.png" />
                <figcaption>
                  <strong>(a)</strong> <span class="math-span">ð¥</span> copy,
                  11 threads.
                </figcaption>
              </figure></td>
            <td><figure id="fig7b">
                <img src="./img/image7b.png" />
                <figcaption>
                  <strong>(b)</strong> <span class="math-span">ð¥</span>
                  compute, 11 threads.
                </figcaption>
              </figure></td>
            <td><figure id="fig7c">
                <img src="./img/image7c.png" />
                <figcaption>
                  <strong>(c)</strong> <span class="math-span">ð¥</span> update,
                  11 threads.
                </figcaption>
              </figure></td>
          </tr>
          <tr>
            <td><figure id="fig7d">
                <img src="./img/image7d.png" />
                <figcaption>
                  <strong>(d)</strong> <span class="math-span">ð¥</span> copy,
                  81 threads.
                </figcaption>
              </figure></td>
            <td><figure id="fig7e">
                <img src="./img/image7e.png" />
                <figcaption>
                  <strong>(e)</strong> <span class="math-span">ð¥</span>
                  compute, 81 threads.
                </figcaption>
              </figure></td>
            <td><figure id="fig7f">
                <img src="./img/image7f.png" />
                <figcaption>
                  <strong>(f)</strong> <span class="math-span">ð¥</span> update,
                  81 threads.
                </figcaption>
              </figure></td>
          </tr>
          <tr>
            <td><figure id="fig7g">
                <img src="./img/image7g.png" />
                <figcaption>
                  <strong>(g)</strong> <span class="math-span">ð¥</span> copy,
                  201 threads.
                </figcaption>
              </figure></td>
            <td><figure id="fig7h">
                <img src="./img/image7h.png" />
                <figcaption>
                  <strong>(h)</strong> <span class="math-span">ð¥</span>
                  compute, 201 threads.
                </figcaption>
              </figure></td>
            <td><figure id="fig7i">
                <img src="./img/image7i.png" />
                <figcaption>
                  <strong>(i)</strong> <span class="math-span">ð¥</span> update,
                  201 threads.
                </figcaption>
              </figure></td>
          </tr>
        </tbody>
        <caption>
          <strong>Figure 7.</strong> <span class="small-caps">Safe</span> copy,
          compute, and update histograms with kernel fits.
        </caption>
      </table>
      <table id="fig8" class="table-figure top">
        <tbody>
          <tr>
            <td><figure id="fig8a">
                <img src="./img/image8a.png" />
                <figcaption>
                  <strong>(a)</strong> <span class="math-span">ð¥</span> copy,
                  11 threads.
                </figcaption>
              </figure></td>
            <td><figure id="fig8b">
                <img src="./img/image8b.png" />
                <figcaption>
                  <strong>(b)</strong> <span class="math-span">ð¥</span>
                  compute, 11 threads.
                </figcaption>
              </figure></td>
            <td><figure id="fig8c">
                <img src="./img/image8c.png" />
                <figcaption>
                  <strong>(c)</strong> <span class="math-span">ð¥</span> update,
                  11 threads.
                </figcaption>
              </figure></td>
          </tr>
          <tr>
            <td><figure id="fig8d">
                <img src="./img/image8d.png" />
                <figcaption>
                  <strong>(d)</strong> <span class="math-span">ð¥</span> copy,
                  81 threads.
                </figcaption>
              </figure></td>
            <td><figure id="fig8e">
                <img src="./img/image8e.png" />
                <figcaption>
                  <strong>(e)</strong> <span class="math-span">ð¥</span>
                  compute, 81 threads.
                </figcaption>
              </figure></td>
            <td><figure id="fig8f">
                <img src="./img/image8f.png" />
                <figcaption>
                  <strong>(f)</strong> <span class="math-span">ð¥</span> update,
                  81 threads.
                </figcaption>
              </figure></td>
          </tr>
          <tr>
            <td><figure id="fig8g">
                <img src="./img/image8g.png" />
                <figcaption>
                  <strong>(g)</strong> <span class="math-span">ð¥</span> copy,
                  201 threads.
                </figcaption>
              </figure></td>
            <td><figure id="fig8h">
                <img src="./img/image8h.png" />
                <figcaption>
                  <strong>(h)</strong> <span class="math-span">ð¥</span>
                  compute, 201 threads.
                </figcaption>
              </figure></td>
            <td><figure id="fig8i">
                <img src="./img/image8i.png" />
                <figcaption>
                  <strong>(i)</strong> <span class="math-span">ð¥</span> update,
                  201 threads.
                </figcaption>
              </figure></td>
          </tr>
        </tbody>
        <caption>
          <strong>Figure 8.</strong> <span class="small-caps">Race</span> copy,
          compute, and update histograms with kernel fits.
        </caption>
      </table>
    </section>

    <section id="sect5.2">
      <h1>Implementation 2: Finite Difference Jacobi Solver</h1>
      <p>
        This second implementation performs the Jacobi relaxation on the grid
        directly using the neighboring points required by the 5-point stencil as
        opposed to explicitly forming the matrix <span class="math-span">ð´</span>,
        and in a sense implements a <em>matrix-free</em> solution. For this
        implementation, the Laplacian was discretized over a <span
          class="math-span">600 Ã 600</span> grid with boundary conditions set
        according to <a href="#tab2">Table 2</a>.
      </p>
      <table id="tab2" class="table-matrix bottom">
        <tbody>
          <tr class="border_top_bottom">
            <td class="border_left_right">0</td>
            <td class="border_left_right">100</td>
            <td>â¦</td>
            <td>â¦</td>
            <td class="border_left_right">100</td>
            <td class="border_left_right">0</td>
          </tr>
          <tr class="border_top_bottom">
            <td class="border_left_right">75</td>
            <td class="border_left_right">XXX</td>
            <td>â¦</td>
            <td>â¦</td>
            <td class="border_left_right">XXX</td>
            <td class="border_left_right">50</td>
          </tr>
          <tr>
            <td class="border_left_right">â®</td>
            <td class="border_left_right">XXX</td>
            <td>â¦</td>
            <td>â¦</td>
            <td class="border_left_right">XXX</td>
            <td class="border_left_right">â®</td>
          </tr>
          <tr>
            <td class="border_left_right">â®</td>
            <td class="border_left_right">XXX</td>
            <td>â¦</td>
            <td>â¦</td>
            <td class="border_left_right">XXX</td>
            <td class="border_left_right">â®</td>
          </tr>
          <tr class="border_top_bottom">
            <td class="border_left_right">75</td>
            <td class="border_left_right">XXX</td>
            <td>â¦</td>
            <td>â¦</td>
            <td class="border_left_right">XXX</td>
            <td class="border_left_right">50</td>
          </tr>
          <tr class="border_top_bottom">
            <td class="border_left_right">0</td>
            <td class="border_left_right">0</td>
            <td>â¦</td>
            <td>â¦</td>
            <td class="border_left_right">0</td>
            <td class="border_left_right">0</td>
          </tr>
        </tbody>
        <caption class="multi-line-html multi-line-pdf">
          <strong>Table 2.</strong> Boundary conditions for the second
          implementation of the Laplacian.
        </caption>
      </table>
      <p>
        The implementation used here stems from code provided by (<a
          role="doc-biblioref" href="#HW10">Hager &amp; Wellein, 2010</a>);
        similar code solves a three dimensional discretization of the Laplacian
        in the study featured in (<a role="doc-biblioref" href="#BBDH11">Bethune,
          Bull, Dingle, &amp; Higham, 2011</a>) and (<a role="doc-biblioref"
          href="#BBDH14">Bethune, Bull, Dingle, &amp; Higham, 2014</a>). The
        routine solves a heat diffusion problem, in which a two-dimensional
        heated plate has Dirichlet boundary-condition temperatures. Two
        matrices, <span class="math-span">ð¢<sub>0</sub></span> and <span
          class="math-span">ð¢<sub>1</sub></span>, store grid point values that
        each thread reads, e.g., from <span class="math-span">ð¢<sub>1</sub></span>,
        to compute newer values to write, e.g., to <span class="math-span">ð¢<sub>0</sub></span>.
        As the method is asynchronous, each thread independently determines
        which matrix stores its newer <span class="math-span">ð¢<sup>(ð¡+1)</sup>(ð,
          ð)
        </span> values and older <span class="math-span">ð¢<sup>(ð¡)</sup>(ð,
          ð)
        </span> values. For an <span class="math-span">ð + 2</span> by <span
          class="math-span">ð + 2</span> grid, each thread solves for <span
          class="math-span">ð<sup>2</sup></span> grid points divided by <span
          class="math-span">ð</span> processing elements, such that the grid is
        evenly divided along the y-axis. When a thread copies grid point values
        above or below its domain for the computation, OpenMP<sup>Â®</sup> locks
        are employed to ensure that data is safely captured from a single
        iteration. Further, locks are used when updating values on domain
        boundaries. Each thread <span class="math-span">ð<sub>ð</sub></span>
        computes its local residual value every <span class="math-span">ð<sup>th</sup></span>
        iteration, which it contributes to the global residual value using an
        OpenMP<sup>Â®</sup> atomic operation, such that it adds the local
        residual from the current iteration and subtracts the local residual
        from the previous iteration. A single thread checks for convergence with
        an atomic capture operation, and updates a shared flag variable if the
        criterion is satisfied. Pseudocode for this implementation is provided
        in <a href="#alg4">Algorithm 4</a>, where bold upper-case text indicates
        that OpenMP<sup>Â®</sup> locks are employed. Locks are used only with
        interior boundary rows, meaning they are unnecessary for the first and
        last rows in the domain.
      </p>
      <p>In this implementation, data was collected only for the time to
        complete an iteration. Thread counts of 10, 25, 50, 75, 100, and 150
        were used in these series of experiments. The average total iteration
        time for the varying.</p>
      <div id="alg4" class="algorithm top">
        <div class="ps-root">
          <div class="ps-algorithm with-caption">
            <p class="ps-line" style="text-indent: -1.2em; padding-left: 1.2em;">
              <span class="ps-keyword">Algorithm 4 </span>OpenMP Implementation
              2
            </p>
            <div class="ps-algorithmic with-linenum">
              <div class="ps-block" style="margin-left: 1.7999999999999998em;">
                <p class="ps-line ps-code">
                  <span class="ps-linenum" style="left: 0em;">1:</span><span
                    style="font-weight: bold;">Input:</span> Initial guess for
                  <math xmlns="http://www.w3.org/1998/Math/MathML">
                      <msup>
                      <mi>u</mi>
                      <mfenced open="(" close=")">
                      <mn>0</mn>
                      </mfenced>
                      </msup>
                      <mo maxsize="1">(</mo>
                      <mi>i</mi>
                      <mo separator="true">,</mo>
                      <mi>j</mi>
                      <mo maxsize="1">)</mo>
                    </math>
                  ,
                  <math xmlns="http://www.w3.org/1998/Math/MathML">
                      <mi>n</mi>
                    </math>
                  processing elements
                  <math xmlns="http://www.w3.org/1998/Math/MathML">
                      <mi>p</mi>
                    </math>
                </p>
                <p class="ps-line ps-code">
                  <span class="ps-linenum" style="left: 0em;">2:</span><span
                    style="font-weight: bold;">Output:</span> Solution vector
                  <math xmlns="http://www.w3.org/1998/Math/MathML">
                      <mi>u</mi>
                      <mo maxsize="1">(</mo>
                      <mi>i</mi>
                      <mo separator="true">,</mo>
                      <mi>j</mi>
                      <mo maxsize="1">)</mo>
                    </math>
                </p>
                <p class="ps-line ps-code">
                  <span class="ps-linenum" style="left: 0em;">3:</span>Assign
                  rows
                  <math xmlns="http://www.w3.org/1998/Math/MathML">
                    <mi>u</mi>
                    <mo maxsize="1">(</mo>
                    <mi>i</mi>
                    <mo maxsize="1">)</mo>
                    <mo>â</mo>
                    <mi>u</mi>
                  </math>
                  to each processing element,
                  <math xmlns="http://www.w3.org/1998/Math/MathML">
                    <mi>i</mi>
                    <mo>=</mo>
                    <mfenced open="[" close="]">
                    <mi>Î±</mi>
                    <mi>Ï</mi>
                    </mfenced>
                  </math>
                </p>
                <p class="ps-line ps-code">
                  <span class="ps-linenum" style="left: 0em;">4:</span><span
                    class="ps-keyword">for </span><span
                    style="font-weight: bold;">parallel</span> each processing
                  element in
                  <math xmlns="http://www.w3.org/1998/Math/MathML">
                    <msub>
                    <mi>p</mi>
                    <mn>1</mn>
                    </msub>
                    <mo>â¦</mo>
                    <msub>
                    <mi>p</mi>
                    <mi>n</mi>
                    </msub>
                  </math>
                  <span class="ps-keyword"> do</span>
                </p>
                <div class="ps-block" style="margin-left: 1.2em;">
                  <p class="ps-line ps-code">
                    <span class="ps-linenum" style="left: -1.5em;">5:</span><span
                      class="ps-keyword">while </span>residual norm
                    <math xmlns="http://www.w3.org/1998/Math/MathML">
                      <mo>&gt;</mo>
                    </math>
                    tolerance<span class="ps-keyword"> do</span>
                  </p>
                  <div class="ps-block" style="margin-left: 1.2em;">
                    <p class="ps-line ps-code">
                      <span class="ps-linenum" style="left: -3em;">6:</span><span
                        class="ps-keyword">for </span>row index
                      <math xmlns="http://www.w3.org/1998/Math/MathML">
                          <mi>i</mi>
                          <mo>=</mo>
                          <mi>Î±</mi>
                          <mo>â¦</mo>
                          <mi>Ï</mi>
                        </math>
                      <span class="ps-keyword"> do</span>
                    </p>
                    <div class="ps-block" style="margin-left: 1.2em;">
                      <p class="ps-line ps-code">
                        <span class="ps-linenum" style="left: -4.5em;">7:</span><span
                          class="ps-keyword">if </span>
                        <math xmlns="http://www.w3.org/1998/Math/MathML">
                            <mi>i</mi>
                            <mo>â </mo>
                            <mn>1</mn>
                          </math>
                        AND
                        <math xmlns="http://www.w3.org/1998/Math/MathML">
                            <mi>i</mi>
                            <mo>â </mo>
                            <mn>N</mn>
                          </math>
                        <math xmlns="http://www.w3.org/1998/Math/MathML">
                            <mi>i</mi>
                            <mo>=</mo>
                            <mn>Î±</mn>
                          </math>
                        OR
                        <math xmlns="http://www.w3.org/1998/Math/MathML">
                            <mi>i</mi>
                            <mo>=</mo>
                            <mn>Ï</mn>
                          </math>
                        <span class="ps-keyword"> then</span>
                      </p>
                      <div class="ps-block" style="margin-left: 1.2em;">
                        <p class="ps-line ps-code">
                          <span class="ps-linenum" style="left: -6em;">8:</span><span
                            style="font-weight: bold;">COPY</span> neighbor
                          <math xmlns="http://www.w3.org/1998/Math/MathML">
                              <msub>
                              <mi>p</mi>
                              <mrow>
                              <mi>n</mi>
                              <mo>â</mo>
                              <mn>1</mn>
                              </mrow>
                              </msub>
                            </math>
                          or
                          <math xmlns="http://www.w3.org/1998/Math/MathML">
                              <msub>
                              <mi>p</mi>
                              <mrow>
                              <mi>n</mi>
                              <mo>+</mo>
                              <mn>1</mn>
                              </mrow>
                              </msub>
                            </math>
                          boundary row values
                          <math xmlns="http://www.w3.org/1998/Math/MathML">
                              <msup>
                              <mi>u</mi>
                              <mfenced open="(" close=")">
                              <mi>t</mi>
                              </mfenced>
                              </msup>
                              <mo maxsize="1">(</mo>
                              <mi>i</mi>
                              <mo separator="true">,</mo>
                              <mi>j</mi>
                              <mo maxsize="1">)</mo>
                            </math>
                          for
                          <math xmlns="http://www.w3.org/1998/Math/MathML">
                              <msup>
                              <mi>u</mi>
                              <mfenced open="(" close=")">
                              <mrow>
                              <mi>t</mi>
                              <mo>+</mo>
                              <mn>1</mn>
                              </mrow>
                              </mfenced>
                              </msup>
                              <mo maxsize="1">(</mo>
                              <mi>i</mi>
                              <mo separator="true">,</mo>
                              <mi>j</mi>
                              <mo maxsize="1">)</mo>
                            </math>
                        </p>
                      </div>
                      <p class="ps-line ps-code">
                        <span class="ps-linenum" style="left: -4.5em;">9:</span><span
                          class="ps-keyword">end if</span>
                      </p>
                      <p class="ps-line ps-code">
                        <span class="ps-linenum" style="left: -4.5em;">10:</span>Compute
                        <math xmlns="http://www.w3.org/1998/Math/MathML">
                          <mrow>
                          <msup>
                          <mi>u</mi>
                          <mfenced open="(" close=")">
                          <mrow>
                          <mi>t</mi>
                          <mo>+</mo>
                          <mn>1</mn>
                          </mrow>
                          </mfenced>
                          </msup>
                          <mo maxsize="1">(</mo>
                          <mi>i</mi>
                          <mo separator="true">,</mo>
                          <mi>j</mi>
                          <mo maxsize="1">)</mo>
                          <mo>=</mo>
                          <mfrac>
                          <mn>1</mn>
                          <mn>4</mn>
                          </mfrac>
                          <mo>â</mo>
                          <mfenced open="(" close=")">
                          <mrow>
                          <mrow>
                          <msup>
                          <mi>u</mi>
                          <mfenced open="(" close=")">
                          <mi>t</mi>
                          </mfenced>
                          </msup>
                          <mo maxsize="1">(</mo>
                          <mrow>
                          <mi>i</mi>
                          <mo>+</mo>
                          <mn>1</mn>
                          </mrow>
                          <mo separator="true">,</mo>
                          <mi>j</mi>
                          <mo maxsize="1">)</mo>
                          </mrow>
                          <mo>+</mo>
                          <mrow>
                          <msup>
                          <mi>u</mi>
                          <mfenced open="(" close=")">
                          <mi>t</mi>
                          </mfenced>
                          </msup>
                          <mo maxsize="1">(</mo>
                          <mrow>
                          <mi>i</mi>
                          <mo>â</mo>
                          <mn>1</mn>
                          </mrow>
                          <mo separator="true">,</mo>
                          <mi>j</mi>
                          <mo maxsize="1">)</mo>
                          </mrow>
                          <mo>+</mo>
                          <mrow>
                          <msup>
                          <mi>u</mi>
                          <mfenced open="(" close=")">
                          <mi>t</mi>
                          </mfenced>
                          </msup>
                          <mo maxsize="1">(</mo>
                          <mi>i</mi>
                          <mo separator="true">,</mo>
                          <mrow>
                          <mi>j</mi>
                          <mo>+</mo>
                          <mn>1</mn>
                          </mrow>
                          <mo maxsize="1">)</mo>
                          </mrow>
                          <mo>+</mo>
                          <mrow>
                          <msup>
                          <mi>u</mi>
                          <mfenced open="(" close=")">
                          <mi>t</mi>
                          </mfenced>
                          </msup>
                          <mo maxsize="1">(</mo>
                          <mi>i</mi>
                          <mo separator="true">,</mo>
                          <mrow>
                          <mi>j</mi>
                          <mo>â</mo>
                          <mn>1</mn>
                          </mrow>
                          <mo maxsize="1">)</mo>
                          </mrow>
                          </mrow>
                          </mfenced>
                          </mrow>
                        </math>
                      </p>
                      <p class="ps-line ps-code">
                        <span class="ps-linenum" style="left: -4.5em;">11:</span><span
                          class="ps-keyword">if </span>
                        <math xmlns="http://www.w3.org/1998/Math/MathML">
                            <mi>i</mi>
                            <mo>â </mo>
                            <mn>1</mn>
                          </math>
                        AND
                        <math xmlns="http://www.w3.org/1998/Math/MathML">
                            <mi>i</mi>
                            <mo>â </mo>
                            <mn>N</mn>
                          </math>
                        <math xmlns="http://www.w3.org/1998/Math/MathML">
                            <mi>i</mi>
                            <mo>=</mo>
                            <mn>Î±</mn>
                          </math>
                        OR
                        <math xmlns="http://www.w3.org/1998/Math/MathML">
                            <mi>i</mi>
                            <mo>=</mo>
                            <mn>Ï</mn>
                          </math>
                        <span class="ps-keyword"> then</span>
                      </p>
                      <div class="ps-block" style="margin-left: 1.2em;">
                        <p class="ps-line ps-code">
                          <span class="ps-linenum" style="left: -6em;">12:</span><span
                            style="font-weight: bold;">UPDATE</span> own
                          <math xmlns="http://www.w3.org/1998/Math/MathML">
                            <msub>
                            <mi>p</mi>
                            <mi>n</mi>
                            </msub>
                          </math>
                          boundary row values
                          <math xmlns="http://www.w3.org/1998/Math/MathML">
                            <msubsup>
                            <mi>u</mi>
                            <mi>j</mi>
                            <mrow>
                            <mfenced open="(" close=")">
                            <mi>t</mi>
                            </mfenced>
                            </mrow>
                            </msubsup>
                            <mo maxsize="1">(</mo>
                            <mi>i</mi>
                            <mo separator="true">,</mo>
                            <mi>j</mi>
                            <mo maxsize="1">)</mo>
                          </math>
                          in shared memory with
                          <math xmlns="http://www.w3.org/1998/Math/MathML">
                            <msubsup>
                            <mi>u</mi>
                            <mi>j</mi>
                            <mrow>
                            <mfenced open="(" close=")">
                            <mrow>
                            <mi>t</mi>
                            <mo>+</mo>
                            <mn>1</mn>
                            </mrow>
                            </mfenced>
                            </mrow>
                            </msubsup>
                            <mo maxsize="1">(</mo>
                            <mi>i</mi>
                            <mo separator="true">,</mo>
                            <mi>j</mi>
                            <mo maxsize="1">)</mo>
                          </math>
                        </p>
                      </div>
                      <p class="ps-line ps-code">
                        <span class="ps-linenum" style="left: -4.5em;">13:</span><span
                          class="ps-keyword">end if</span>
                      </p>
                    </div>
                    <p class="ps-line ps-code">
                      <span class="ps-linenum" style="left: -3em;">14:</span><span
                        class="ps-keyword">end for</span>
                    </p>
                  </div>
                  <p class="ps-line ps-code">
                    <span class="ps-linenum" style="left: -1.5em;">15:</span><span
                      class="ps-keyword">end while</span>
                  </p>
                </div>
                <p class="ps-line ps-code">
                  <span class="ps-linenum" style="left: 0em;">16:</span><span
                    class="ps-keyword">end for</span>
                </p>
              </div>
            </div>
          </div>
        </div>
      </div>
      <p>
        <a href="#fig9">Figure 9</a> provides histograms and kernel fits for
        each of the thread counts. <a href="#tab3">Table 3</a> and <a
          href="#fig9">Figure 9</a> show that with increasing thread count, mean
        iteration time decreases, but iteration times variance increases. This
        increase in iteration time variation may result from increased
        opportunities for lock collisions with greater thread counts.
      </p>
      <table id="fig9" class="table-figure top">
        <tr>
          <td><figure id="fig9a">
              <img src="./img/image9a.png" />
              <figcaption>
                <strong>(a)</strong> 10 threads.
              </figcaption>
            </figure></td>
          <td><figure id="fig9b">
              <img src="./img/image9b.png" />
              <figcaption>
                <strong>(b)</strong> 25 threads.
              </figcaption>
            </figure></td>
          <td><figure id="fig9c">
              <img src="./img/image9c.png" />
              <figcaption>
                <strong>(c)</strong> 50 threads.
              </figcaption>
            </figure></td>
        </tr>
        <tr>
          <td><figure id="fig9d">
              <img src="./img/image9d.png" />
              <figcaption>
                <strong>(d)</strong> 75 threads.
              </figcaption>
            </figure></td>
          <td><figure id="fig9e">
              <img src="./img/image9e.png" />
              <figcaption>
                <strong>(e)</strong> 100 threads.
              </figcaption>
            </figure></td>
          <td><figure id="fig9f">
              <img src="./img/image9f.png" />
              <figcaption>
                <strong>(f)</strong> 150 threads.
              </figcaption>
            </figure></td>
        </tr>
        <caption>
          <strong>Figure 9.</strong> Iteration time histograms with kernel fits.
        </caption>
      </table>
      <p>
        Since this implementation is even more compute bound than the first one,
        <a href="#tab3">Table 3</a> shows a general decrease in the time for
        each iteration as the thread count is increased. While there is no
        inflection point evident in the data presented in <a href="#tab3">Table
          3</a>, compared to <span class="small-caps">Race</span> in <a href="#tab1">Table
          1</a>, <a href="#tab3">Table 3</a> still suggests that once the number of
        threads outnumbers physical cores, performance gains diminish. For
        denser matrices, or for different applications on different systems,
        these trends could change as the memory-based activities become
        relatively more expensive. The finite difference discretization of the
        Laplacian is a very sparse matrix that does not require much data
        movement.
      </p>
      <table id="tab3" class="bottom">
        <thead>
          <tr>
            <th>Threads</th>
            <th>Mean<br /> (10<sup>â5</sup>s)
            </th>
            <th>Std.<br /> (10<sup>â6</sup>s)
            </th>
          </tr>
        </thead>
        <tbody style="text-align: right">
          <tr>
            <td style="text-align: center">10</td>
            <td>8.86</td>
            <td>3.87</td>
          </tr>
          <tr>
            <td style="text-align: center">25</td>
            <td>3.92</td>
            <td>2.08</td>
          </tr>
          <tr>
            <td style="text-align: center">50</td>
            <td>2.55</td>
            <td>2.34</td>
          </tr>
          <tr>
            <td style="text-align: center">75</td>
            <td>2.53</td>
            <td>5.80</td>
          </tr>
          <tr>
            <td style="text-align: center">100</td>
            <td>2.61</td>
            <td>5.95</td>
          </tr>
          <tr>
            <td style="text-align: center">150</td>
            <td>2.64</td>
            <td>5.76</td>
          </tr>
        </tbody>
        <caption class="multi-line-html multi-line-pdf">
          <strong>Table 3.</strong> Mean iteration time and standard deviation
          by thread count.
        </caption>
      </table>
    </section>

    <section id="sect5.3">
      <h1>Implementation Comparison</h1>
      <p>
        The <span class="small-caps">Safe</span> variant of the first
        implementation incurs significant overhead costs for <span
          class="math-span">ð¥</span> copy and update operations, as thread
        count increases, because each thread must copy the entire <span
          class="math-span">ð¥</span> vector. In the second implementation, data
        shared between threads is differentiated and specific to domain
        location; therefore, specific locks may be used when copying and
        updating segments of the subdomain. Assuming an appropriate number of
        processing elements for a given grid, i.e., a thread has significantly
        more middle rows than boundary rows, copy operations, and the associated
        variability and costs, are minimal compared with compute operations. The
        <span class="small-caps">Race</span> implementation of the general
        solver eliminates much of the overhead cost from mutex locks, and
        convergence time is satisfactory for the given system. Implementation 2
        is more constrained than Implementation 1, generalizing only to finite
        difference discretizations of partial differential equations over
        rectangular grids. Implementation 1 generalizes further to any sparse
        matrix, <span class="math-span">ð´</span> with which the Jacobi method
        can be used. According to <a href="#theo1">Theorem 1</a>, convergence
        will occur if the spectral radius of the iteration matrix, <span
          class="math-span">ð¶</span>, is less than 1. In the case of the Jacobi
        method, the iteration matrix is given by
      </p>
      <div class="equation">
        <math display="block" xmlns="http://www.w3.org/1998/Math/MathML">
            <mtable>
              <mtr>
                <mtd>
                  <mrow>
                  <mi>C</mi>
                  <mo>=</mo>
                  <msup>
                  <mrow>
                  <mo>â</mo>
                  <mi>D</mi>
                  </mrow>
                  <mn>â1</mn>
                  </msup>
                  <mfenced open="(" close=")">
                  <mrow>
                  <mi>L</mi>
                  <mo>+</mo>
                  <mi>U</mi>
                  </mrow>
                  </mfenced>
                  <mtext maxsize="1" mathvariant="normal">.</mtext>
                  </mrow>
                </mtd>
              </mtr>
            </mtable>
          </math>
      </div>
      <p>
        Note that in the two dimensional discretization of the Laplacian, the
        spectral radius of the Jacobian is less than l, which says that both the
        synchronous and asynchronous variants of the Jacobi algorithm will
        converge. Note <span class="small-caps">Race</span> behavior is unknown
        for different problems and HPC systems.
      </p>
      <p>The purpose of the two distinct implementations is to emphasize
        that the simulation framework proposed here can adapt to the behavior of
        different problems and platforms. The framework may be apdapted to any
        asynchronous iterative method through the process of collecting data
        representative of individual update times and using the resultant data
        to model the system in the framework.</p>
    </section>

    <section id="sect5.4">
      <h1>Framework Validation</h1>
      <p>
        To validate the performance of the simulation framework when initialized
        with appropriate distributions, a case study utilizing output from
        Implementation 1 (see <a href="#sect5.1">Section 5.1</a> for details)
        was considered. Data was collected for a smaller problem size only in
        order to facilitate the collection of data over a large number of runs.
        Specifically, the Laplacian was discretized over a <span
          class="math-span">20 Ã 20</span> grid resulting in a matrix of size <span
          class="math-span">400 Ã 400</span>. Similarly to the process in <a
          href="#sect5.1">Section 5.1</a>, distributions were fit to the output
        of the OpenMP<sup>Â®</sup> implementation, and these distributions were
        used in the simulation framework to provide update times to the
        simulated processors that are reflective of the HPC hardware that the
        data was collected on. Output from the average of these runs is provided
        in <a href="#tab4">Table 4</a>. The leftmost column provides the number
        of threads that were used (or simulated), the middle column shows the
        average over multiple runs of the parallel implementation, and the
        rightmost column shows the average over multiple runs of the simulation
        generated by the simulation framework. In the case of this small
        problem, the similarity of actual and simulated run times helps to
        validate the model. Running multiple trials of larger problems in the
        framework is currently time-prohibitive, which is an issue that may be
        improved with framework implementation changes. Future work includes
        model validation for other problems and larger problems.
      </p>
      <table id="tab4" class="top">
        <thead>
          <tr>
            <th>Thread<br /> Count
            </th>
            <th>Run<br /> Average (s)
            </th>
            <th>Simulation<br /> Average (s)
            </th>
          </tr>
        </thead>
        <tbody style="text-align: right">
          <tr>
            <td style="text-align: center">11</td>
            <td>0.01</td>
            <td>0.01</td>
          </tr>
          <tr>
            <td style="text-align: center">21</td>
            <td>0.02</td>
            <td>0.02</td>
          </tr>
          <tr>
            <td style="text-align: center">41</td>
            <td>0.04</td>
            <td>0.04</td>
          </tr>
          <tr>
            <td style="text-align: center">51</td>
            <td>0.04</td>
            <td>0.05</td>
          </tr>
          <tr>
            <td style="text-align: center">81</td>
            <td>0.09</td>
            <td>0.09</td>
          </tr>
          <tr>
            <td style="text-align: center">101</td>
            <td>0.12</td>
            <td>0.12</td>
          </tr>
          <tr>
            <td style="text-align: center">201</td>
            <td>0.34</td>
            <td>0.35</td>
          </tr>
        </tbody>
        <caption class="multi-line-html multi-line-pdf">
          <strong>Table 4.</strong> Comparisons of run times between parallel
          executions and simulation.
        </caption>
      </table>
    </section>
  </section>

  <section id="sect6">
    <h1>Framework Extension for Fault-Tolerance Requirements</h1>
    <p>
      The modular nature of this framework allows for extra functionality to be
      easily added to the framework itself that can be used to adapt the base
      algorithm to suit a specific set of requirements. With the projected
      increase of faults (see the references in <a href="#sect2">Section 2</a>),
      development of fault tolerant algorithms is an important endeavor. A block
      diagram showing the additional functionality dealing with fault-tolerance
      is shown in <a href="#fig10">Figure 10</a>. The new functionality is
      achieved by passing in another function handle that performs the fault
      tolerance check and recovery work.
    </p>
    <figure id="fig10" class="bottom">
      <img src="./img/image10.png" />
      <figcaption class="multi-line-html multi-line-pdf">
        <strong>Figure 10.</strong> Block diagram of the simulation framework
        with added support for fault tolerance mechanisms.
      </figcaption>
    </figure>
    <p>
      The contents of the newly added <em>Fault tolerance check</em> module may
      be organized as follows: Each processor makes a call to find the global
      residual and rolls the state back to the previous known good state if the
      behavior of the residual is not as expected. See <a href="#sect6.2">Section
        6.2</a> for more details. Note that this strategy is not being advocated for
      due to its optimality, but is being shown as an example of how to extend
      the framework for algorithm development. Techniques such as monitoring the
      progression of the component-wise residuals (e.g., (<a
        role="doc-biblioref" href="#ADQO15">Anzt, Dongarra, &amp;
        Quintana-OrtÃ­, 2015</a>; <a role="doc-biblioref" href="#ADQO16">Anzt,
        Dongarra, &amp; Quintana-OrtÃ­, 2016</a>)) or only rolling back portions of
      the state vector (e.g., (<a role="doc-biblioref" href="#CS17">Coleman
        &amp; Sosonkina, 2017</a>; <a role="doc-biblioref" href="#CS18">Coleman
        &amp; Sosonkina, 2018</a>)) would probably be more computationally
      efficient.
    </p>

    <section id="sect6.1">
      <h1>Fault Model</h1>
      <p>
        For this part of the study, faults are modeled as perturbations similar
        to several recent studies (<a role="doc-biblioref" href="#CS16b">Coleman
          &amp; Sosonkina, 2016b</a>; <a role="doc-biblioref" href="#CSC17">Coleman,
          Sosonkina, &amp; Chow, 2017</a>; <a role="doc-biblioref" href="#SW15">Stoyanov
          &amp; Webster, 2015</a>); the goal being producing fault tolerant
        algorithms for future computing platforms that are not too dependent on
        the precise mechanism of a fault (e.g., bit flip). Modifying the
        perturbation-based fault model described in (<a role="doc-biblioref"
          href="#CSC17">Coleman, Sosonkina, &amp; Chow, 2017</a>), a single data
        structure is targeted and a small random perturbation is injected into
        each component transiently. For example, if the targeted data structure
        is a vector <span class="math-span">ð¥</span> and the maximum size of
        the perturbation-based fault is <span class="math-span">Îµ</span> then
        proceed as follows: sample a random number <span class="math-span">ð<sub>ð</sub>
          â (-Îµ, Îµ)
        </span>, using a uniform distribution, and then set
      </p>
      <div class="equation">
        <math display="block" xmlns="http://www.w3.org/1998/Math/MathML">
            <mtable>
              <mtr>
                <mtd>
                  <mrow>
                  <msub>
                  <mover accent="true">
                  <mi>x</mi>
                  <mo>^</mo>
                  </mover>
                  <mi>i</mi>
                  </msub>
                  <mo>=</mo>
                  <msub>
                  <mi>x</mi>
                  <mi>i</mi>
                  </msub>
                  <mo>+</mo>
                  <msub>
                  <mi>r</mi>
                  <mi>i</mi>
                  </msub>
                  </mrow>
                </mtd>
              </mtr>
            </mtable>
          </math>
      </div>
      <p>
        for all values of <span class="math-span">ð</span>. The resultant
        vector
        <math xmlns="http://www.w3.org/1998/Math/MathML">
      <mover accent="true">
      <mi>x</mi>
      <mo style="font-size: small;">â</mo>
      </mover>
      </math>
        is then perturbed away from the original vector <span class="math-span">ð¥</span>.
        Other similar perturbation-based fault models have sampled the
        components <span class="math-span">ð<sub>ð</sub></span> from different
        ranges. This can allow the creation of scenarios where some components
        are perturbed by large amounts, and some are only changed incrementally.
      </p>
      <p>In this study, faults are injected into the asynchronous Jacobi
        algorithm following the perturbation based methodology described above.
        Due to the relatively short execution time of the asynchronous Jacobi
        algorithm on the given test problems, a fault is induced only once
        during each run, and the fault is designated to occur at a random
        iteration number before convergence. To be precise â since "iteration"
        loses some meaning in an asynchronous iterative algorithm â the fault is
        injected on a single simulated time before the algorithm terminates. It
        is not necessary for the program to have an update scheduled on the same
        simulated time for the fault to be injected.</p>
    </section>

    <section id="sect6.2">
      <h1>Experiments with the Fault-Tolerance Module</h1>
      <p>
        Similar to the earlier results in the paper, this study covers the
        solution of the linear system resulting from a two-dimensional finite
        difference discretization of the Laplacian. Before presenting simulation
        results, it is important to note that faults, as modeled here, will not
        prevent the <em>eventual</em> solution of the linear system using the
        (asynchronous) Jacobi method. Since the spectral radius of the
        associated iteration matrix is strictly less than 1, it will converge
        for any initial guess <span class="math-span">ð¥<sup>(0)</sup></span>.
      </p>
      <p>
        Since faults are assumed to only affect the memory storing the vector <span
          class="math-span">ð¥</span> and are assumed to occur in a transient
        manner, if a fault occurs on iteration <span class="math-span">ð¹</span>
        then the subsequent iterate, <span class="math-span">ð¥<sup>(ð¹+1)</sup></span>
        can be taken to be the new starting iterate and eventual convergence is
        guaranteed due to the iteration matrix which has remained the same
        throughout the occurrence of the fault. This model can reflect the
        scenario where certain parts of the routine are designated to run on
        hardware with a higher reliability threshold, and other parts of the
        algorithm are allowed to run on hardware that may be more susceptible to
        the occurrence of a fault. This sandbox type design has been suggested
        as a possible means for providing energy efficient fault tolerance on
        future HPC environments (<a role="doc-biblioref" href="#BFHH12">Bridges,
          Ferreira, Heroux, &amp; Hoemmen, 2012</a>; <a role="doc-biblioref"
          href="#HH11">Hoemmen &amp; Heroux, 2011</a>; <a role="doc-biblioref"
          href="#SV13">Sao &amp; Vuduc, 2013</a>).
      </p>
      <p>
        While eventual convergence may be guaranteed, greatly accelerated
        convergence is possible through a simple checkpointing scheme. An
        example of such a scheme (as an extension of the asynchronous Jacobi
        simulation provided by <a href="#alg2">Algorithm 2</a>) is provided in <a
          href="#alg5">Algorithm 5</a>.
      </p>
      <div id="alg5" class="algorithm top">
        <div class="ps-root">
          <div class="ps-algorithm with-caption">
            <p class="ps-line" style="text-indent: -1.2em; padding-left: 1.2em;">
              <span class="ps-keyword">Algorithm 5 </span>Asynchronous Jacobi
              simulation with checkpointing
            </p>
            <div class="ps-algorithmic with-linenum">
              <div class="ps-block" style="margin-left: 1.7999999999999998em;">
                <p class="ps-line ps-code">
                  <span class="ps-linenum" style="left: 0em;">1:</span><span
                    style="font-weight: bold;">Input:</span>
                  <math xmlns="http://www.w3.org/1998/Math/MathML">
                      <msub>
                      <mi>a</mi>
                      <mrow>
                      <mi>i</mi>
                      <mi>j</mi>
                      </mrow>
                      </msub>
                      <mo>â</mo>
                      <mi>A</mi>
                    </math>
                  ; initial guess
                  <math xmlns="http://www.w3.org/1998/Math/MathML">
                      <msub>
                      <mi>x</mi>
                      <mn>0</mn>
                      </msub>
                    </math>
                  ; number of processing elements
                  <math xmlns="http://www.w3.org/1998/Math/MathML">
                    <mi>p</mi>
                  </math>
                  ; input random number distribution; checkpointing tolerance
                  <math xmlns="http://www.w3.org/1998/Math/MathML">
                    <mi>Î±</mi>
                  </math>
                  ; checkpointing frequency
                  <math xmlns="http://www.w3.org/1998/Math/MathML">
                    <mi>Ï</mi>
                  </math>
                </p>
                <p class="ps-line ps-code">
                  <span class="ps-linenum" style="left: 0em;">2:</span><span
                    style="font-weight: bold;">Output:</span> Solution vector
                  <math xmlns="http://www.w3.org/1998/Math/MathML">
                      <mi>x</mi>
                  </math>
                </p>
                <p class="ps-line ps-code">
                  <span class="ps-linenum" style="left: 0em;">3:</span>Assign
                  processor update times
                  <math xmlns="http://www.w3.org/1998/Math/MathML">
                    <msub>
                    <mi>Ï</mi>
                    <mn>1</mn>
                    </msub>
                    <mo separator="true">,</mo>
                    <msub>
                    <mi>Ï</mi>
                    <mn>2</mn>
                    </msub>
                    <mo separator="true">,</mo>
                    <mo>â¦</mo>
                    <mo separator="true">,</mo>
                    <msub>
                    <mi>Ï</mi>
                    <mi>p</mi>
                    </msub>
                  </math>
                  by sampling from an appropriate random number distribution
                </p>
                <p class="ps-line ps-code">
                  <span class="ps-linenum" style="left: 0em;">4:</span>Assign a
                  part of
                  <math xmlns="http://www.w3.org/1998/Math/MathML">
                    <mi>x</mi>
                  </math>
                  to each processing element
                </p>
                <p class="ps-line ps-code">
                  <span class="ps-linenum" style="left: 0em;">5:</span>Initialize
                  <math xmlns="http://www.w3.org/1998/Math/MathML">
                    <msub>
                    <mi>r</mi>
                    <mtext>old</mtext>
                    </msub>
                  </math>
                  to a large value
                </p>
                <p class="ps-line ps-code">
                  <span class="ps-linenum" style="left: 0em;">6:</span><span
                    class="ps-keyword">for </span>
                  <math xmlns="http://www.w3.org/1998/Math/MathML">
                      <mi>t</mi>
                      <mo>=</mo>
                      <mn>1</mn>
                      <mo separator="true">,</mo>
                      <mn>2</mn>
                      <mo separator="true">,</mo>
                      <mo>â¦</mo>
                    </math>
                  , until convergence<span class="ps-keyword"> do</span>
                </p>
                <div class="ps-block" style="margin-left: 1.2em;">
                  <p class="ps-line ps-code">
                    <span class="ps-linenum" style="left: -1.5em;">7:</span><span
                      class="ps-keyword">for </span>each processing element,
                    <math xmlns="http://www.w3.org/1998/Math/MathML">
                        <msub>
                        <mi>P</mi>
                        <mi>l</mi>
                        </msub>
                      </math>
                    <span class="ps-keyword"> do</span>
                  </p>
                  <div class="ps-block" style="margin-left: 1.2em;">
                    <p class="ps-line ps-code">
                      <span class="ps-linenum" style="left: -3em;">8:</span><span
                        class="ps-keyword">if </span>
                      <math xmlns="http://www.w3.org/1998/Math/MathML">
                        <msub>
                        <mi>Ï</mi>
                        <mi>k</mi>
                        </msub>
                        <mo>=</mo>
                        <mi>t</mi>
                      </math>
                      <span class="ps-keyword"> then</span>
                    </p>
                    <div class="ps-block" style="margin-left: 1.2em;">
                      <p class="ps-line ps-code">
                        <span class="ps-linenum" style="left: -4.5em;">9:</span><span
                          class="ps-keyword">for </span>each element
                        <math xmlns="http://www.w3.org/1998/Math/MathML">
                            <msub>
                            <mi>x</mi>
                            <mi>i</mi>
                            </msub>
                            <mo>â</mo>
                            <mi>x</mi>
                          </math>
                        assigned to
                        <math xmlns="http://www.w3.org/1998/Math/MathML">
                            <msub>
                            <mi>P</mi>
                            <mi>l</mi>
                            </msub>
                          </math>
                        <span class="ps-keyword"> do</span>
                      </p>
                      <div class="ps-block" style="margin-left: 1.2em;">
                        <p class="ps-line ps-code">
                          <span class="ps-linenum" style="left: -6em;">10:</span>
                          <math xmlns="http://www.w3.org/1998/Math/MathML">
                                <msub>
                                <mi>x</mi>
                                <mi>i</mi>
                                </msub>
                                <mo>=</mo>
                                <mfrac>
                                <mrow>
                                <mo>â</mo>
                                <mn>1</mn>
                                </mrow>
                                <msub>
                                <mi>a</mi>
                                <mrow>
                                <mi>i</mi>
                                <mi>i</mi>
                                </mrow>
                                </msub>
                                </mfrac>
                                <mfenced open="[" close="]">
                                <mrow>
                                <msub>
                                <mo>â</mo>
                                <mrow>
                                <mi>j</mi>
                                <mo>â </mo>
                                <mi>i</mi>
                                </mrow>
                                </msub>
                                <msub>
                                <mi>a</mi>
                                <mrow>
                                <mi>i</mi>
                                <mi>j</mi>
                                </mrow>
                                </msub>
                                <msub>
                                <mi>x</mi>
                                <mi>j</mi>
                                </msub>
                                <mo>â</mo>
                                <msub>
                                <mi>b</mi>
                                <mi>i</mi>
                                </msub>
                                </mrow>
                                </mfenced>
                              </math>
                        </p>
                      </div>
                      <p class="ps-line ps-code">
                        <span class="ps-linenum" style="left: -4.5em;">11:</span><span
                          class="ps-keyword">end for</span>
                      </p>
                      <p class="ps-line ps-code">
                        <span class="ps-linenum" style="left: -4.5em;">12:</span>Retrieve
                        a new update time
                        <math xmlns="http://www.w3.org/1998/Math/MathML">
                          <msub>
                          <mi>Ï</mi>
                          <mi>k</mi>
                          </msub>
                      </math>
                        by sampling from the input distribution
                      </p>
                    </div>
                    <p class="ps-line ps-code">
                      <span class="ps-linenum" style="left: -3em;">13:</span><span
                        class="ps-keyword">end if</span>
                    </p>
                  </div>
                  <p class="ps-line ps-code">
                    <span class="ps-linenum" style="left: -1.5em;">14:</span><span
                      class="ps-keyword">end for</span>
                  </p>
                  <p class="ps-line ps-code">
                    <span class="ps-linenum" style="left: -1.5em;">15:</span>Inject
                    a fault if appropriate
                  </p>
                  <p class="ps-line ps-code">
                    <span class="ps-linenum" style="left: -1.5em;">16:</span>Calculate
                    the residual
                    <math xmlns="http://www.w3.org/1998/Math/MathML">
                      <msub>
                      <mi>r</mi>
                      <mtext>new</mtext>
                      </msub>
                    </math>
                    as in <a href="#eq3">Equation (3)</a>
                  </p>
                  <p class="ps-line ps-code">
                    <span class="ps-linenum" style="left: -1.5em;">17:</span><span
                      class="ps-keyword">if </span>
                    <math xmlns="http://www.w3.org/1998/Math/MathML">
                      <msub>
                      <mi>r</mi>
                      <mtext>new</mtext>
                      </msub>
                      <mo>&gt;</mo>
                      <mi>Î±</mi>
                      <mo>x</mo>
                      <msub>
                      <mi>r</mi>
                      <mtext>old</mtext>
                      </msub>
                    </math>
                    <span class="ps-keyword"> then</span>
                  </p>
                  <div class="ps-block" style="margin-left: 1.2em;">
                    <p class="ps-line ps-code">
                      <span class="ps-linenum" style="left: -3em;">18:</span>
                      <math xmlns="http://www.w3.org/1998/Math/MathML">
                        <mi>x</mi>
                        <mo>â</mo>
                        <msub>
                        <mi>x</mi>
                        <mtext>cp</mtext>
                        </msub>
                      </math>
                    </p>
                  </div>
                  <p class="ps-line ps-code">
                    <span class="ps-linenum" style="left: -1.5em;">19:</span><span
                      class="ps-keyword">end if</span>
                  </p>
                  <p class="ps-line ps-code">
                    <span class="ps-linenum" style="left: -1.5em;">20:</span><span
                      class="ps-keyword">if </span>
                    <math xmlns="http://www.w3.org/1998/Math/MathML">
                        <mtext mathvariant="">mod</mtext>
                        <mo maxsize="1">(</mo>
                        <mi>t</mi>
                        <mo separator="true">,</mo>
                        <mi>Ï</mi>
                        <mo maxsize="1">)</mo>
                        <mo>==</mo>
                        <mn>0</mn>
                      </math>
                    <span class="ps-keyword"> then</span>
                  </p>
                  <div class="ps-block" style="margin-left: 1.2em;">
                    <p class="ps-line ps-code">
                      <span class="ps-linenum" style="left: -3em;">21:</span>
                      <math xmlns="http://www.w3.org/1998/Math/MathML">
                        <msub>
                        <mi>x</mi>
                        <mtext>cp</mtext>
                        </msub>
                        <mo>â</mo>
                        <mi>x</mi>
                      </math>
                    </p>
                  </div>
                  <p class="ps-line ps-code">
                    <span class="ps-linenum" style="left: -1.5em;">22:</span><span
                      class="ps-keyword">end if</span>
                  </p>
                  <p class="ps-line ps-code">
                    <span class="ps-linenum" style="left: -1.5em;">23:</span>Check
                    termination conditions
                  </p>
                </div>
                <p class="ps-line ps-code">
                  <span class="ps-linenum" style="left: 0em;">24:</span><span
                    class="ps-keyword">end for</span>
                </p>
              </div>
            </div>
          </div>
        </div>
      </div>
      <p>
        Note that the asynchronous nature of the iterative method means that a
        strict check on the decrease of the residual (i.e., expecting monotonic
        decrease) is not possible. In particular, the checkpointing tolerance <span
          class="math-span">Î±</span> needs to be taken such that <span
          class="math-span">Î± &gt; 1</span>. However, the expected manifestation
        of faults as rare, transient events allows Î± to be taken fairly large.
        Taking <span class="math-span">Î±</span> too large results in a fault
        having a substantial impact on the convergence rate of algorithm since
        large faults will be allowed to impact the algorithm with no correction.
        Conversely, taking <span class="math-span">Î±</span> too small causes the
        algorithm to checkpoint more frequently than needed. Examples of the
        effects of a fault with different values selected for <span
          class="math-span">Î±</span> are given by <a href="#fig11">Figure 11</a>.
      </p>
      <table id="fig11" class="table-figure top">
        <tr>
          <td>
            <figure id="fig11a">
              <img src="./img/image11a.png" />
              <figcaption>
                <strong>(a)</strong> Effect of fault â no checkpointing.
              </figcaption>
            </figure>
          </td>
          <td><figure id="fig11b">
              <img src="./img/image11b.png" />
              <figcaption>
                <strong>(b)</strong> Effect of fault â <span class="math-span">Î±
                  = 1</span>.
              </figcaption>
            </figure></td>
        </tr>
        <tr>
          <td>
            <figure id="fig11c">
              <img src="./img/image11c.png" />
              <figcaption>
                <strong>(c)</strong> Effect of fault â <span class="math-span">Î±
                  = 2</span>.
              </figcaption>
            </figure>
          </td>
          <td><figure id="fig11d">
              <img src="./img/image11d.png" />
              <figcaption>
                <strong>(d)</strong> Effect of fault â <span class="math-span">Î±
                  = 10</span>.
              </figcaption>
            </figure></td>
        </tr>
        <caption>
          <strong>Figure 11.</strong> Effect of differing values of <span
            class="math-span">Î±</span> on the progression of the residual.
        </caption>
      </table>
      <p>
        Note in <a href="#fig11">Figure 11</a> that no checkpointing results in
        a delay to convergence relative to the use of checkpointing with either
        <span class="math-span">Î± = 1</span> or <span class="math-span">Î±
          = 10</span>. The size of the fault selected in this study, <span
          class="math-span">ð<sub>ð</sub> â (-100, 100)
        </span>, which may be reflective of an exponent or sign bit flip (<a
          role="doc-biblioref" href="#CS18">Coleman &amp; Sosonkina, 2018</a>),
        results in the values <span class="math-span">Î± = 1</span> and <span
          class="math-span">Î± = 10</span> having the same performance since the
        error induced by the fault is sufficiently large that the new residual
        is more than <span class="math-span">Î± = 10</span> times the prior
        residual. Faults that induce a smaller error may be detected by certain
        values of <span class="math-span">Î±</span> and not by others which would
        lead to differing performance.
      </p>
      <p>
        The residual progress in the plot showing the effects of using <span
          class="math-span">Î± = 1</span> can be explained by the updates
        provided by certain simulated processing elements being rejected despite
        being necessary for the convergence of the algorithm. This can be seen
        in the small, momentary jumps in the progression of the residual visible
        in the other graphs. These rejections lead to stagnation in the
        progression of the algorithm and show why the value of <span
          class="math-span">Î± = 1</span> should not be selected for a
        checkpointing scheme for an asynchronous iterative method.
      </p>
    </section>
  </section>

  <section id="sect7">
    <h1>Conclusions and Future Work</h1>
    <p>This work has developed a framework that can be used to efficiently
      simulate the outcomes of asynchronous methods for future High Performance
      Computing environments. Given that asynchronous methods are notoriously
      difficult to study theoretically, their simulation is an invaluable tool
      for observing behavior and making quantitative and qualitative assertions.
      The modular and extensible nature of the framework proposed here allows
      for easy experimentation with modifications to a popular class of
      algorithms that finds uses in many areas of science and engineering.</p>
    <p>
      The work presented was designed to show the ability of the framework to
      adapt to new algorithm variants, such as those capable of handling
      algorithm recovery in the presence of transient soft faults as was shown
      by example in <a href="#sect6.2">Section 6.2</a>.
    </p>
    <p>The simulation framework presented here is extensible and flexible
      and is able to:</p>
    <ol>
      <li>admit a variety of asynchronous methods (i.e., beyond the simple
        Jacobi algorithm)</li>
      <li>incorporate different fault models and recovery techniques for
        the development of fault tolerant algorithms, and</li>
      <li>vary hardware parameters such as thread and processor counts and
        the performance of those parameters as governed by the timing
        distributions that are supplied.</li>
    </ol>
    <p>
      In the future, the most obvious extension of the simulation framework is
      to add modules that allow it to be accurately used for experiments on
      either distributed or cloud-based computing environments. Additionally, it
      is planned to add features for optimization that could allow for the
      automation of the selection of the checkpointing tolerance as well as
      checkpointing frequency in the course of simulation. Adding the capability
      for the framework to take a range of parameters and find optimal values
      without direct input from the user could aid in the development of
      algorithms. Furthermore, the simulation framework is intended to be
      augmented with runtime simulation measurements, such those provided by
      Intel<sup>Â®</sup> Running Average Power Limit (RAPL) interface (<a
        role="doc-biblioref" href="#Int16">Intel, 2016</a>), to obtain simulated
      application execution traces in order to model application performance and
      energy consumption.
    </p>
  </section>

  <section id="sect8" role="doc-acknowledgements">
    <h1>Acknowledgements</h1>
    <p>
      This work was supported in part by the Air Force Office of Scientific
      Research under the AFOSR award FA9550-12-1-0476, by the U.S. Department of
      Energy (DOE) Office of Advanced Scientific Computing Research under the
      grant DE-SC-0016564 and the Exascale Computing Project (ECP) through the
      Ames Laboratory, operated by Iowa State University under contract No.
      DE-AC00-07CH11358, by the U.S. Department of Defense High Performance
      Computing Modernization Program, through a HASI grant, the High
      Performance Computing facilities at Old Dominion University, and through
      the ILIR/IAR program at the Naval Surface Warfare Center â Dahlgren
      Division. Edmond Chow provided the MATLAB<sup>Â®</sup> script that evolved
      into part of the proposed simulation framework.
    </p>
  </section>

  <section id="sect9" class="suppressInPDF" role="doc-endnotes">
    <h1>Footnotes</h1>
    <ol>
      <li id="ftn1" role="doc-endnote">Throughout the text, vector notation
        is occasionally adopted to emphasize when functions take all components
        of <span class="math-span">ð¥</span> as opposed to a single component,
        such as <span class="math-span">ð¥<sub>1</sub></span>.<sup><a
          role="doc-backlink" href="#fn_pointer_ftn1">[back]</a></sup>
      </li>
      <li id="ftn2" role="doc-endnote">Rulfo is a part of computing
        resources of the Department of Modeling, Simulation and Visualization
        Engineering at Old Dominion University.<sup><a role="doc-backlink"
          href="#fn_pointer_ftn2">[back]</a></sup>
      </li>
    </ol>
  </section>

  <section id="sect10" role="doc-bibliography">
    <h1>Bibliography</h1>
    <ul>
      <li id="AB05" role="doc-biblioentry">Addou, A., &amp; Benahmed, A.
        (2005). Parallel Synchronous Algorithm for Nonlinear Fixed Point
        Problems. <em>International Journal of Mathematics and Mathematical
          Sciences</em>. 2005(19), 3175â3183. <a
        href="https://dx.doi.org/10.1155/IJMMS.2005.3175"><img
          src="../../icons/doi.png" /></a>
      </li>
      <li id="Anz12" role="doc-biblioentry">Anzt, H. (2012). <em>Asynchronous
          and Multiprecision Linear Solvers: Scalable and Fault-Tolerant
          Numerics for Energy Efficient High Performance Computing</em> (Doctoral
        dissertation, Karlsruher Institut fÃ¼r Technologie, Karlsruhe, Germany).
        <a href="https://d-nb.info/1029764689/34"><img
          src="../../icons/html.png" /></a>
      </li>
      <li id="ACD15" role="doc-biblioentry">Anzt, H., Chow, E., &amp;
        Dongarra, J. (2015). Iterative Sparse Triangular Solves for
        Preconditioning. In TrÃ¤ff, J., Hunold, S., &amp; Versaci, F. (eds) <em>Euro-Par
          2015: Parallel Processing. Euro-Par 2015</em> (pp. 650â661). Lecture Notes
        in Computer Science, vol 9233. Berlin, Heidelberg: Springer. <a
        href="https://dx.doi.org/10.1007/978-3-662-48096-0_50"><img
          src="../../icons/doi.png" /></a>
      </li>
      <li id="ADQO15" role="doc-biblioentry">Anzt, H., Dongarra, J., &amp;
        Quintana-OrtÃ­, E.S. (2015). Tuning Stationary Iterative Solvers for
        Fault Resilience. In <em>Proceedings of the 6th Workshop on Latest
          Advances in Scalable Algorithms for Large-Scale Systems</em> (pp. 1:1â1:8).
        New York, NY: ACM. <a href="https://dx.doi.org/10.1145/2832080.2832081"><img
          src="../../icons/doi.png" /></a>
      </li>
      <li id="ADQO16" role="doc-biblioentry">Anzt, H., Dongarra, J., &amp;
        Quintana-OrtÃ­, E.S. (2016). Fine-Grained Bit-Flip Protection for
        Relaxation Methods. <em>Journal of Computational Science</em>. <a
        href="https://dx.doi.org/10.1016/j.jocs.2016.11.013"><img
          src="../../icons/doi.png" /></a>
      </li>
      <li id="ALDH12" role="doc-biblioentry">Anzt, H., Luszczek, P.,
        Dongarra, J., &amp; Heuveline, V. (2012). GPU-Accelerated Asynchronous
        Error Correction for Mixed Precision Iterative Refinement. In
        Kaklamanis, C., Papatheodorou, T., &amp; Spirakis, P.G. (eds) <em>Euro-Par
          2012 Parallel Processing</em> (pp. 908â919). Lecture Notes in Computer
        Science, vol 7484. Berlin, Heidelberg: Springer. <a
        href="https://dx.doi.org/10.1007/978-3-642-32820-6_89"><img
          src="../../icons/doi.png" /></a>
      </li>
      <li id="ABC+06" role="doc-biblioentry">AsanoviÄ, K., Bodik, R.,
        Catanzaro, B.C., Gebis, J.J., Husbands, P., Keutzer, K., Patterson, D.
        A., Plishker, W.L., Shalf, J., Williams, S.W., &amp; Yelick, K.A.
        (2006). <em>The Landscape of Parallel Computing Research: A View
          from Berkeley</em> (Report No. UCB/EECS-2006-183). Berkeley, CA: EECS
        Department, University of California. <a
        href="https://www2.eecs.berkeley.edu/Pubs/TechRpts/2006/EECS-2006-183.html"><img
          src="../../icons/html.png" /></a>
      </li>
      <li id="ABC+10" role="doc-biblioentry">Ashby, S., Beckman, P., Chen,
        J., Colella, P., Collins, B., Crawford, D., Dongarra, J., Kothe, D.,
        Lusk, R., Messina, P., Mezzacappa, T., Moin, P., Norman, M. Rosner, R.,
        Sarkar, V., Siegel, A., Streitz, F., White, A., &amp; Wright, M. (2010).
        <em>The Opportunities and Challenges of Exascale Computing: Summary
          Report of the Advanced Scientific Computing Advisory Committee (ASCAC)
          subcommittee.</em> (Report No. Fall 2010). Washington, D.C.: U.S.
        Department of Energy Office of Science. <a
        href="https://science.energy.gov/~/media/ascr/ascac/pdf/reports/Exascale_subcommittee_report.pdf"><img
          src="../../icons/pdf.png" /></a>
      </li>
      <li id="ADG15" role="doc-biblioentry">Avron, H., Druinsky, A. &amp;
        Gupta, A. (2015). Revisiting Asynchronous Linear Solvers: Provable
        Convergence Rate through Randomization. <em>Journal of the ACM</em>,
        62(6), 51:1â51:27. <a href="https://dx.doi.org/10.1145/2814566"><img
          src="../../icons/doi.png" /></a>
      </li>
      <li id="Bau78" role="doc-biblioentry">Baudet, G.M. (1978).
        Asynchronous Iterative Methods for Multiprocessors. <em>Journal of
          the ACM</em>, 25(2), 226â244. <a
        href="https://dx.doi.org/10.1145/322063.322067"><img
          src="../../icons/doi.png" /></a>
      </li>
      <li id="Ben07" role="doc-biblioentry">Benahmed, A. (2007). A
        Convergence Result for Asynchronous Algorithms and Applications. <em>Proyecciones
          (Antofagasta)</em>, 26(2), 219â236. <a
        href="https://scielo.conicyt.cl/scielo.php?script=sci_arttext&amp;pid=S0716-09172007000200005&amp;lng=pt&amp;nrm=iso&amp;tlng=en"><img
          src="../../icons/html.png" /></a>
      </li>
      <li id="BT89" role="doc-biblioentry">Bertsekas, D.P., &amp;
        Tsitsiklis, J.N. (1989). <em>Parallel and Distributed Computation:
          Numerical Methods</em>. Englewood Cliffs, NJ: Prentice-Hall.
      </li>
      <li id="BBDH11" role="doc-biblioentry">Bethune, I., Bull, J.M.,
        Dingle, N.J., &amp; Higham, N.J. (2011). <em>Investigating the
          Performance of Asynchronous Jacobi's Method for Solving Systems of
          Linear Equations</em> (Report No. 2011.82). Manchester: The University of
        Manchester. <a href="http://eprints.maths.manchester.ac.uk/1684/"><img
          src="../../icons/html.png" /></a> <a
        href="http://eprints.maths.manchester.ac.uk/1684/1/paper.pdf"><img
          src="../../icons/pdf.png" /></a>
      </li>
      <li id="BBDH14" role="doc-biblioentry">Bethune, I., Bull, J.M.,
        Dingle, N.J., &amp; Higham, N.J. (2014). Performance Analysis of
        Asynchronous Jacobi's Method Implemented in MPI, SHMEM and OpenMP. <em>International
          Journal of High Performance Computing Applications</em>, 28(1), 97â111. <a
        href="https://dx.doi.org/10.1177/1094342013493123"><img
          src="../../icons/doi.png" /></a>
      </li>
      <li id="BPC+11" role="doc-biblioentry">Boyd, S., Parikh, N., Chu, E.,
        Peleato, B., &amp; Eckstein, J. (2011). Distributed Optimization and
        Statistical Learning via the Alternating Direction Method of
        Multipliers. <em>Foundations and Trends in Machine Learning</em>, 3(1),
        1â122. <a
        href="https://web.stanford.edu/~boyd/papers/admm_distr_stats.html"><img
          src="../../icons/html.png" /></a>
      </li>
      <li id="BFHH12" role="doc-biblioentry">Bridges, P.G., Ferreira, K.
        B., Heroux, M.A., &amp; Hoemmen, M. (2012). Fault-Tolerant Linear
        Solvers via Selective Reliability. <em>arXiv preprint
          arXiv:1206.1390</em>. <a href="https://arxiv.org/abs/1206.1390"><img
          src="../../icons/html.png" /></a>
      </li>
      <li id="BM02" role="doc-biblioentry">Buyya, R. &amp; Murshed, M.
        (2002). Gridsim: A Toolkit for the Modeling and Simulation of
        Distributed Resource Management and Scheduling for Grid Computing. <em>Concurrency
          and Computation: Practice and Experience</em>, 14(13â15), 1175â1220. <a
        href="https://dx.doi.org/10.1002/cpe.710"><img
          src="../../icons/doi.png" /></a>
      </li>
      <li id="CRB+11" role="doc-biblioentry">Calheiros, R.N., Ranjan, R.,
        Beloglazov, A., De Rose, C.A.F., &amp; Buyya, R. (2011). CloudSim: A
        Toolkit for Modeling and Simulation of Cloud Computing Environments and
        Evaluation of Resource Provisioning Algorithms. <em>Software:
          Practice and Experience</em>, 41(1), 23â50. <a
        href="https://dx.doi.org/10.1002/spe.995"><img
          src="../../icons/doi.png" /></a>
      </li>
      <li id="CRDRB09" role="doc-biblioentry">Calheiros, R.N., Ranjan, R.,
        De Rose, C.A.F., &amp; Buyya, R. (2009). Cloudsim: A Novel Framework for
        Modeling and Simulation of Cloud Computing Infrastructures and Services.
        <em>arXiv preprint arXiv:0903.2525</em>. <a
        href="https://arxiv.org/abs/0903.2525"><img
          src="../../icons/html.png" /></a>
      </li>
      <li id="CGG+09" role="doc-biblioentry">Cappello, F., Geist, A.,
        Gropp, B., Kale, L., Kramer, B., &amp; Snir, M. (2009). Toward Exascale
        Resilience. <em>International Journal of High Performance Computing
          Applications</em>, 23(4), 374â388. <a
        href="https://dx.doi.org/10.1177/1094342009347767"><img
          src="../../icons/doi.png" /></a>
      </li>
      <li id="CGG+14" role="doc-biblioentry">Cappello, F., Geist, A.,
        Gropp, B., Kale, L., Kramer, B., &amp; Snir, M. (2014). Toward Exascale
        Resilience: 2014 update. <em>Supercomputing Frontiers and
          Innovations</em>, 1(1). <a href="https://dx.doi.org/10.14529/jsfi140101"><img
          src="../../icons/doi.png" /></a>
      </li>
      <li id="Cas01" role="doc-biblioentry">Casanova, H. (2001). Simgrid: A
        Toolkit for the Simulation of Application Scheduling. In <em>Proceedings
          of the First IEEE/ACM International Symposium on Cluster Computing and
          the Grid</em> (pp. 430â437). Piscataway, NJ: IEEE Press. <a
        href="https://dx.doi.org/10.1109/CCGRID.2001.923223"><img
          src="../../icons/doi.png" /></a>
      </li>
      <li id="CLQ08" role="doc-biblioentry">Casanova, H., Legrand, A.,
        &amp; Quinson, M. (2008). Simgrid: A Generic Framework for Large-Scale
        Distributed Experiments. In <em>Proceedings of the Tenth
          International Conference on Computer Modeling and Simulation (UKSIM
          2008)</em> (pp. 126â131). Piscataway, NJ: IEEE Press. <a
        href="https://dx.doi.org/10.1109/UKSIM.2008.28"><img
          src="../../icons/doi.png" /></a>
      </li>
      <li id="CM69" role="doc-biblioentry">Chazan, D., &amp; Miranker, W.
        (2014). Chaotic Relaxation. <em>Linear Algebra and its Applications</em>,
        2(2), 199â222. <a href="https://dx.doi.org/10.1016/0024-3795(69)90028-7"><img
          src="../../icons/doi.png" /></a>
      </li>
      <li id="CC16" role="doc-biblioentry">Cheung, Y.K. &amp; Cole, R.
        (2016). A Unified Approach to Analyzing Asynchronous Coordinate Descent
        and Tatonnement. <em>arXiv preprint arXiv:1612.09171</em>. <a
        href="https://arxiv.org/abs/1612.09171"><img
          src="../../icons/html.png" /></a>
      </li>
      <li id="CAD15" role="doc-biblioentry">Chow, E., Anzt, H., &amp;
        Dongarra, J. (2015). Asynchronous Iterative Algorithm for Computing
        Incomplete Factorizations on GPUs. In Kunkel, J., &amp; Ludwig, T. (eds)
        <em>High Performance Computing. ISC High Performance 2015</em>. Lecture
        Notes in Computer Science, vol 9137. Cham: Springer. <a
        href="https://dx.doi.org/10.1007/978-3-319-20119-1_1"><img
          src="../../icons/doi.png" /></a>
      </li>
      <li id="CP15" role="doc-biblioentry">Chow, E., &amp; Patel, A.
        (2015). Fine-Grained Parallel Incomplete LU Factorization. <em>SIAM
          Journal on Scientific Computing</em>, 37(2), C169âC193. <a
        href="https://dx.doi.org/10.1137/140968896"><img
          src="../../icons/doi.png" /></a>
      </li>
      <li id="CJB+18" role="doc-biblioentry">Coleman, E.C., Jamal, A.,
        Baboulin, M., Khabou, A., &amp; Sosonkina, M. (2018). A Comparison of
        Soft-Fault Error Models in the Parallel Preconditioned Flexible GMRES.
        In Wyrzykowski, R., Dongarra, J., Deelman, E., Karczewski, K. (eds) <em>Parallel
          Processing and Applied Mathematics. PPAM 2017.</em> (pp. 36â46). Lecture
        Notes in Computer Science, vol 10777. Cham: Springer. <a
        href="https://dx.doi.org/10.1007/978-3-319-78024-5_4"><img
          src="../../icons/doi.png" /></a>
      </li>
      <li id="CS16a" role="doc-biblioentry">Coleman, E.C., &amp; Sosonkina,
        M. (2016a). A Comparison and Analysis of Soft-Fault Error Models using
        FGMRES. In <em>Proceedings of the 6th Annual Virginia Modeling,
          Simulation, and Analysis Center Capstone Conference</em> (pp. 135â142).
        Norfolk, VA: Virginia Modeling, Simulation, and Analysis Center.
      </li>
      <li id="CS16b" role="doc-biblioentry">Coleman, E.C., &amp; Sosonkina,
        M. (2016b). Evaluating a Persistent Soft Fault Model on Preconditioned
        Iterative Methods. In <em>Proceedings of the 22nd Annual
          International Conference on Parallel and Distributed Processing
          Techniques and Applications</em> (pp. 98â104). Athens: The Steering
        Committee of The World Congress in Computer Science, Computer
        Engineering and Applied Computing (WorldComp). <a
        href="http://worldcomp-proceedings.com/proc/p2016/PDP3831.pdf"><img
          src="../../icons/pdf.png" /></a>
      </li>
      <li id="CS17" role="doc-biblioentry">Coleman, E.C., &amp; Sosonkina,
        M. (2017). Fault Tolerance for Fine-Grained Iterative Methods. In <em>Proceedings
          of the 7th Annual Virginia Modeling, Simulation, and Analysis Center
          Capstone Conference</em>. Norfolk, VA: Virginia Modeling, Simulation, and
        Analysis Center.
      </li>
      <li id="CS18" role="doc-biblioentry">Coleman, E.C., &amp; Sosonkina,
        M. (2018). Self-Stabilizing Fine-Grained Parallel Incopmlete LU
        Factorization. <em>Sustainable Computing: Informatics and Systems</em>,
        (In Press). <a href="https://dx.doi.org/10.1016/j.suscom.2018.01.003"><img
          src="../../icons/doi.png" /></a>
      </li>
      <li id="CSC17" role="doc-biblioentry">Coleman, E.C., Sosonkina, M.,
        &amp; Chow, E. (2017). Fault Tolerant Variants of the Fine-Grained
        Parallel Incomplete LU Factorization. In <em>Proceedings of the
          25th High Performance Computing Symposium</em>. San Diego, CA: Society for
        Computer Simulation International. <a
        href="http://scs.org/wp-content/uploads/2017/06/50_Final_Manuscript.pdf"><img
          src="../../icons/pdf.png" /></a>
      </li>
      <li id="DHB+14" role="doc-biblioentry">Dongarra, J., Hittinger, J.,
        Bell, J., Chacon, L., Falgout, R., Heroux, M., Hovland, P., Ng, E.,
        Webster, A., &amp; Wild, S. (2014). <em>Applied Mathematics
          Research for Exascale Computing</em> (Report No. LLNL-TR-651000).
        Livermore, CA: Lawrence Livermore National Laboratory. <a
        href="https://dx.doi.org/10.2172/1149042"><img
          src="../../icons/doi.png" /></a>
      </li>
      <li id="DF05" role="doc-biblioentry">Dumitrescu, C.L., &amp; Foster,
        I. (2005). GangSim: A Simulator for Grid Scheduling Studies. In <em>CCGrid
          2005. IEEE International Symposium on Cluster Computing and the Grid</em>
        (pp. 1151â1158, vol 2). Piscataway, NJ: IEEE Press. <a
        href="https://dx.doi.org/10.1109/CCGRID.2005.1558689"><img
          src="../../icons/doi.png" /></a>
      </li>
      <li id="EHM15" role="doc-biblioentry">Elliott, J., Hoemmen, M., &amp;
        Mueller, F. (2015). A Numerical Soft Fault Model for Iterative Linear
        Solvers. In <em>Proceedings of the 24th International Symposium on
          High-Performance Parallel and Distributed Computing</em> (pp. 271â274). New
        York, NY: ACM. <a href="https://dx.doi.org/10.1145/2749246.2749254"><img
          src="../../icons/doi.png" /></a>
      </li>
      <li id="FS00" role="doc-biblioentry">Frommer, A. &amp; Szyld, D. B.
        (2000). On Asynchronous Iterations. <em>Journal of Computational
          and Applied Mathematics</em>, 123(1), 201â216. <a
        href="https://dx.doi.org/10.1016/S0377-0427(00)00409-X"><img
          src="../../icons/doi.png" /></a>
      </li>
      <li id="GÃ¤r99" role="doc-biblioentry">GÃ¤rtner, F.C. (1999).
        Fundamentals of Fault-Tolerant Distributed Computing in Asynchronous
        Environments. <em>ACM Computing Surveys</em>, 31(1), 1â26. <a
        href="https://dx.doi.org/10.1145/311531.311532"><img
          src="../../icons/doi.png" /></a>
      </li>
      <li id="Gei11" role="doc-biblioentry">Geist, A. (2011). What is the
        Monster in the Closet?. In <em>Invited Talk at Workshop on
          Architectures I: Exascale and Beyond: Gaps in Research, Gaps in our
          Thinking</em> (vol 2).
      </li>
      <li id="Gei12" role="doc-biblioentry">Geist, A. (2012). Exascale
        Monster in the Closet. In <em>2012 IEEE Workshop on Silicon Errors
          in Logic-System Effects, Champaign-Urbana, IL, March</em> (pp. 27â28).
        Piscataway, NJ: IEEE Press.
      </li>
      <li id="Gei16" role="doc-biblioentry">Geist, A. (2016).
        Supercomputing's Monster in the Closet. <em>IEEE Spectrum</em>. 53(3),
        30â35. <a href="https://doi.org/10.1109/MSPEC.2016.7420396"><img
          src="../../icons/doi.png" /></a>
      </li>
      <li id="GL09" role="doc-biblioentry">Geist, A., &amp; Lucas, R.
        (2009). Major Computer Science Challenges at Exascale. <em>International
          Journal of High Performance Computing Applications</em>. 23(4), 427â436. <a
        href="https://doi.org/10.1177%2F1094342009347445"><img
          src="../../icons/doi.png" /></a>
      </li>
      <li id="GBH14" role="doc-biblioentry">Gerstenberger, R., Besta, M.,
        &amp; Hoefler, T. (2014). Enabling Highly-Scalable Remote Memory Access
        Programming with MPI-3 One Sided. <em>Scientific Programming</em>,
        22(2), 75â91. <a href="https://dx.doi.org/10.3233/SPR-140383"><img
          src="../../icons/doi.png" /></a>
      </li>
      <li id="HW10" role="doc-biblioentry">Hager, G., &amp; Wellein, G.
        (2010). <em>Introduction to High Performance Computing for
          Scientists and Engineers</em>. Boca Raton, FL: CRC Press. <a
        href="https://dx.doi.org/10.1109/TCNS.2017.2657460"><img
          src="../../icons/doi.png" /></a>
      </li>
      <li id="HH11" role="doc-biblioentry">Hoemmen, M., &amp; Heroux, M.A.
        (2011). <em>Fault-Tolerant Iterative Methods via Selective
          Reliability</em>. vol 3. <a
        href="http://www.sandia.gov/%7Emaherou/docs/FTGMRES.pdf"><img
          src="../../icons/pdf.png" /></a>
      </li>
      <li id="Hon17" role="doc-biblioentry">Hong, M. (2017). A Distributed,
        Asynchronous and Incremental Algorithm for Nonconvex Optimization: An
        ADMM Approach. <em>IEEE Transactions on Control of Network Systems</em>.
        Piscataway, NJ: IEEE Press. <a
        href="https://dx.doi.org/10.1109/TCNS.2017.2657460"><img
          src="../../icons/doi.png" /></a>
      </li>
      <li id="HD18" role="doc-biblioentry">Hook, J., &amp; Dingle, N.
        (2018). Performance Analysis of Asynchronous Parallel Jacobi. <em>Numerical
          Algorithms</em>. 77(3), 831â866. <a
        href="https://dx.doi.org/10.1007/s11075-017-0342-9"><img
          src="../../icons/doi.png" /></a>
      </li>
      <li id="Int16" role="doc-biblioentry">Intel (2016). <em>Intel<sup>Â®</sup>
          64 and IA-32 Architectures Developer's Manual
      </em>. Volume 3B: System Programming Guide, Part 2. <a
        href="https://www.intel.de/content/www/de/de/architecture-and-technology/64-ia-32-architectures-software-developer-vol-3b-part-2-manual.html"><img
          src="../../icons/html.png" /></a>
      </li>
      <li id="IBCH13" role="doc-biblioentry">Iutzeler, F., Bianchi, P.,
        Ciblat, P., &amp; Hachem, W. (2013). Asynchronous Distributed
        Optimization using a Randomized Alternating Direction Method of
        Multipliers. In <em>Decision and Control (CDC), 2013 IEEE 52nd
          Annual Conference on</em> (pp. 3671â3676). Piscataway, NJ: IEEE Press. <a
        href="https://dx.doi.org/10.1109/CDC.2013.6760448"><img
          src="../../icons/doi.png" /></a>
      </li>
      <li id="OR00" role="doc-biblioentry">Ortega, J.M., &amp; Rheinboldt,
        W.C. (2000). <em>Iterative Solution of Nonlinear Equations in
          Several Variables</em>. Philadelphia, PA: SIAM. <a
        href="https://dx.doi.org/10.1137/1.9780898719468"><img
          src="../../icons/doi.png" /></a>
      </li>
      <li id="Saa03" role="doc-biblioentry">Saad, Y. (2003). <em>Iterative
          Methods for Sparse Linear Systems</em>. Philadelphia, PA: SIAM. <a
        href="https://dx.doi.org/10.1137/1.9780898718003"><img
          src="../../icons/doi.png" /></a>
      </li>
      <li id="SV13" role="doc-biblioentry">Sao, P. &amp; Vuduc, R. (2013).
        Self-Stabilizing Iterative Solvers. In <em>Proceedings of the
          Workshop on Latest Advances in Scalable Algorithms for Large-Scale
          Systems</em> (pp. 4:1â4:8). New York, NY: ACM. <a
        href="https://dx.doi.org/10.1145/2530268.2530272"><img
          src="../../icons/doi.png" /></a>
      </li>
      <li id="SWA+14" role="doc-biblioentry">Snir, M., Wisniewski, R.W.,
        Abraham, J.A., Adve, S.V., Bagchi, S., Balaji, P., Belak, J., Bose, P.,
        Cappello, F., Carlson, B., Chien, A.A., Coteus, P., Debardeleben, N.A.,
        Diniz, P.C., Engelmann, C., Erez, M., Fazzari, S., Geist, A., Gupta, R.,
        Johnson, F., Krishnamoorthy, S., Leyffer, S., Liberty, D., Mitra, S.,
        Munson, T., Schreiber, R., Stearley, J., &amp; van Hensbergen, E.
        (2014). Addressing Failures in Exascale Computing. <em>International
          Journal of High Performance Computing Applications</em>. 28(2), 129â173. <a
        href="https://dx.doi.org/10.1177/1094342014522573"><img
          src="../../icons/doi.png" /></a>
      </li>
      <li id="SN11" role="doc-biblioentry">Srivastava, K., &amp; Nedic, A.
        (2011). Distributed Asynchronous Constrained Stochastic Optimization. <em>IEEE
          Journal of Selected Topics in Signal Processing</em>. 5(4), 772â790. <a
        href="https://dx.doi.org/10.1109/JSTSP.2011.2118740"><img
          src="../../icons/doi.png" /></a>
      </li>
      <li id="SW15" role="doc-biblioentry">Stoyanov, M. &amp; Webster, C.
        (2015). Numerical Analysis of Fixed Point Algorithms in the Presence of
        Hardware Faults. <em>SIAM Journal on Scientific Computing</em>. 37(5),
        C532âC553. <a href="https://dx.doi.org/10.1137/140991406"><img
          src="../../icons/doi.png" /></a>
      </li>
      <li id="Szy98" role="doc-biblioentry">Szyld, D.B. (1998). Different
        Models of Parallel Asynchronous Iterations with Overlapping Blocks. <em>Computational
          and Applied Mathematics</em>. 17, 101â115. <a
        href="https://dx.doi.org/10.1137/140991406"><img
          src="../../icons/doi.png" /></a>
      </li>
      <li id="TBA86" role="doc-biblioentry">Tsitsiklis, J., Bertsekas, D.,
        &amp; Athans, M. (1986). Distributed Asynchronous Deterministic and
        Stochastic Gradient Optimization Algorithms. <em>IEEE Transactions
          on Automatic Control</em>. 31(9), 803â812. <a
        href="https://dx.doi.org/10.1137/140991406"><img
          src="../../icons/doi.png" /></a>
      </li>
      <li id="VV09" role="doc-biblioentry">Venkatasubramanian, S., &amp;
        Vuduc, R.W. (2009). Tuned and Wildly Asynchronous Stencil Kernels for
        Hybrid CPU/GPU Systems. In <em>Proceedings of the 23rd
          International Conference on Supercomputing</em> (pp. 244â255). New York,
        NY: ACM. <a href="https://dx.doi.org/10.1145/1542275.1542312"><img
          src="../../icons/doi.png" /></a>
      </li>
      <li id="WPC16" role="doc-biblioentry">Wolfson-Pou, J., &amp; Chow, E.
        (2016). Reducing Communication in Distributed Asynchronous Iterative
        Methods. <em>Procedia Computer Science</em>. 80, 1906â1916. <a
        href="https://dx.doi.org/10.1016/j.procs.2016.05.501"><img
          src="../../icons/doi.png" /></a>
      </li>
      <li id="ZC10" role="doc-biblioentry">Zhong, M., &amp; Cassandras, C.
        G. (2010). Distributed Asynchronous Deterministic and Stochastic
        Gradient Optimization Algorithms. <em>IEEE Transactions on
          Automatic Control</em>. 55(12), 2735â2750. <a
        href="https://dx.doi.org/10.1109/TAC.2010.2049518"><img
          src="../../icons/doi.png" /></a>
      </li>
    </ul>
  </section>

  <section id="sect11" class="no-counter">
    <h1>Copyright Information</h1>
    <p>
      <a rel="license" href="https://creativecommons.org/licenses/by/4.0/"><img
        alt="Creative Commons License"
        src="https://i.creativecommons.org/l/by/4.0/88x31.png" /></a> Copyright
      &copy; 2018 Evan C. Coleman, Erik Jensen, Masha Sosonkina. This article is
      licensed under a <a href="https://creativecommons.org/licenses/by/4.0/">Creative
        Commons Attribution 4.0 International License</a>.
    </p>
  </section>
  </main>

  <nav id="toc" class="suppressInPDF">
    <h1>Outline</h1>
    <ol>
      <li><a href="#sect0">Abstract</a></li>
      <li><a href="#sect1">Introduction</a></li>
      <li><a href="#sect2">Related Work</a></li>
      <li><a href="#sect3">Asynchronous Iterative Methods</a></li>
      <li><a href="#sect4">Design of Simulation Framework</a></li>
      <li><a href="#sect5">Asynchronous Jacobi Implementations for the
          Framework</a></li>
      <li><a href="#sect6">Framework Extension for Fault-Tolerance
          Requirements</a></li>
      <li><a href="#sect7">Conclusions and Future Work</a></li>
      <li><a href="#sect8">Acknowledgements</a></li>
      <li><a href="#sect9">Footnotes</a></li>
      <li><a href="#sect10">Bibliography</a></li>
      <li><a href="#sect11">Copyright Information</a></li>
    </ol>
  </nav>
</body>
</html>