<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8" />
<meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
<title>JSimE 1/5 - Simulation Framework for Asynchronous Iterative
  Methods</title>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<!--Google Scholar-->
<meta name="citation_journal_title" content="Journal of Simulation Engineering" />
<meta name="citation_issn" content="2569-9466" />
<meta name="citation_volume" content="1" />
<meta name="citation_title"
  content="Simulation Framework for Asynchronous Iterative Methods" />
<meta name="citation_publication_date" content="2018-06-28" />
<meta name="citation_author" content="Evan Christopher Coleman" />
<meta name="citation_author_email" content="evanccoleman@gmail.com" />
<meta name="citation_author_institution"
  content="Old Dominion University, Norfolk, VA, United States" />
<meta name="citation_author" content="Erik Jensen" />
<meta name="citation_author_email" content="ejens005@odu.edu" />
<meta name="citation_author_institution"
  content="Old Dominion University, Norfolk, VA, United States" />
<meta name="citation_author" content="Masha Sosonkina" />
<meta name="citation_author_email" content="msosonki@odu.edu" />
<meta name="citation_author_institution"
  content="Old Dominion University, Norfolk, VA, United States" />
<meta name="citation_abstract_html_url"
  content="https://jsime.org/index.php/jsimeng/article/view/6" />
<meta name="citation_pdf_url"
  content="https://articles.jsime.org/1/jsime-article-1-5.pdf" />
<!--Canonical URL-->
<link rel="canonical"
  href="https://articles.jsime.org/1/5/Simulation-Framework-for-Asynchronous-Iterative-Methods" />
<!-- Global site tag (gtag.js) - Google Analytics -->
<script async="async"
  src="https://www.googletagmanager.com/gtag/js?id=UA-115543812-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag () {
    dataLayer.push( arguments );
  }
  gtag( 'js', new Date() );
  gtag( 'config', 'UA-115543812-1', {
    'anonymize_ip': true
  } );
</script>

<!--Stylesheets-->
<link rel="stylesheet"
  href="https://cdnjs.cloudflare.com/ajax/libs/normalize/7.0.0/normalize.min.css" />
<link rel="stylesheet"
  href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.9.0-beta/katex.min.css"
  integrity="sha384-L/SNYu0HM7XECWBeshTGLluQO9uVI1tvkCtunuoUbCHHoTH76cDyXty69Bb9I0qZ"
  crossorigin="anonymous" />
<link rel="stylesheet" href="https://articles.jsime.org/pseudocode.min.css" />
<link rel="stylesheet" href="../../jsime.css" />
<!--Scripts-->
<script
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
<script
  src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.9.0-beta/katex.min.js"
  integrity="sha384-ad+n9lzhJjYgO67lARKETJH6WuQVDDlRfj81AJJSswMyMkXTD49wBj5EP004WOY6"
  crossorigin="anonymous"></script>
<script src="https://articles.jsime.org/pseudocode.min.js"></script>
<!--schema.org JSON-LD-->
<script type="application/ld+json">
{
  "@context": "http://schema.org",
  "@graph": [
    {
      "@id": "#jsime",
      "@type": "Periodical",
      "name": "Journal of Simulation Engineering",
      "issn": "2569-9466",
      "publisher": {
        "@type": "Organization",
        "name": "Consortium for True Open Access in Modeling and Simulation"
      },
      "publishingPrinciples": "http://publicationethics.org/files/Code_of_conduct_for_journal_editors.pdf"
    },
    {
      "@id": "#volume-1",
      "@type": "PublicationVolume",
      "datePublished": "2018-06-28",
      "volumeNumber": "1",
      "isPartOf": { "@id": "#jsime" }
    },
    {
      "@id": "#affiliation-1",
      "@type": "Organization",
      "address": "Old Dominion University, Norfolk, VA, United States"
    },
    {
      "@id": "#author-1",
      "@type": "Person",
      "name": "Evan Christopher Coleman",
      "email": "evanccoleman@gmail.com",
      "affiliation": "#affiliation-1"
    },
    {
      "@id": "#author-2",
      "@type": "Person",
      "name": "Erik Jensen",
      "email": "ejens005@odu.edu",
      "affiliation": "#affiliation-1"
    },
    {
      "@id": "#author-3",
      "@type": "Person",
      "name": "Masha Sosonkina",
      "email": "msosonki@odu.edu",
      "affiliation": "#affiliation-1"
    },
    {
      "@id": "#artcile-1-5",
      "@type": "ScholarlyArticle",
      "isPartOf": { "@id": "#volume-1" },
      "name": "Simulation Framework for Asynchronous Iterative Methods",
      "author": [ "#author-1", "#author-2", "#author-3" ],
      "keywords": [ "Asynchronous iterative methods", "Fault tolerance", "Asynchronous simulation", "Shared memory", "Intel Xeon Phi" ],
      "description": "As high-performance computing (HPC) platforms progress towards exascale, computational methods must be revamped to successfully leverage them. In particular, (1) asynchronous methods become of great importance because synchronization becomes prohibitively expensive and (2) resilience of computations must be achieved, e.g., using checkpointing selectively which may otherwise become prohibitively expensive due to the sheer scale of the computing environment. In this work, a simulation framework for asynchronous iterative methods is proposed and tested on HPC accelerator (shared-memory) architecture. The design proposed here offers a lightweight alternative to existing computational frameworks to allow for easy experimentation with various relaxation iterative techniques, solution updating schemes, and predicted performance. The simulation framework is implemented in MATLAB® using function handles which offers a modular, easily extensible design. An example of a case study using the simulation framework is presented to examine the efficacy of different checkpointing schemes for asynchronous relaxation methods.",
      "url": "https://articles.jsime.org/1/5/Simulation-Framework-for-Asynchronous-Iterative-Methods",
      "inLanguage": "en-US",
      "license": "https://creativecommons.org/licenses/by/4.0/",
      "copyrightHolder": [ "#author-1", "#author-2", "#author-3" ],
      "copyrightYear": "2018",
      "dateCreated": "2018-06-25",
      "dateModified": "2018-06-25",
      "datePublished": "2018-06-28"
    },
    {
      "@type": "CategoryCodeSet",
      "@id": "http://totem.semedica.com/taxonomy/The%20ACM%20Computing%20Classification%20System%20(CCS)",
      "name": "The 2012 ACM Computing Classification System"
    },
    {
      "@type": "CategoryCode",
      "identifier": "10010147.10010341.10010342.10010344",
      "codeValue": "Computing methodologies~Model verification and validation",
      "inCodeSet": "http://totem.semedica.com/taxonomy/The%20ACM%20Computing%20Classification%20System%20(CCS)"
    }
  ]
}
</script>
</head>
<body vocab="http://schema.org/">
  <header>
    <small><a href="https://jsime.org"><img src="../../JSimE.svg"
        style="height: 20px; margin-right: 0.7em;" /></a>Journal of Simulation
      Engineering, Vol. 1 (under construction). Article URL: <a
      href="https://articles.jsime.org/1/5/Simulation-Framework-for-Asynchronous-Iterative-Methods"
      id="articleURL">https://articles.jsime.org/1/5</a><a id="PDF"
      class="suppressInPDF"
      href="https://jsime.org/index.php/jsimeng/article/view/6"
      style="position: relative; left: 1em"><img src="../../icons/pdf.png" /></a></small>
  </header>
  <main>
  <div id="front-matter">
    <h1 id="article-title">Simulation Framework for Asynchronous Iterative
      Methods</h1>
    <div id="authors">
      <address typeof="Person">
        <div property="name">
          Evan C. Coleman<sup><img src="../../icons/envelope.png"></sup>
        </div>
        <div property="email">
          <a href="mailto:evanccoleman@gmail.com">evanccoleman@gmail.com</a>
        </div>
      </address>
      <address typeof="Person">
        <div property="name">Erik Jensen</div>
        <div property="email">
          <a href="mailto:ejens005@odu.edu">ejens005@odu.edu</a>
        </div>
      </address>
      <address style="display:block; margin: 0.5em auto" typeof="Person">
        <div property="name">Masha Sosonkina</div>
        <div property="email">
          <a href="mailto:msosonki@odu.edu">msosonki@odu.edu</a>
        </div>
      </address>
      <div class="affiliation">Old Dominion University, Norfolk, VA,
        United States</div>
    </div>
    <div id="acm-subject-categories">
      <h1>ACM Subject Categories</h1>
      <ul>
        <li><code>TBD~TBD</code></li>
      </ul>
    </div>
    <div id="keywords">
      <h1>Keywords</h1>
      <ul class="list-inline comma-separated">
        <li>Asynchronous iterative methods</li>
        <li>Fault tolerance</li>
        <li>Asynchronous simulation</li>
        <li>Shared memory</li>
        <li>Intel<sup>&reg;</sup> Xeon Phi<sup>&trade;</sup></li>
      </ul>
    </div>
  </div>
  <section role="doc-abstract">
    <h1>Abstract</h1>
    <p>
      As high-performance computing (HPC) platforms progress towards exascale,
      computational methods must be revamped to successfully leverage them. In
      particular, (1) asynchronous methods become of great importance because
      synchronization becomes prohibitively expensive and (2) resilience of
      computations must be achieved, e.g., using checkpointing selectively which
      may otherwise become prohibitively expensive due to the sheer scale of the
      computing environment. In this work, a simulation framework for
      asynchronous iterative methods is proposed and tested on HPC accelerator
      (shared-memory) architecture. The design proposed here offers a
      lightweight alternative to existing computational frameworks to allow for
      easy experimentation with various relaxation iterative techniques,
      solution updating schemes, and predicted performance. The simulation
      framework is implemented in MATLAB<sup>&reg;</sup> using function handles,
      which offers a modular and easily extensible design. An example of a case
      study using the simulation framework is presented to examine the efficacy
      of different checkpointing schemes for asynchronous relaxation methods.
    </p>
  </section>
  <section id="Sect1">
    <h1>Introduction</h1>
    <p>
      Asynchronous iterative methods are increasing in popularity recently due
      to their ability to be parallelized naturally on modern co-processors such
      as GPUs and Intel<sup>&reg;</sup> Xeon Phi<sup>&trade;</sup>. Many
      examples of recent work using fine-grained parallel methods are available
      (see <a role="doc-biblioref" href="#ADQO16">Anzt, Dongarra, &amp;
        Quintana-Ortí, 2016</a>, <a role="doc-biblioref" href="#Anz12">Anzt,
        2012</a>, <a role="doc-biblioref" href="#CAD15">Chow, Anzt, &amp;
        Dongarra, 2015</a>, <a role="doc-biblioref" href="#CP15">Chow, &amp;
        Patel, 2015</a>, <a role="doc-biblioref" href="#ACD15">Anzt, Chow, &amp;
        Dongarra, 2015</a> and many others in <a href="#Sect2">Section 2</a>). A
      specific area of interest is on techniques that utilize fixed point
      iteration, i.e., those techniques that solve equations of the form
    </p>
    <p>$$x = G\left( x \right)\label{a}\tag{1}$$</p>
    <p>
      for some vector
      <math xmlns="http://www.w3.org/1998/Math/MathML">
    <mi>x</mi>
    <mo>&isin;</mo>
    <mi>D</mi>
    </math>
      and some map
      <math xmlns="http://www.w3.org/1998/Math/MathML">
      <mi>G</mi>
      <mo>:</mo>
      <mi>D</mi>
      <mo>&rarr;</mo>
      <mi>D</mi>
      <mo>.</mo>
      </math>
      These techniques are well suited for fine-grained computation and they can
      be executed either synchronously or asynchronously, which helps tolerate
      latency in high-performance computing (HPC) environments. Looking forward
      to the future of HPC, it is important to prioritize the develop of
      algorithms that are resilient to faults since on future platforms, the
      rate at which faults occur is expected to increase dramatically (<a
        role="doc-biblioref" href="#CGG+09">Cappello, et al., 2009</a>; <a
        role="doc-biblioref" href="#CGG+14">Cappello, et al., 2014</a>; <a
        role="doc-biblioref" href="#ABC+06">Asanović, et al., 2006</a>; <a
        role="doc-biblioref" href="#GL09">Geist, &amp; Lucas, 2009</a>).
    </p>
    <p>
      While many asynchronous methods are designed for shared memory
      architectures and asynchronous iterative methods have gained popularity
      for their efficient use of resources on shared memory accelerators in
      modern HPC environments (<a role="doc-biblioref" href="#VV09">Venkatasubramanian,
        &amp; Vuduc, 2009</a>), lately there has been some work done at improving
      the performance of asynchronous iterative methods in distributed memory
      environments. Such works include attempts to implement asynchronous
      iterative methods in MPI-3 using one sided remote memory access (<a
        role="doc-biblioref" href="#GBH14">Gerstenberger, Besta, &amp;
        Hoefler, 2014</a>) as well as efforts to reduce the cost of communication in
      these environments (<a role="doc-biblioref" href="#WPC16">Wolfson-Pou,
        &amp; Chow, 2016</a>).
    </p>
    <p>Developing algorithms that are resilient to faults is of paramount
      importance, and fine-grained parallel fixed point methods are no
      exception. In this paper, we propose a simulation framework that can help
      developing algorithms resilient to faults. These types of frameworks allow
      for experimentation that is not specific to any singular platform or
      hardware architectures and allows experiments to simulate performance on
      both current computing environments and look at how those results may
      continue to evolve along with the computer hardware. Hence, they enable
      the possibility to: (1) test and validate different fault-models (which
      are still emerging), (2) experiment with different checkpointing
      libraries/mechanisms, and (3) help in efficiently implementing
      asynchronous iterative methods. Additionally, it can be difficult to
      implement asynchronous iterative methods on a variety of architectures to
      observe performance behavior in different computing environments, and
      having a working simulation framework allows users to conduct extensive
      experiments without any major programming investment.</p>
    <p>This study aims to develop a simulation framework that is focused on
      the performance of asynchronous iterative methods. The goal is to produce
      a lightweight computational framework capable of being used for various
      asynchronous iterative methods, with an emphasis on methods for solving
      linear systems, and simulating the performance of these methods on shared
      memory devices. The contributions of this work are</p>
    <ol>
      <li>the development, testing, and validation of a modular simulation
        framework for asynchronous iterative methods that can be used in the
        creation of new and improved algorithms,</li>
      <li>a process for the generation of time models from HPC
        implementation code, which may be used to initialize the framework,</li>
      <li>a case study on how to use the framework in the development of
        fault tolerant algorithms, and</li>
      <li>a comparison of several implementations of an asynchronous
        iterative relaxation method, used in the proposed framework.</li>
    </ol>
    <p>The simulation framework developed here is capable of predicting
      performance on various HPC system configurations and to show the
      performance of an algorithm subject to resiliency or reproducibility
      requirements.</p>
    <p>
      The rest of this paper is organized as follows. <a href="#Sect2">Section
        2</a> provides a brief summary of related studies. <a href="#Sect3">Section
        3</a> gives an overview of asynchronous iterative methods, while <a
        href="#Sect4">Section 4</a> describes the design and utilization of the
      simulation framework in modeling the behavior of these methods. <a
        href="#Sect5">Section 5</a> describes a process for collecting time data
      from HPC implementations and developing time models from the data for use
      in the simulation framework. A comparison of different implementations is
      given in <a href="#Sect5.3">Section 5.3</a>, while framework validation is
      considered in <a href="#Sect5.4">Section 5.4</a>. <a href="#Sect6">Section
        6</a> gives background information related to the creation of efficient
      checkpointing routines and provides a series of numerical results. <a
        href="#Sect7">Section 7</a> provides conclusions.
    </p>
  </section>

  <section id="Sect2">
    <h1>Related Work</h1>
    <p>
      Development of computational frameworks for the purposes of simulating
      performance has a long history in the literature. Examples of such
      frameworks include SimGrid (<a role="doc-biblioref" href="#Cas01">Casanova,
        2001</a>; <a role="doc-biblioref" href="#CLQ08">Casanova, Legrand, &amp;
        Quinson, 2008</a>) that focuses on distributed experiments, GangSim (<a
        role="doc-biblioref" href="#DF05">Dumitrescu, &amp; Foster, 2005</a>)
      and GridSim (<a role="doc-biblioref" href="#BM02">Buyya, &amp;
        Murshed, 2002</a>) that focus on grid scheduling, and CloudSim (<a
        role="doc-biblioref" href="#CRDRB09">Calheiros, et al., 2009</a>; <a
        role="doc-biblioref" href="#CRB+11">Calheiros, et al., 2011</a>) that
      models performance of cloud computing environments. These environments
      focus on specific HPC implementation features, such as job scheduling and
      data movement, and on providing a view of how the systems themselves
      behave in HPC-like scenarios. On the other hand, the framework developed
      here is intended to simulate not only the HPC performance but also the
      algorithmic performance of a particular class of problems (e.g., iterative
      convergence to a linear system solution) under various system
      configurations (e.g., asynchronous thread behavior in shared-memory
      systems) and with various additional requirements (e.g., resiliency or
      reproducibility).
    </p>
    <p>
      The class of problems that the framework proposed in this study addresses
      are stationary solvers, also referred to as relaxation methods. The focus
      is on the behavior of these methods in asynchronous computing
      environments. However, the framework also easily admits synchronous
      updates; the key is the fine-grained nature of the algorithm. Fine-grained
      parallel methods, specifically parallel fixed point methods, are an area
      of increased research activity due to their practical use on HPC
      environments. An initial exploration of fault tolerance for stationary
      iterative linear solvers (i.e., Jacobi) is given in (<a
        role="doc-biblioref" href="#ADQO15">Anzt, Dongarra, &amp;
        Quintana-Ortí, 2015</a>) and expanded on (<a role="doc-biblioref"
        href="#ADQO16">Anzt, Dongarra, &amp; Quintana-Ortí, 2016</a>). The
      general convergence of parallel fixed point methods has been explored
      extensively (<a role="doc-biblioref" href="#AB05">Addou, &amp;
        Benahmed, 2005</a>; <a role="doc-biblioref" href="#FS00">Frommer, &amp;
        Szyld, 2000</a>; <a role="doc-biblioref" href="#BT89">Bertsekas, &amp;
        Tsitsiklis, 1989</a>; <a role="doc-biblioref" href="#OR00">Ortega, &amp;
        Rheinboldt, 2000</a>; <a role="doc-biblioref" href="#Bau78">Baudet, 1978</a>;
      <a role="doc-biblioref" href="#Ben07">Benahmed, 2007</a>).
    </p>
    <p>
      Examples of work examining the performance of asynchronous iterative
      methods include an in-depth analysis from the perspective of utilizing a
      system with a co-processor (<a role="doc-biblioref" href="#Anz12">Anzt,
        2012</a>; <a role="doc-biblioref" href="#ADG15">Avron, Druinsky, &amp;
        Gupta, 2015</a>), as well as performance analysis of asynchronous methods (<a
        role="doc-biblioref" href="#BBDH11">Bethune, et al., 2011</a>; <a
        role="doc-biblioref" href="#BBDH14">Bethune, et al., 2014</a>; <a
        role="doc-biblioref" href="#HD18">Hook, &amp; Dingle, 2018</a>). In
      particular, both (<a role="doc-biblioref" href="#BBDH11">Bethune, et
        al., 2011</a>) and (<a role="doc-biblioref" href="#BBDH14">Bethune, et
        al., 2014</a>) focus on low level analysis of the asynchronous Jacobi
      method, similar to the example problem that is featured here. While many
      recent research results for asynchronous iterative methods are focused on
      implementations that utilize a shared memory architecture, one area of
      asynchronous iterative methods that has seen significant development using
      a distributed memory architecture is optimization (<a role="doc-biblioref"
        href="#CC16">Cheung, &amp; Cole, 2016</a>; <a role="doc-biblioref"
        href="#IBCH13">Iutzeler, et al., 2013</a>; <a role="doc-biblioref"
        href="#Hon17">Hong, 2017</a>; <a role="doc-biblioref" href="#ZC10">Zhong,
        &amp; Cassandras, 2010</a>; <a role="doc-biblioref" href="#SN11">Srivastava,
        &amp; Cassandras, 2011</a>; <a role="doc-biblioref" href="#TBA86">Tsitsiklis,
        Bertsekas, &amp; Athans, 1986</a>; <a role="doc-biblioref" href="#BPC+11">Boyd,
        et al., 2011</a>).
    </p>
    <p>
      The use case of the simulation framework that is featured in <a
        href="#Sect6">Section 6</a> shows the ability of the framework to be
      used in the development of fault tolerance techniques. The development of
      these techniques is important as HPC platforms continue to become both
      larger and more susceptible to faults. The expected increase in faults for
      future HPC systems is detailed in a variety of different sources. A high
      level article detailing the expected increase in failure rate from a
      reasonably non-technical point of view is available in the various
      versions of the "Monster in the Closet" talk and paper (<a
        role="doc-biblioref" href="#Gei11">Geist, 2011</a>; <a
        role="doc-biblioref" href="#Gei12">Geist, 2012</a>; <a
        role="doc-biblioref" href="#Gei16">Geist, 2016</a>). More technical and
      detailed reports are given in a variety of sources composed of groups of
      different researchers from both academia and industry (<a
        role="doc-biblioref" href="#ABC+06">Asanović, et al., 2006</a>; <a
        role="doc-biblioref" href="#CGG+09">Cappello, et al., 2009</a>; <a
        role="doc-biblioref" href="#CGG+14">Cappello, et al., 2014</a>; <a
        role="doc-biblioref" href="#SWA+14">Snir, et al., 2014</a>; <a
        role="doc-biblioref" href="#GL09">Geist, &amp; Lucas, 2009</a>).
      Additionally, the Department of Energy has commissioned two very detailed
      reports about the progression towards exascale level computing; one from a
      general computing standpoint (<a role="doc-biblioref" href="#ABC+10">Ashby,
        et al., 2010</a>), and a report aimed specifically at applied mathematics
      for exascale computing (<a role="doc-biblioref" href="#DHB+14">Dongarra,
        et al., 2014</a>). Fault tolerance for fine-grained asynchronous iterative
      methods has been studied at a fundamental level (<a role="doc-biblioref"
        href="#Gär99">Gärtner, 1999</a>; <a role="doc-biblioref" href="#CS17">Coleman,
        &amp; Sosonkina, 2017</a>), as well as made specific to certain algorithms (<a
        role="doc-biblioref" href="#CSC17">Coleman, Sosonkina, &amp; Chow,
        2017</a>; <a role="doc-biblioref" href="#CS18">Coleman, &amp; Sosonkina,
        2018</a>; <a role="doc-biblioref" href="#ADQO15">Anzt, Dongarra, &amp;
        Quintana-Ortí, 2015</a>; <a role="doc-biblioref" href="#ADQO16">Anzt,
        Dongarra, &amp; Quintana-Ortí, 2016</a>). Fault tolerance for synchronous
      fixed point algorithms from a numerical analysis has been investigated in
      (<a role="doc-biblioref" href="#SW15">Stoyanov, &amp; Webster, 2015</a>).
      Error correction for GPU based oriented asynchronous methods were
      investigated in (<a role="doc-biblioref" href="#ALDH12">Anzt, et al.,
        2012</a>).
    </p>
    <p>
      Several numerically based fault models similar to the one that is used in
      this study have been developed recently, and are used as a basis for the
      generalized fault simulation that is developed here. These include a
      "numerical" fault model that is predicated on shuffling the components of
      an important data structure (<a role="doc-biblioref" href="#EHM15">Elliott,
        Hoemmen, &amp; Mueller, 2015</a>), and a perturbation based model put forth
      in (<a role="doc-biblioref" href="#SW15">Stoyanov, &amp; Webster, 2015</a>)
      and (<a role="doc-biblioref" href="#CS16b">Coleman, &amp; Sosonkina,
        2016b</a>). Other models that are not based upon directly injecting a bit
      flip, such as inducing a small shift to a single component of a vector
      have been considered as well (<a role="doc-biblioref" href="#HH11">Hoemmen,
        &amp; Heroux, 2011</a>; <a role="doc-biblioref" href="#BFHH12">Bridges,
        et al., 2012</a>). Comparisons between various numerical soft fault models
      have been made in (<a role="doc-biblioref" href="#CS16a">Coleman,
        &amp; Sosonkina, 2016a</a>; <a role="doc-biblioref" href="#CJB+18">Coleman,
        et al., 2018</a>).
    </p>
  </section>

  <section id="Sect3">
    <h1>Asynchronous iterative methods</h1>
    <p>
      In fine-grained parallel computation, each component of the problem (i.e.,
      a matrix or vector entry) is updated in a manner that does not require
      information from the computations involving other components while the
      update is being made. This allows for each computing element (e.g., for a
      single processor, CUDA core, or Xeon Phi<sup>&trade;</sup> core) to act
      independently from all other computing elements. Depending on the size of
      both the problem and the computing environment, each computing element may
      be responsible for updating a single entry to update, or may be assigned a
      block that contains multiple components. The generalized mathematical
      model that is used throughout this paper comes from (<a
        role="doc-biblioref" href="#FS00">Frommer, &amp; Szyld, 2000</a>), which
      in turn comes from (<a role="doc-biblioref" href="#CM69">Chazan, &amp;
        Miranker, 1969</a>; <a role="doc-biblioref" href="#Bau78">Baudet, 1978</a>;
      <a role="doc-biblioref" href="#Szy98">Szyld, 1998</a>) among many others.
    </p>
    <p>
      To keep the mathematical model as general as possible, consider a
      function,
      <math xmlns="http://www.w3.org/1998/Math/MathML">
      <mi>G</mi>
      <mo>:</mo>
      <mi>D</mi>
      <mo>&rarr;</mo>
      <mi>D</mi>
      <mo>.</mo>
      </math>
      where
      <math xmlns="http://www.w3.org/1998/Math/MathML">
      <mi>D</mi>
      </math>
      is a domain that represents a product space
      <math xmlns="http://www.w3.org/1998/Math/MathML">
      <mi>D</mi>
      <mo>=</mo>
      <msub>
      <mi>D</mi>
      <mn>1</mn>
      </msub>
      <mo>&times;</mo>
      <msub>
      <mi>D</mi>
      <mn>2</mn>
      </msub>
      <mo>&times;</mo>
      <mo>&hellip;</mo>
      <mo>&times;</mo>
      <msub>
      <mi>D</mi>
      <mi>m</mi>
      </msub>
      <mo>.</mo>
      </math>
      The goal is to find a fixed point of the function
      <math xmlns="http://www.w3.org/1998/Math/MathML">
      <mi>G</mi>
      </math>
      inside of the domain
      <math xmlns="http://www.w3.org/1998/Math/MathML">
      <mi>D</mi>
      <mo>.</mo>
      </math>
      To this end, a fixed point iteration is performed such that
    </p>
    <p>$$x^{k+1} = G \left( x^{k} \right),\label{b}\tag{2}$$</p>
    <p>
      and a fixed point is declared if
      <math xmlns="http://www.w3.org/1998/Math/MathML">
      <msup>
      <mi>x</mi>
      <mrow>
      <mi>k</mi>
      <mo>+</mo>
      <mn>1</mn>
      </mrow>
      </msup>
      <mo>&asymp;</mo>
      <msup>
      <mi>x</mi>
      <mi>k</mi>
      </msup>
      <mo>.</mo>
      </math>
      Note that the function
      <math xmlns="http://www.w3.org/1998/Math/MathML">
      <mi>G</mi>
      </math>
      has internal component functions
      <math xmlns="http://www.w3.org/1998/Math/MathML">
      <msub>
      <mi>G</mi>
      <mi>i</mi>
      </msub>
      </math>
      for each sub-domain,
      <math xmlns="http://www.w3.org/1998/Math/MathML">
      <msub>
      <mi>D</mi>
      <mi>i</mi>
      </msub>
      <mo>,</mo>
      </math>
      in the product space,
      <math xmlns="http://www.w3.org/1998/Math/MathML">
      <mi>D</mi>
      <mo>.</mo>
      </math>
      In particular,
      <math xmlns="http://www.w3.org/1998/Math/MathML">
      <msub>
      <mi>G</mi>
      <mi>i</mi>
      </msub>
      <mo>:</mo>
      <mi>D</mi>
      <mo>&rarr;</mo>
      <msub>
      <mi>D</mi>
      <mi>i</mi>
      </msub>
      <mo>,</mo>
      </math>
      which gives that
    </p>
    <p>$$ \begin{eqnarray} x = \left( x_{1}, x_{2}, \ldots, x_{m} \right)
      \in D \longrightarrow G\left( x \right) &=& G\left( x_{1}, x_{2}, \ldots,
      x_{m} \right)\label{c}\tag{3} \\ &=& \left( G_{1}\left(x \right),
      G_{2}\left(x \right), \ldots, G_{m}\left(x \right) \right) \in
      D\label{d}\tag{4} \end{eqnarray} $$</p>
    <p>
      As a concrete example, let each
      <math xmlns="http://www.w3.org/1998/Math/MathML">
      <msub>
      <mi>D</mi>
      <mi>i</mi>
      </msub>
      <mo>=</mo>
      <mo>&reals;</mo>
      <mo>.</mo>
      </math>
      Forming the product space of each of these
      <math xmlns="http://www.w3.org/1998/Math/MathML">
      <msub>
      <mi>D</mi>
      <mi>i</mi>
      </msub>
      <mo>'s</mo>
      </math>
      gives that
      <math xmlns="http://www.w3.org/1998/Math/MathML">
      <mi>D</mi>
      <mo>=</mo>
      <msup>
      <mo>&reals;</mo>
      <mi>m</mi>
      </msup>
      <mo>.</mo>
      </math>
      This leads to the more formal functional mapping,
      <math xmlns="http://www.w3.org/1998/Math/MathML">
      <mi>f</mi>
      <mo>:</mo>
      <msup>
      <mi>&reals;</mi>
      <mi>m</mi>
      </msup>
      <mo>&rarr;</mo>
      <msup>
      <mi>&reals;</mi>
      <mi>m</mi>
      </msup>
      <mo>.</mo>
      </math>
      Additionally, let
      <math xmlns="http://www.w3.org/1998/Math/MathML">
      <mi>f</mi>
      <mfenced open="(" close=")">
      <mi>x&#8407;</mi>
      </mfenced>
      <mo>=</mo>
      <mn>2</mn>
      <mi>x&#8407;</mi>
      <mo>.</mo>
      </math>
      In this case, each of the individual
      <math xmlns="http://www.w3.org/1998/Math/MathML">
      <msub>
      <mi>f</mi>
      <mi>i</mi>
      </msub>
      </math>
      component functions is defined by
      <math xmlns="http://www.w3.org/1998/Math/MathML">
      <mi>f</mi>
      <mfenced open="(" close=")">
      <mi>x&#8407;</mi>
      </mfenced>
      <mo>=</mo>
      <mn>2</mn>
      <msub>
      <mi>x</mi>
      <mi>i</mi>
      </msub>
      <mo>.</mo>
      </math>
      Note that each component functions operates on all of the vector
      <math xmlns="http://www.w3.org/1998/Math/MathML">
      <mi>x&#8407;</mi>
      </math>
      even if the individual function definition does not require all of the
      components of
      <math xmlns="http://www.w3.org/1998/Math/MathML">
      <mi>x&#8407;</mi>
      </math>
      to perform its specific update.
    </p>
    <p>
      The assumption is also made that there is some finite number of processing
      elements
      <math xmlns="http://www.w3.org/1998/Math/MathML">
      <msub>
      <mi>P</mi>
      <mn>1</mn>
      </msub>
      <mo>,</mo>
      <msub>
      <mi>P</mi>
      <mn>2</mn>
      </msub>
      <mo>,</mo>
      <mo>&hellip;</mo>
      <mo>,</mo>
      <msub>
      <mi>P</mi>
      <mn>p</mn>
      </msub>
      </math>
      each of which is assigned to a block
      <math xmlns="http://www.w3.org/1998/Math/MathML">
      <mi>B</mi>
      </math>
      of components
      <math xmlns="http://www.w3.org/1998/Math/MathML">
      <msub>
      <mi>B</mi>
      <mn>1</mn>
      </msub>
      <mo>,</mo>
      <msub>
      <mi>B</mi>
      <mn>2</mn>
      </msub>
      <mo>,</mo>
      <mo>&hellip;</mo>
      <mo>,</mo>
      <msub>
      <mi>B</mi>
      <mn>m</mn>
      </msub>
      </math>
      to update. Note that the number
      <math xmlns="http://www.w3.org/1998/Math/MathML">
      <mi>p</mi>
      </math>
      of processing elements will typically be significantly smaller than the
      number
      <math xmlns="http://www.w3.org/1998/Math/MathML">
      <mi>m</mi>
      </math>
      of blocks to update. With these assumptions, the computational model can
      be stated in <a href="#Alg1">Algorithm 1</a>.
    </p>
    <div id="Alg1">
      <pre id="Alg1t" style="display: none">
        \begin{algorithm}
        \caption{General computational model}
        \begin{algorithmic}
          \FOR{each processing element $P_{l}$}
            \FOR{$i = 1, 2, \ldots$ until convergence}
              \STATE Read $x$ from common memory
              \STATE Compute $x_{j}^{i+1} = G_{j}(x)$ for all  $j \in \text{\ss}_{l}$
              \STATE Update $x_{j}$ in common memory with $x_{j}^{i+1}$ for all $j \in \text{\ss}_{l}$
            \ENDFOR
          \ENDFOR
        \end{algorithmic}
        \end{algorithm}
      </pre>
    </div>
    <script>
          var alg1 = document.getElementById( "Alg1t" ).textContent;
          pseudocode.render( alg1, document.getElementById( "Alg1" ), {
            lineNumber: true
          } );
        </script>
    <p>
      This computational model has each processing element read all pertinent
      data from global memory that is accessible by each of the processors,
      update the pieces of data specific to the component functions that it has
      been assigned, and update those components in the global memory. Note that
      the computational model presented in <a href="#Alg1">Algorithm 1</a>
      allows for either synchronous or asynchronous computation; it only
      prescribes that an update has to be made as an "atomic" operation (in line
      5), i.e., without interleaving of its result. If each processing element
      <math xmlns="http://www.w3.org/1998/Math/MathML">
      <msub>
      <mi>P</mi>
      <mi>l</mi>
      </msub>
      </math>
      is to wait for the other processors to finish each update, then the model
      describes a parallel synchronous form of computation. On the other hand,
      if no order is established for
      <math xmlns="http://www.w3.org/1998/Math/MathML">
      <msub>
      <mi>P</mi>
      <mi>l</mi>
      </msub>
      <mo>s</mo>
      <mo>,</mo>
      </math>
      then an asynchronous form of computation arises.
    </p>
    <p>
      To continue formalizing this computational model a few more definitions
      are necessary. First, set a global iteration counter
      <math xmlns="http://www.w3.org/1998/Math/MathML">
      <mi>k</mi>
      </math>
      that increases <em>every</em> time any processor reads
      <math xmlns="http://www.w3.org/1998/Math/MathML">
      <mi>x&#8407;</mi>
      </math>
      from common memory. At the end of the work done by any individual
      processor,
      <math xmlns="http://www.w3.org/1998/Math/MathML">
      <mi>p</mi>
      <mo>,</mo>
      </math>
      the components associated with the block
      <math xmlns="http://www.w3.org/1998/Math/MathML">
      <msub>
      <mi>B</mi>
      <mi>p</mi>
      </msub>
      </math>
      will be updated. This results in a vector,
      <math xmlns="http://www.w3.org/1998/Math/MathML">
      <mi>x&#8407;</mi>
      <mo>=</mo>
      <mfenced open="(" close=")">
      <msubsup>
      <mi>x</mi>
      <mn>1</mn>
      <mrow>
      <msub>
      <mi>s</mi>
      <mn>1</mn>
      </msub>
      <mfenced open="(" close=")">
      <mo>x</mo>
      </mfenced>
      </mrow>
      </msubsup>
      <msubsup>
      <mi>x</mi>
      <mn>2</mn>
      <mrow>
      <msub>
      <mi>s</mi>
      <mn>2</mn>
      </msub>
      <mfenced open="(" close=")">
      <mo>x</mo>
      </mfenced>
      </mrow>
      </msubsup>
      <mo>&hellip;</mo>
      <msubsup>
      <mi>x</mi>
      <mi>m</mi>
      <mrow>
      <msub>
      <mi>s</mi>
      <mi>m</mi>
      </msub>
      <mfenced open="(" close=")">
      <mo>x</mo>
      </mfenced>
      </mrow>
      </msubsup>
      </mfenced>
      </math>
      where the function
      <math xmlns="http://www.w3.org/1998/Math/MathML">
      <msub>
      <mi>s</mi>
      <mi>l</mi>
      </msub>
      <mfenced open="(" close=")">
      <mo>k</mo>
      </mfenced>
      </math>
      indicates how many times an specific component has been updated. Finally,
      a set of individual components can be grouped into a set,
      <math xmlns="http://www.w3.org/1998/Math/MathML">
      <msup>
      <mi>I</mi>
      <mi>k</mi>
      </msup>
      <mo>,</mo>
      </math>
      that contains all of the components that were updated on the
      <math xmlns="http://www.w3.org/1998/Math/MathML">
      <msup>
      <mi>k</mi>
      <mi>th</mi>
      </msup>
      </math>
      iteration. Given these basic definitions, the three following conditions
      (along with the model presented in <a href="#Alg1">Algorithm 1</a>)
      provide a working mathematical framework for fine-grained asynchronous
      computation.
    </p>
    <p id="Def1">
      <strong>Definition 1.</strong> If the following three conditions hold
    </p>
    <ol>
      <li><math xmlns="http://www.w3.org/1998/Math/MathML">
      <msub>
      <mi>s</mi>
      <mi>i</mi>
      <mfenced open="(" close=")">
      <mi>k</mi>
      </mfenced>
      </msub>
      <mo>&le;</mo>
      <mi>k</mi>
      <mo>-</mo>
      <mn>1</mn>
      <mo>,</mo>
      </math> i.e., only components that have finished computing are used in the
        current approximation.</li>
      <li><math xmlns="http://www.w3.org/1998/Math/MathML">
      <msub>
      <mtext>lim</mtext>
      <mrow>
      <mi>k</mi>
      <mo>&rarr;</mo>
      <mn>&infin;</mn>
      </mrow>
      </msub>
      <msub>
      <mi>s</mi>
      <mi>i</mi>
      <mfenced open="(" close=")">
      <mi>k</mi>
      </mfenced>
      </msub>
      <mo>=</mo>
      <mn>&infin;</mn>
      <mo>,</mo>
      </math> i.e., the newest updates for each component are used.</li>
      <li><math xmlns="http://www.w3.org/1998/Math/MathML">
      <mfenced open="|" close="|">
      <mrow>
      <mi>k</mi>
      <mo>&isin;</mo>
      <mo>&naturals;</mo>
      <mo>:</mo>
      <mi>i</mi>
      <mo>&isin;</mo>
      <msup>
      <mi>I</mi>
      <mi>k</mi>
      </msup>
      </mrow>
      </mfenced>
      <mo>=</mo>
      <mn>&infin;</mn>
      <mo>,</mo>
      </math> i.e., all components will continue to be updated.</li>
    </ol>
    <p>
      Then given an initial
      <math xmlns="http://www.w3.org/1998/Math/MathML">
      <msup>
      <mi>x&#8407;</mi>
      <mn>0</mn>
      </msup>
      <mo>&isin;</mo>
      <mi>D</mi>
      <mo>,</mo>
      </math>
      the iterative update process defined by
    </p>
    <p>$$ x^{k}_{i} = \begin{cases} x^{k-1}_{i} & i \notin I_{k} \\
      G_{i}\left( \vec{x} \right) & i \in I_{k} \end{cases} $$</p>
    <p>
      where the function
      <math xmlns="http://www.w3.org/1998/Math/MathML">
      <msub>
      <mi>G</mi>
      <mi>i</mi>
      </msub>
      <mfenced open="(" close=")">
      <mi>x&#8407;</mi>
      </mfenced>
      </math>
      uses the latest updates available is called an asynchronous iteration.
    </p>
    <p>
      This basic computational model (i.e., the combination of <a href="#Alg1">Algorithm
        1</a> and <a href="#Def1">Definition 1</a> together) allows for many
      different results on fine-grained iterative methods that are both
      synchronous and asynchronous, though the three conditions given in <a
        href="#Def1">Definition 1</a> are unnecessary in the synchronous case.
    </p>

    <section id="Sect3.1">
      <h1>Asynchronous relaxation methods</h1>

      <p>
        Relaxation methods have been the focus of many of the works mentioned in
        <a href="Sect2">Section 2</a> such as (<a role="doc-biblioref"
          href="#CM69">Chazan, &amp; Miranker, 1969</a>) and (<a
          role="doc-biblioref" href="#Bau78">Baudet, 1978</a>); a much more
        detailed description can be found in (<a role="doc-biblioref"
          href="#BT89">Bertsekas, &amp; Tsitsiklis, 1989</a>) among many other
        sources. This section provides an introduction that will serve as a
        reference for the later work in this study.
      </p>
      <p>Relaxation methods can be expressed as general fixed point
        iterations of the form</p>
      <p>$$ x^{k+1} = Cx^{k} + d \label{e}\tag{5} $$
      <p>
        where
        <math xmlns="http://www.w3.org/1998/Math/MathML">
      <mi>C</mi>
      </math>
        is the
        <math xmlns="http://www.w3.org/1998/Math/MathML">
      <mi>n</mi>
      <mo>&times;</mo>
      <mi>n</mi>
      </math>
        iteration matrix,
        <math xmlns="http://www.w3.org/1998/Math/MathML">
      <mi>x</mi>
      </math>
        is an
        <math xmlns="http://www.w3.org/1998/Math/MathML">
      <mi>x</mi>
      <mtext>-dimensional</mtext>
      </math>
        vector that represents the solution, and
        <math xmlns="http://www.w3.org/1998/Math/MathML">
      <mi>d</mi>
      </math>
        is another
        <math xmlns="http://www.w3.org/1998/Math/MathML">
      <mi>x</mi>
      <mtext>-dimensional</mtext>
      </math>
        vector that can be used to help define the particular problem at hand.
      </p>
      <p>The Jacobi method is an asynchronous relaxation method built for
        solving linear systems of the form</p>
      <p>$$ Ax = b,\label{f}\tag{6} $$
      <p>
        and following the methodology put forth in (<a role="doc-biblioref"
          href="#BT89">Bertsekas, &amp; Tsitsiklis, 1989</a>), this can be
        broken down to view a specific row - say the
        <math xmlns="http://www.w3.org/1998/Math/MathML">
        <msup>
        <mi>i</mi>
        <mo>th</mo>
        </msup>
        </math>
        - of the matrix
        <math xmlns="http://www.w3.org/1998/Math/MathML">
        <mi>A</mi>
        <mo>,</mo>
        </math>
      </p>
      <p>$$ \sum^{n}_{j=1} a_{ij}x_{j} = b_{i},\label{g}\tag{7} $$</p>
      <p>
        and this equation can be solved for the
        <math xmlns="http://www.w3.org/1998/Math/MathML">
        <msup>
        <mi>i</mi>
        <mo>th</mo>
        </msup>
        </math>
        component of the solution,
        <math xmlns="http://www.w3.org/1998/Math/MathML">
        <msub>
        <mi>x</mi>
        <mo>i</mo>
        </msub>
        <mo>,</mo>
        </math>
        to give,
      </p>
      <p>$$ x_{i} = \frac{-1}{a_{ii}} \left[ \sum_{j \neq i} a_{ij}x_{j} -
        b_{i} \right].\label{h}\tag{8} $$</p>
      <p>
        This equation can then be computed in an iterative manner in order to
        give successive updates to the solution vector. In synchronous computing
        environments, each update to an element of the solution vector,
        <math xmlns="http://www.w3.org/1998/Math/MathML">
        <msub>
        <mi>x</mi>
        <mo>i</mo>
        </msub>
        <mo>,</mo>
        </math>
        is computed sequentially using the same data for the other components of
        the solution vector (i.e., the
        <math xmlns="http://www.w3.org/1998/Math/MathML">
        <msub>
        <mi>x</mi>
        <mo>j</mo>
        </msub>
        <mo>,</mo>
        </math>
        in Eq. \eqref{h}). Conversely, in an asynchronous computing environment,
        each update to an element of the solution vector occurs when the
        computing element responsible for updating that component is ready to
        write the update to memory and the other components used are simply the
        latest ones available to the computing element.
      </p>
      <p>Expressing Eq. \eqref{h} in a block matrix form more similar to the
        original form of the iteration expressed in Eq. \eqref{e},</p>
      <p>$$ \begin{eqnarray} x &=& -D^{-1} \left( \left( L + U \right) x - b
        \right)\label{i}\tag{9} \\ &=& -D^{-1} \left( L + U \right) x + D^{-1}
        b,\label{j}\tag{10} \end{eqnarray} $$</p>
      <p>
        where
        <math xmlns="http://www.w3.org/1998/Math/MathML">
        <mi>D</mi>
        </math>
        is the diagonal portion of
        <math xmlns="http://www.w3.org/1998/Math/MathML">
        <mi>A</mi>
        </math>
        , and
        <math xmlns="http://www.w3.org/1998/Math/MathML">
        <mi>L</mi>
        </math>
        and
        <math xmlns="http://www.w3.org/1998/Math/MathML">
        <mi>U</mi>
        </math>
        are the strictly lower and upper triangular portions of
        <math xmlns="http://www.w3.org/1998/Math/MathML">
        <mi>A</mi>
        </math>
        respectively. This gives an iteration matrix of
        <math xmlns="http://www.w3.org/1998/Math/MathML">
        <mi>D</mi>
        <mo>=</mo>
        <mo>-</mo>
        <msup>
        <mi>D</mi>
        <mrow>
        <mo>-</mo>
        <mn>1</mn>
        </mrow>
        </msup>
        <mfenced open="(" close=")">
        <mrow>
        <mi>L</mi>
        <mo>+</mo>
        <mi>U</mi>
        </mrow>
        </mfenced>
        <mo>.</mo>
        </math>
      </p>
      <p>
        Convergence of asynchronous fixed point methods of the form presented in
        Eq. \eqref{e} is determined by the spectral radius of the iteration
        matrix, C, and dates back to the pioneering work done by both (<a
          role="doc-biblioref" href="#CM69">Chazan, &amp; Miranker, 1969</a>)
        and (<a role="doc-biblioref" href="#Bau78">Baudet, 1978</a>):
      </p>
      <p id="Theo1">
        <strong>Theorem 1.</strong> <em>For a fixed point iteration of the
          form given in Eq. \eqref{e} that adheres to the asynchronous
          computational model provided by <a href="#Alg1">Algorithm 1</a> and <a
          href="#Def1">Definition 1</a>, if the spectral radius of <math
            xmlns="http://www.w3.org/1998/Math/MathML">
            <mi>C</mi>
            </math>, <math xmlns="http://www.w3.org/1998/Math/MathML">
            <mi>&rho;</mi>
            <mfenced open="(" close=")">
            <mfenced open="|" close="|">
            <mi>C</mi>
            </mfenced>
            </mfenced>
            <mo>,</mo>
            </math> is less than one, then the iterative method will converge to the
          fixed point solution
        </em>.
      </p>
      <p>
        As noted in (<a role="doc-biblioref" href="#WPC16">Wolfson-Pou,
          &amp; Chow, 2016</a>), the iteration matrix
        <math xmlns="http://www.w3.org/1998/Math/MathML">
<mi>C</mi></math>
        that is used in the Jacobi relaxation method serves as a worst case for
        relaxation methods of the form discussed here. However, because of the
        ubiquitous use of the Jacobi method in parallel solutions of large
        problems in many different domains in science and engineering we use the
        Asynchronous (Block) Jacobi method predominantly throughout the
        remainder of this study. Note that many of the concepts and ideas
        expressed in this paper can be easily adapted to more complex
        algorithms.
      </p>
    </section>
  </section>

  <section id="Sect4">
    <h1>Design of simulation framework</h1>
    <p>
      The simulation framework proposed here is designed to simulate the
      performance of an asynchronous iterative method operating on multiple
      computing elements using a single processing element. In this simulation
      framework, the emphasis is on fixed-point iterations<span class="fn"><sup><a
          href="#ftn1" id="fn_pointer_ftn1" role="doc-noteref"
          title="Footnote 1: Throughout the text, vector notation is occasionally adopted to emphasize when functions take all components of x as opposed to a single component, such as x1.">1</a></sup></span>
    </p>
    <p>$$ \vec{x} = G \left( \vec{x} \right),\label{k}\tag{11} $$</p>
    <p>
      for some
      <math xmlns="http://www.w3.org/1998/Math/MathML">
      <mi>x&#8407;</mi>
      <mo>&isin;</mo>
      <msup>
      <mi>&reals;</mi>
      <mi>n</mi>
      </msup>
      <mo>.</mo>
      </math>
      In the framework, certain components are assigned (possibly distinct)
      times for performing an update to their components, and the effects of
      various delay structures can be examined.
    </p>
    <p>
      The development of the present computational framework is shown by the
      flow diagram in <a href="#Fig1">Figure 1</a>, which is typical for
      computation frameworks, except for the third Timing Distributions stage. A
      mathematical formulation of a problem (e.g., as a set of equations) is
      presented first (Mathematical Model stage). The mathematical model is then
      implemented in an HPC environment (Parallel Implementation stage). Timing
      and algorithm-performance data (e.g., iterations to convergence) are
      collected from parallel executions on a subset of configurations and
      problem sizes, such that, in the proposed framework, timing distributions
      may be constructed (Timing Distributions stage) and used to simulate the
      performance of the mathematical model for target configurations and
      requirements. Since such simulations are faster and less-cumbersome to
      set-up, they allow for easy experimenting with variations of the
      underlying mathematical model, parallel implementation type and
      environment, or, eventually, in the expected performance.
    </p>
    <figure id="Fig1">
      <img src="./img/image1.png" />
      <figcaption>
        <strong>Figure 1.</strong> Stages in the proposed framework development
      </figcaption>
    </figure>
    <p>The simulation framework developed here works to simulate the
      performance of generic asynchronous relaxation methods in shared memory
      environments. The simulation framework can then be modified to reflect
      changes in the environment, or else can be utilized to demonstrate the
      effectiveness of algorithmic modifications.</p>
    <p>
      As a simple example, take
      <math xmlns="http://www.w3.org/1998/Math/MathML">
      <mi>n</mi>
      <mo>=</mo>
      <mn>2</mn>
      <mo>.</mo>
      </math>
      Then
      <math xmlns="http://www.w3.org/1998/Math/MathML">
      <mi>x&#8407;</mi>
      <mo>=</mo>
      <mfenced open="(" close=")">
      <msub>
      <mo>x</mo>
      <mn>1</mn>
      </msub>
      <msub>
      <mo>x</mo>
      <mn>2</mn>
      </msub>
      </mfenced>
      <mo>&isin;</mo>
      <msup>
      <mi>&reals;</mi>
      <mn>2</mn>
      </msup>
      </math>
      and, using the terminology of <a href="#Sect3">Section 3</a>,
    </p>
    <p>$$ \begin{eqnarray} x_{1} &=& G_{1} \left( \vec{x} \right) = G_{1}
      \left( x_{1}, x_{2} \right),\label{l}\tag{12} \\ x_{2} &=& G_{2} \left(
      \vec{x} \right) = G_{2} \left( x_{1}, x_{2} \right).\label{m}\tag{13}
      \end{eqnarray} $$</p>
    <p>
      In a traditional fully synchronous environment, both functions,
      <math xmlns="http://www.w3.org/1998/Math/MathML">
    <msub>
    <mi>G</mi>
    <mn>1</mn>
    </msub>
    </math>
      and
      <math xmlns="http://www.w3.org/1998/Math/MathML">
    <msub>
    <mi>G</mi>
    <mn>2</mn>
    </msub>
    <mo>,</mo>
    </math>
      would be called simultaneously and no subsequent calls would be executed
      until both functions had returned and <em>synchronized</em> all results.
      In a fully asynchronous environment, both functions would be allowed to
      execute again immediately upon their own return, leading to a case where
      one of
      <math xmlns="http://www.w3.org/1998/Math/MathML">
    <msub>
    <mi>x</mi>
    <mn>1</mn>
    </msub>
    </math>
      or
      <math xmlns="http://www.w3.org/1998/Math/MathML">
    <msub>
    <mi>x</mi>
    <mn>2</mn>
    </msub>
    </math>
      may be updated more frequently than the other. Per <a href="#Def1">Definition
        1</a>, both functions use the latest values of
      <math xmlns="http://www.w3.org/1998/Math/MathML">
    <mi>x&#8407;</mi>
    </math>
      that are available to them when the function call is initiated. For
      instance, if the processing element that was assigned to update the
      component
      <math xmlns="http://www.w3.org/1998/Math/MathML">
    <msub>
    <mi>x</mi>
    <mn>1</mn>
    </msub>
    </math>
      was ten times as fast as the processing element assigned to update
      <math xmlns="http://www.w3.org/1998/Math/MathML">
    <msub>
    <mi>x</mi>
    <mn>2</mn>
    </msub>
    <mo>,</mo>
    </math>
      then in the amount of time needed to update
      <math xmlns="http://www.w3.org/1998/Math/MathML">
    <msub>
    <mi>x</mi>
    <mn>2</mn>
    </msub>
    </math>
      once, the component
      <math xmlns="http://www.w3.org/1998/Math/MathML">
    <msub>
    <mi>x</mi>
    <mn>1</mn>
    </msub>
    </math>
      will have been updated ten times, and when
      <math xmlns="http://www.w3.org/1998/Math/MathML">
    <msub>
    <mi>G</mi>
    <mn>2</mn>
    </msub>
    </math>
      is called for the second time it will be called using the latest component
      of
      <math xmlns="http://www.w3.org/1998/Math/MathML">
    <msub>
    <mi>x</mi>
    <mn>1</mn>
    </msub>
    </math>
      (which has been updated 10 times), and the latest component of
      <math xmlns="http://www.w3.org/1998/Math/MathML">
    <msub>
    <mi>x</mi>
    <mn>2</mn>
    </msub>
    </math>
      (which has only been updated once).
    </p>
    <p>
      A block diagram showing the flow of the simulation framework is provided
      in <a href="#Fig2">Figure 2</a>. The framework models the performance of
      methods that solve the linear system
    </p>
    <p>$$ Ax = b\label{n}\tag{14} $$</p>
    <p>using relaxation methods in either a synchronous or asynchronous
      manner.</p>
    <figure id="Fig2">
      <img src="./img/image2.png" />
      <figcaption>
        <strong>Figure 2.</strong> Block diagram of the simulation framework
      </figcaption>
    </figure>
    <p>
      The simulation requires as input the matrix
      <math xmlns="http://www.w3.org/1998/Math/MathML">
      <mi>A</mi>
      <mo>,</mo>
      </math>
      the right hand side
      <math xmlns="http://www.w3.org/1998/Math/MathML">
      <mi>b</mi>
      </math>
      and an initial guess at the solution,
      <math xmlns="http://www.w3.org/1998/Math/MathML">
      <msub>
      <mi>x</mi>
      <mn>0</mn>
      </msub>
      <mo>.</mo>
      </math>
      The important pieces of the simulation are all passed as functions to the
      tool. There are three functions required:
    </p>
    <ol>
      <li>An update function that specifies how to perform the relaxation.
        A common technique for this is given by Eq. \eqref{h}. It is certainly
        possible to modify this equation to obtain different updates, as
        described, e.g., in (<a role="doc-biblioref" href="#Saa03">Saad,
          2003</a>).
      </li>
      <li>An update pattern function that determines which elements of the
        matrix <math xmlns="http://www.w3.org/1998/Math/MathML">
      <mi>A</mi>
      </math> are assigned to each simulated processor. A common technique for this
        assignment, is to evenly divide the work among all of the available
        processors, however other patterns are also possible. For example, the
        use of randomization in the solution of linear systems via relaxation
        methods has gained some popularity in the fields of optimization and
        machine learning (see, e.g., (<a role="doc-biblioref" href="#ADG15">Avron,
          Druinsky, &amp; Gupta, 2015</a>) and references therein) and update
        patterns such as this are easy to implement inside of this framework.
      </li>
      <li>An update time function that captures the empirical information
        that was captured from parallel performance runs on the HPC hardware.
        This function will typically be used to sample from the timing
        distribution that was generated beforehand. Note that, since each
        simulated processor makes calls to this function independently, the
        simulated performance will be asynchronous so long as the function
        returns different values upon different calls. Defining an update time
        function that has constant return (or constant return for every
        processor) provides a means to show synchronous performance.</li>
    </ol>
    <p>
      By varying the three functions that are passed to the framework, not only
      can the HPC performance be predicted by making changes to the update time
      function, but various modifications to the basic algorithm can be quickly
      and easily compared in a manner that reflects real world asynchronous
      performance. With the renewed research interest in asynchronous iterative
      methods that perform relaxation updates, oftentimes performance between
      new variants and existing algorithms is <em>only</em> compared in simple
      synchronous experiments; the simulation framework proposed here allows for
      a more meaningful comparison between methods that does not require
      development of parallel implementations of all the methods or algorithm
      variations that are involved.
    </p>
    <p>
      The simulation framework requires some data that specifies parameters
      concerning the particular run of the simulation such as the desired
      tolerance, the number of processors to simulate, and a computational scale
      factor. The framework itself is developed in MATLAB<sup>&reg;</sup> and
      the three required functions are passed as function handles.
    </p>
    <p>
      The simulation itself (see <em>Simulation</em> block in <a href="#Fig2">Figure
        2</a>) progresses by reading in the user provided input data, assigning an
      initial update pattern and time to each processor, and then beginning the
      main loop. Inside of the main loop, the time increments and a check is
      performed to see if the current time matches with the scheduled update
      time for any of the processors, if so, the update function is called and
      then a time for the next update is assigned to the processor that just
      updated and (if desired) the update pattern for the current processor is
      changed. After this, a check is performed on the size of the residual to
      determine if the exit criteria is met before the time is incremented again
      and the loop starts over. A pseudocode representation of the simulation
      framework for simulated asynchronous Jacobi is given in <a href="#Alg2">Algorithm
        2</a>.
    </p>
    <div id="Alg2">
      <pre id="Alg2t" style="display: none">
        \begin{algorithm}
        \caption{Asynchronous Jacobi simulation}
        \begin{algorithmic}
          \STATE \textbf{Input:} $a_{ij} \in A$, initial guess for $x_{0}$, a number of processing elements $p$, an input random number distribution
          \STATE \textbf{Output:} Solution vector $x$
          
          \STATE Assign processor update times. $\tau_{1}, \tau_{2}, \ldots, \tau_{p}$  by sampling from an appropriate random number distribution
          
          \STATE Assign elements $x_{i} \in x$ to each simulated processing element
          
          \FOR{$t = 1, 2, \ldots$ until convergence}
            \FOR{each processing element $P_{l}$}
              \IF{$\tau_{l} = t$}
                \FOR{each element $x_{i} \in x$ assigned to $P_{l}$}
                  \STATE $x_{i} = \frac{-1}{a_{ii}} \left[ \sum_{j \neq i} a_{ij} x_{j} - b_{i} \right]$
                \ENDFOR
                \STATE Retrieve a new update time $\tau_{l}$ by sampling from the input distribution
              \ENDIF
            \ENDFOR
            \STATE Calculate the residual as in Eq. (18) and check termination conditions
          \ENDFOR
        \end{algorithmic}
        \end{algorithm}
      </pre>
    </div>
    <script>
          var alg1 = document.getElementById( "Alg2t" ).textContent;
          pseudocode.render( alg1, document.getElementById( "Alg2" ), {
            lineNumber: true
          } );
        </script>
    <p>
      In <a href="#Alg2">Algorithm 2</a>, a given update time
      <math xmlns="http://www.w3.org/1998/Math/MathML">
      <msub>
      <mi>&tau;</mi>
      <mi>l</mi>
      </msub>
      </math>
      will often not be sampled as an integer. The simulation adjusts for this
      by scaling the number that is sampled by the appropriate order of
      magnitude, adjusting the maximum value allowed for
      <math xmlns="http://www.w3.org/1998/Math/MathML">
      <mi>t</mi>
      </math>
      accordingly, and then scaling back the final time calculated by the
      simulation. For example, if the desired time precision is hundredths of a
      second, and the time resulting for the first sampling of
      <math xmlns="http://www.w3.org/1998/Math/MathML">
      <msub>
      <mi>&tau;</mi>
      <mi>l</mi>
      </msub>
      </math>
      was 1.234<em>s</em>, then the simulation would perform the following
      steps:
    </p>
    <ol>
      <li><math xmlns="http://www.w3.org/1998/Math/MathML">
      <msubsup>
      <mi>&tau;</mi>
      <mi>l</mi>
      <mtext>new</mtext>
      </msubsup>
      <mo>=</mo>
      <mi>s</mi>
      <mo>&times;</mo>
      <msubsup>
      <mi>&tau;</mi>
      <mi>l</mi>
      <mtext>old</mtext>
      </msubsup>
      </math></li>
      <li><math xmlns="http://www.w3.org/1998/Math/MathML">
      <msubsup>
      <mi>&tau;</mi>
      <mtext>max</mtext>
      <mtext>new</mtext>
      </msubsup>
      <mo>=</mo>
      <mi>s</mi>
      <mo>&times;</mo>
      <msubsup>
      <mi>&tau;</mi>
      <mtext>max</mtext>
      <mtext>old</mtext>
      </msubsup>
      </math></li>
      <li><math xmlns="http://www.w3.org/1998/Math/MathML">
      <msubsup>
      <mi>&tau;</mi>
      <mtext>final</mtext>
      <mtext>new</mtext>
      </msubsup>
      <mo>=</mo>
      <mfenced open="(" close=")">
      <mfrac>
      <mn>1</mn>
      <mi>s</mi>
      </mfrac>
      </mfenced>
      <mo>&times;</mo>
      <msubsup>
      <mi>&tau;</mi>
      <mtext>final</mtext>
      <mtext>old</mtext>
      </msubsup>
      <mo>,</mo>
      </math></li>
    </ol>
    <p>
      where
      <math xmlns="http://www.w3.org/1998/Math/MathML">
      <mi>s</mi>
      </math>
      referenced is the "scale_factor" defined in the block diagram given by <a
        href="#Fig2">Figure 2</a>. For example, if the desired precision is
      hundredths of a second,
      <math xmlns="http://www.w3.org/1998/Math/MathML">
      <mi>s</mi>
      <mo>=</mo>
      <msup>
      <mn>10</mn>
      <mn>2</mn>
      </msup>
      <mo>,</mo>
      </math>
      and the sampled value
      <math xmlns="http://www.w3.org/1998/Math/MathML">
      <msub>
      <mi>&tau;</mi>
      <mi>l</mi>
      </msub>
      </math>
      becomes
    </p>
    <p>$$ \begin{eqnarray} \tau_{l} &=& 1.234 - \text{initial sample} \\
      \tau_{l} &=& 123.4 - \text{apply scale factor} \\ \tau_{l} &=& 123 -
      \text{round to nearest integer}. \end{eqnarray} $$</p>
    <p>
    <p>Inside of the simulation framework, time is abstracted away to "units
      of time", and then the final time is scaled back into the appropriate
      units. This allows the framework to be adapted to future HPC environments,
      as well as examining the impact of the standard variance of single core
      performance on multi-core hardware elements if the method that is used is
      tuned to be completely asynchronous. It should be possible - by adding or
      removing appropriate communication penalties - to simulate the performance
      of different memory architectures (e.g., distributed or cloud computing
      environments). This is left as future work although the method for doing
      this inside of the proposed framework is straightforward.</p>

    <section id="Sect4.1">
      <h1>Sample use-cases for the framework</h1>
      <p>
        Let the matrix
        <math xmlns="http://www.w3.org/1998/Math/MathML">
      <mi>A</mi>
      </math>
        result from a simple two dimensional finite-difference discretization of
        the Laplacian over a
        <math xmlns="http://www.w3.org/1998/Math/MathML">
        <mn>10</mn>
        <mo>&times;</mo>
        <mn>10</mn>
        </math>
        grid, resulting in a
        <math xmlns="http://www.w3.org/1998/Math/MathML">
        <mn>100</mn>
        <mo>&times;</mo>
        <mn>100</mn>
        </math>
        matrix with an average of 4.6 non-zero entries per row. The Laplacian
      </p>
      <p>$$ \begin{eqnarray} \Delta u &=& g,\label{o}\tag{15} \\ u_{xx} +
        u_{yy} &=& g\label{p}\tag{16} \end{eqnarray} $$</p>
      <p>is a partial differential equation (PDE) commonly found in both
        science and engineering. The example problems taken in this study can be
        thought of as simulating the diffusion of heat across a two dimensional
        surface given some heat source along the boundary of the problem.</p>
      <p>Once the PDE is discretized over the desired grid using finite
        differences, typically central finite differences, the linear system</p>
      <p>$$ Ax = b\label{q}\tag{17} $$</p>
      <p>is set up to be solved for a random right-hand side b that
        represents the desired boundary conditions. All problems considered in
        this study use Dirichlet boundary conditions. For the examples in this
        particular subsection, the righthand side is generated by taking each
        component sampled as a uniform random number between −0:5 and 0:5, and
        then normalizing the resultant vector. The iterative Jacobi method
        proceeds until the residual</p>
      <p>$$ r = b - Ax\label{r}\tag{18} $$</p>
      <p>is reduced past some desired threshold.</p>
      <p>
        To begin with, an example of nominal performance of the solution of the
        two dimensional Laplacian in a synchronous environment is provided by <a
          href="#Fig3">Figure 3</a>.
      </p>
      <figure id="Fig3">
        <img src="./img/image3.png" />
        <figcaption>
          <strong>Figure 3.</strong> Example of nominal performance of the
          synchronous Jacobi iteration
        </figcaption>
      </figure>
      <p>
        Next, consider the same problem from above, but in two slightly more
        complicated scenarios. In <a href="#Fig4">Figure 4</a> one of the ten
        processors involved in updating blocks of components of
        <math xmlns="http://www.w3.org/1998/Math/MathML">
        <mi>x</mi>
        </math>
        is provided updates more slowly than the other processors. This could
        reflect the scenario where updates are either performed synchronously or
        asynchronously where the effect of variance in performance is
        negligible, and a single processor has degraded performance. This can
        also be viewed as a look at the impact of asynchronous behavior on the
        Jacobi algorithm. Each curve shows the progression of the (global)
        residual subject to having a single slower processor with different
        degrees of slowdown (from zero to 11x).
      </p>
      <figure id="Fig4">
        <img src="./img/image4.png" />
        <figcaption>
          <strong>Figure 4.</strong> Example of experiments within the
          simulation framework; single processor slowdown
        </figcaption>
      </figure>
      <p>
        In <a href="#Fig5">Figure 5</a> the processor updates are not restricted
        to occur synchronously. Instead, the processors are assumed to have
        similar performance and perform their updates in time
        <math xmlns="http://www.w3.org/1998/Math/MathML">
        <msub>
        <mi>t</mi>
        <mi>i</mi>
        </msub>
        <mo>&sim;</mo>
        <mi>N</mi>
        <mfenced open="(" close=")">
        <mo>&mu;</mo>
        <msup>
        <mo>&sigma;</mo>
        <mn>2</mn>
        </msup>
        </mfenced>
        </math>
        where the mean is set to 10 units of time and the variance is different
        for each curve depicted in the plot. An increase in the variance of
        processor performance, regardless of the timing distribution, could come
        about for a variety of reasons; an example of a scenario in the future
        could be having chips with more cores and lower voltage that are
        designed to address the challenges in creating very large scale HPC
        environments.
      </p>
      <figure id="Fig5">
        <img src="./img/image5.png" />
        <figcaption>
          <strong>Figure 5.</strong> Example of experiments within the
          simulation framework; effect of variance
        </figcaption>
      </figure>
    </section>
  </section>

  <section id="Sect5">
    <h1>Asynchronous Jacobi implementations for the framework</h1>
    <p>
      <a href="#Fig4">Figure 4</a> and <a href="#Fig5">Figure 5</a> show
      relative differences in compute times among sharedmemory computing
      elements for a specific problem and a specific asynchronous iterative
      method. A more general simulation framework, which can be used for
      modeling and testing any synchronous or asynchronous iterative relaxation
      method, is presented here. Baseline, non-resilient method behavior may be
      reproduced in the framework; further, the user may also investigate fault
      injection and checkpointing.
    </p>
    <p>
      The user decomposes the method according to the input parameters required
      by the simulation framework. The update function that performs the
      relaxation has an associated operational time, both of which are defined
      by the user. Functionality within the relaxation may be isolated into
      discrete operations with corresponding time information; the level of
      granularity is decided by the user. For example, time to complete an
      operation in the simulation framework may be modeled with a probability
      density function derived from empirical data. To model time to perform
      specific operations or calculations during method execution, data is
      collected from the application during execution. In the implementation
      code, operations are enclosed within calls to time functions, which
      measure time to perform the operations. In this work the OpenMP<sup>&reg;</sup>
      library function
      <code>omp_get_wtime()</code>
      is used to measure wall time. For HPC implementations that use MPI,
      <code>MPI_Wtime()</code>
      may be used to measure wall time. Fine-grained operations in the code
      should not overlap such that measurements overlap, i.e., for one
      operation, do not measure time function calls of another operation. After
      taking sufficient measurements, an operation is modeled by fitting a
      probability density function to a normalized histogram of the time data.
      This function may be included as part of the input to the framework. Note
      that when comparing simulated run times with HPC run times, it may be
      preferable to use an unmodified version of the HPC implementation code
      that does not have time function calls and mechanisms for storing or
      printing times. These functions and activities may increase run time and
      provide an inaccurate metric for comparison.
    </p>
    <p>This section describes two asynchronous relaxation method
      implementations and two corresponding use cases of the simulation
      framework. For both implementations, the test problem is a two dimensional
      discretization of the Laplacian</p>
    <p>$$ \Delta u = b,\label{s}\tag{19} $$</p>
    <p>
      where the right-hand side is initialized with Dirichlet boundary
      conditions. Both implementations use OpenMP<sup>&reg;</sup> for
      shared-memory parallelism and are executed on the shared-memory computing
      platform nicknamed Rulfo, which is an Intel<sup>&reg;</sup> Xeon Phi<sup>&trade;</sup>
      Knight's Landing<span class="fn"><sup><a href="#ftn2"
          id="fn_pointer_ftn2" role="doc-noteref"
          title="Footnote 2: Rulfo is a part of computing resources of the Department of Modeling, Simulation and Visualization Engineering at Old Dominion University.">2</a></sup></span>
      having 7210 model processor with 64 cores. Each core may optimally execute
      4 threads for 256 threads total, and runs at 1.30 GHz. The simulation
      framework and experiments were implemented in MATLAB<sup>&reg;</sup>
      R2018a, while the Jacobi implementations were written in C/C++ using the
      Intel C compiler version 17.04 and OpenMP<sup>&reg;</sup> version 4.5.
    </p>

    <section id="Sect5.1">
      <h1>Implementation 1: General Jacobi solver</h1>
      <p>
        In this case, the heat problem is represented mathematically by a sparse
        matrix, which is solved by an asynchronous general Jacobi method. The
        Laplacian is generated over a
        <math xmlns="http://www.w3.org/1998/Math/MathML">
        <mn>100</mn>
        <mo>&times;</mo>
        <mn>100</mn>
        </math>
        grid resulting in a matrix of size
        <math xmlns="http://www.w3.org/1998/Math/MathML">
        <mn>10,000</mn>
        <mo>&times;</mo>
        <mn>10,000</mn>
        </math>
        with 49,600 non-zeros with an average of 4.96 non-zeros per row. The
        vector b from the resulting linear system,
      </p>
      <p>$$ Ax = b,\label{t}\tag{20} $$</p>
      <p>
        is initialized such that the final solution vector has
        <math xmlns="http://www.w3.org/1998/Math/MathML">
      <msub>
      <mi>x</mi>
      <mi>i</mi>
      </msub>
      <mo>=</mo>
      <mn>1</mn>
      </math>
        for all
        <math xmlns="http://www.w3.org/1998/Math/MathML">
      <mi>i</mi>
      <mo>.</mo>
      </math>
        The initial guess
        <math xmlns="http://www.w3.org/1998/Math/MathML">
      <msub>
      <mi>x</mi>
      <mn>0</mn>
      </msub>
      </math>
        is all zeros.
      </p>
      <p>
        In this implementation, all threads but one perform relaxations on
        assigned components, and a dedicated thread computes the global residual
        norm value
        <math xmlns="http://www.w3.org/1998/Math/MathML">
        <mi>b</mi>
        <mo>-</mo>
        <mi>A</mi>
        <mi>x</mi>
        <msup>
        <mfenced open="(" close=")">
        <mi>t</mi>
        </mfenced>
        </msup>
        </math>
        that determines satisfactory convergence. Each thread retrieves the data
        it needs from shared memory, performs the necessary computations, and,
        in the case of the relaxation threads, writes the result back to shared
        memory. Synchronous shared-memory implementations of all classes of
        algorithms commonly use mutex locks to avoid race conditions with read
        and write operations. However, this type of asynchronous relaxation
        method may be less dependent on these safeguards for two reasons: (1)
        iterative methods can correct some errors with more iterations, if
        necessary, and (2) threads executing operations in asynchronous
        iterative methods are more likely to be at different stages of the
        iterative cycle, meaning fewer threads may be writing to and reading
        from the same memory location concurrently. This general Jacobi solver
        has two varieties: (a) <span class="small-caps">Safe</span> which uses
        mutex locks to avoid race conditions, and (b) <span class="small-caps">Race</span>
        which permits race conditions. <span class="small-caps">Safe</span> uses
        OpenMP<sup>&reg;</sup> locks to copy
        <math xmlns="http://www.w3.org/1998/Math/MathML">
        <msup>
        <mi>x</mi>
        <mfenced open="(" close=")">
        <mi>t</mi>
        </mfenced>
        </msup>
        </math>
        safely from shared memory and to update
        <math xmlns="http://www.w3.org/1998/Math/MathML">
        <msup>
        <mi>x</mi>
        <mfenced open="(" close=")">
        <mrow>
        <mi>t</mi>
        <mo>+</mo>
        <mn>1</mn>
        </mrow>
        </mfenced>
        </msup>
        <mo>.</mo>
        </math>
        Pseudocode for this process is given in <a href="#Alg3">Algorithm 3</a>,
        where bold upper-case text indicates that OpenMP<sup>&reg;</sup> locks
        are employed. The algorithm for <span class="small-caps">Race</span> is
        identical to <a href="#Alg3">Algorithm 3</a>, with the exception that
        locks are omitted.
      </p>
      <div id="Alg3">
        <pre id="Alg3t" style="display: none">
        \begin{algorithm}
        \caption{OpenMP Implementation 1 (a) \textsc{Safe}}
        \begin{algorithmic}
          \STATE \textbf{Input:} $a_{ij} \in A$, $b$, initial guess for $X_{0}$, $n$ processing elements $p$
          \STATE \textbf{Output:} Solution vector $X$
          
          \STATE Assign elements $X_i \in X$ to $n-1$ processing elements, i = $[\alpha, \omega]$
          
          \FOR{\textbf{parallel} each processing element in $p_{1} \ldots p_{n}$}
            \WHILE{residual norm $>$ tolerance}
              \STATE \textbf{COPY} global $X^{\left(t\right)}$ from shared memory to local $x^{\left(t\right)}$
              \IF{$p_{1}$}
                \STATE Compute residual norm $||b - Ax||_2$
              \ELSIF{$p_{2} \ldots p_{n}$}
                \FOR{$x$ index $i = \alpha \ldots \omega$}
                  \STATE Compute $x^{\left(t + 1\right)}_{i} = \frac{-1}{a_{ii}}\left[\sum_{j \neq i} a_{ij}x^{\left(t\right)}_{j} - b_{i} \right]$
                \ENDFOR
                \STATE \textbf{UPDATE} $X^{\left(t+1\right)}_{i}$ in shared memory with $x_{i}^{\left(t + 1\right)}$ for all $i$ belonging to processing element
              \ENDIF
            \ENDWHILE
          \ENDFOR
        \end{algorithmic}
        \end{algorithm}
      </pre>
      </div>
      <script>
              var alg1 = document.getElementById( "Alg3t" ).textContent;
              pseudocode.render( alg1, document.getElementById( "Alg3" ), {
                lineNumber: true
              } );
            </script>
      <p>
        <a href="#Fig6">Figure 6</a> compares <span class="small-caps">Safe</span>
        and <span class="small-caps">Race</span> calculation times and number of
        iterations. Calculation times and average iteration counts are similar
        for thread counts up to 81, but behavior diverges beyond that. For
        thread counts 101 through 501, <span class="small-caps">Race</span>
        requires more iterations, perhaps to compensate for threads reading and
        computing with inaccurate
        <math xmlns="http://www.w3.org/1998/Math/MathML">
        <mi>x</mi>
        </math>
        vectors. Despite this, <a href="#Fig6a">Figure 6a</a> shows that <span
          class="small-caps">Race</span> is still quicker for the largest thread
        counts, perhaps because threads do not use locks to access data and
        eliminate that overhead cost. <a href="#Fig6a">Figure 6a</a> also shows
        that perhaps locks are not too costly for intermediate thread counts
        101, 201, and 251, where <span class="small-caps">Safe</span>
        outperforms <span class="small-caps">Race</span> in terms of calculation
        time.
      </p>
      <table id="Fig6">
	   <tr>
        <td><figure id="Fig6a">
          <img src="./img/image6a.png" />
          <figcaption>
            <strong>(a)</strong> Calculation time
          </figcaption>
        </figure></td>
        <td><figure id="Fig6b">
          <img src="./img/image6b.png" />
          <figcaption>
            <strong>(b)</strong> Number of iterations, average per thread
          </figcaption>
        </figure></td>
	   </tr>
        <caption>
          <strong>Figure 6.</strong> Performance variations between <span
            class="small-caps">Safe</span> and <span class="small-caps">Race</span>
          as a function of thread count
        </caption>
      </table>
      <p>
        Both <span class="small-caps">Safe</span> and <span class="small-caps">Race</span>
        were executed over several trials and varying thread counts on the
        experimental HPC platform. For each trial, the times for a thread to
        access the solution in shared memory (Line 6 of <a href="#Alg3">Algorithm
          3</a>), compute the relaxation for the rows assigned to it (Line 11), and
        to update the solution in shared memory (Line 13) were captured. This
        data was used to generate MATLAB<sup>&reg;</sup> kernel probability
        density functions for modeling the amount of time a thread takes to
        complete a copy, compute, or update operation. These distributions may
        be used in the simulation framework as an input parameter, for the
        generation of random variables corresponding to key operational times in
        the HPC architecture. <a href="#Alg2">Algorithm 2</a> demonstrates the
        use of a time distribution in the framework. Thread counts of 11, 21,
        41, 81, 101, 201, 251, and 401 were used to collect data for the
        generation of distributions, some of which are in <a href="#Fig7">Figure
          7</a> and <a href="#Fig8">Figure 8</a>. For 201 threads, <span
          class="small-caps">Safe</span> in <a href="#Fig7d">Figure 7d</a> and <a
          href="#Fig7f">Figure 7f</a> shows the tendency of locks to stratify
        copy and update times, compared with <span class="small-caps">Race</span>
        in <a href="#Fig8d">Figure 8d</a> and <a href="#Fig8f">Figure 8f</a>,
        which are less uniform. These findings are mirrored in <a href="#Tab1">Table
          1</a>, which provides mean times for each of the three operations that
        were benchmarked in this implementation, for <span class="small-caps">Safe</span>
        and <span class="small-caps">Race</span>. <span class="small-caps">Race</span>
        copy and update times are slightly or significantly quicker than
        comparable <span class="small-caps">Safe</span> times. Compute times
        typically dominate total iteration time, except for <span
          class="small-caps">Safe</span> copy and update times for threads 201,
        251, and 401. <a href="#Tab1">Table 1</a> shows that increasing the
        number of threads decreases <span class="small-caps">Race</span> copy,
        compute, and update times until cores are sufficiently over-subscribed:
        at 201 threads, these operations have become significantly more costly,
        as compared with 101 threads. This cost may be attributed to thread
        context switching. Compute times for <span class="small-caps">Safe</span>
        do not increase with higher thread counts because thread behavior is
        controlled explicitly using locks. These statistics can be used to
        validate the performance of the time distributions, so that the
        framework provides results comparable to the HPC hardware.
      </p>
      <table id="Tab1">
        <thead>
          <tr>
            <th rowspan="2">Threads</th>
            <th colspan="3"><span class="small-caps">Safe</span></th>
            <th colspan="3"><span class="small-caps">Race</span></th>
          </tr>
          <tr>
            <th>Copy<br /> <math
                xmlns="http://www.w3.org/1998/Math/MathML">
          <mfenced open="(" close=")">
          <mrow>
          <msup>
          <mn>10</mn>
          <mn>-5</mn>
          </msup>
          <mi>s</mi>
          </mrow>
          </mfenced>
          </math>
            </th>
            <th>Compute<br /> <math
                xmlns="http://www.w3.org/1998/Math/MathML">
          <mfenced open="(" close=")">
          </mrow>
          <msup>
          <mn>10</mn>
          <mn>-4</mn>
          </msup>
          <mi>s</mi>
          </mrow>
          </mfenced>
          </math>
            </th>
            <th>Update<br /> <math
                xmlns="http://www.w3.org/1998/Math/MathML">
          <mfenced open="(" close=")">
          </mrow>
          <msup>
          <mn>10</mn>
          <mn>-6</mn>
          </msup>
          <mi>s</mi>
          </mrow>
          </mfenced>
          </math>
            </th>
            <th>Copy<br /> <math
                xmlns="http://www.w3.org/1998/Math/MathML">
          <mfenced open="(" close=")">
          </mrow>
          <msup>
          <mn>10</mn>
          <mn>-5</mn>
          </msup>
          <mi>s</mi>
          </mrow>
          </mfenced>
          </math>
            </th>
            <th>Compute<br /> <math
                xmlns="http://www.w3.org/1998/Math/MathML">
          <mfenced open="(" close=")">
          </mrow>
          <msup>
          <mn>10</mn>
          <mn>-4</mn>
          </msup>
          <mi>s</mi>
          </mrow>
          </mfenced>
          </math>
            </th>
            <th>Update<br /> <math
                xmlns="http://www.w3.org/1998/Math/MathML">
          <mfenced open="(" close=")">
          </mrow>
          <msup>
          <mn>10</mn>
          <mn>-6</mn>
          </msup>
          <mi>s</mi>
          </mrow>
          </mfenced>
          </math>
            </th>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td>11</td>
            <td>1.28</td>
            <td>167.00</td>
            <td>7.76</td>
            <td>1.15</td>
            <td>167.00</td>
            <td>2.79</td>
          </tr>
          <tr>
            <td>21</td>
            <td>1.31</td>
            <td>84.30</td>
            <td>6.98</td>
            <td>1.17</td>
            <td>83.60</td>
            <td>1.96</td>
          </tr>
          <tr>
            <td>41</td>
            <td>1.38</td>
            <td>43.00</td>
            <td>7.09</td>
            <td>1.23</td>
            <td>43.10</td>
            <td>1.63</td>
          </tr>
          <tr>
            <td>81</td>
            <td>2.98</td>
            <td>27.30</td>
            <td>20.60</td>
            <td>1.43</td>
            <td>27.20</td>
            <td>1.79</td>
          </tr>
          <tr>
            <td>101</td>
            <td>36.70</td>
            <td>23.40</td>
            <td>357.00</td>
            <td>1.64</td>
            <td>25.20</td>
            <td>1.79</td>
          </tr>
          <tr>
            <td>201</td>
            <td>251.00</td>
            <td>15.30</td>
            <td>2500.00</td>
            <td>11.80</td>
            <td>74.30</td>
            <td>4.33</td>
          </tr>
          <tr>
            <td>251</td>
            <td>345.00</td>
            <td>13.30</td>
            <td>3440.00</td>
            <td>16.60</td>
            <td>90.90</td>
            <td>4.55</td>
          </tr>
          <tr>
            <td>401</td>
            <td>1880.00</td>
            <td>8.23</td>
            <td>18,700.00</td>
            <td>20.20</td>
            <td>91.60</td>
            <td>4.52</td>
          </tr>
        </tbody>
        <caption>
          <strong>Table 1.</strong> Mean times for copy, compute, and update
          operations
        </caption>
      </table>
      <table id="Fig7">
	   <tr>
        <td><figure id="Fig7a">
          <img src="./img/image7a.png" />
          <figcaption>
            <strong>(a)</strong> 𝓍 copy, 11 threads
          </figcaption>
        </figure></td>
        <td><figure id="Fig7b">
          <img src="./img/image7b.png" />
          <figcaption>
            <strong>(b)</strong> 𝓍 compute, 11 threads
          </figcaption>
        </figure></td>
        <td><figure id="Fig7c">
          <img src="./img/image7c.png" />
          <figcaption>
            <strong>(c)</strong> 𝓍 update, 11 threads
          </figcaption>
        </figure></td>
	   </tr>
	   <tr>
        <td><figure id="Fig7d">
          <img src="./img/image7d.png" />
          <figcaption>
            <strong>(d)</strong> 𝓍 copy, 81 threads
          </figcaption>
        </figure></td>
        <td><figure id="Fig7e">
          <img src="./img/image7e.png" />
          <figcaption>
            <strong>(e)</strong> 𝓍 compute, 81 threads
          </figcaption>
        </figure></td>
        <td><figure id="Fig7f">
          <img src="./img/image7f.png" />
          <figcaption>
            <strong>(f)</strong> 𝓍 update, 81 threads
          </figcaption>
        </figure></td>
	   </tr>
	   <tr>
        <td><figure id="Fig7g">
          <img src="./img/image7g.png" />
          <figcaption>
            <strong>(g)</strong> 𝓍 copy, 201 threads
          </figcaption>
        </figure></td>
        <td><figure id="Fig7h">
          <img src="./img/image7h.png" />
          <figcaption>
            <strong>(h)</strong> 𝓍 compute, 201 threads
          </figcaption>
        </figure></td>
        <td><figure id="Fig7i">
          <img src="./img/image7i.png" />
          <figcaption>
            <strong>(i)</strong> 𝓍 update, 201 threads
          </figcaption>
        </figure></td>
	   </tr>
       <caption>
          <strong>Figure 7.</strong> <span class="small-caps">Safe</span> copy,
          compute, and update histograms with kernel fits
       </caption>
      </table>
	  
	  <table id="Fig8">
	   <tr>
	    <td><figure id="Fig8a">
          <img src="./img/image8a.png" width="250"/>
          <figcaption>
            <strong>(a)</strong> 𝓍 copy, 11 threads
          </figcaption>
        </figure></td>
        <td><figure id="Fig8b">
          <img src="./img/image8b.png" width="250"//>
          <figcaption>
            <strong>(b)</strong> 𝓍 compute, 11 threads
          </figcaption>
        </figure></td>
        <td><figure id="Fig8c">
          <img src="./img/image8c.png" width="250"//>
          <figcaption>
            <strong>(c)</strong> 𝓍 update, 11 threads
          </figcaption>
        </figure></td>
	   </tr>
	   <tr>
        <td><figure id="Fig8d">
          <img src="./img/image8d.png" width="250"//>
          <figcaption>
            <strong>(d)</strong> 𝓍 copy, 81 threads
          </figcaption>
        </figure></td>
        <td><figure id="Fig8e">
          <img src="./img/image8e.png" width="250"//>
          <figcaption>
            <strong>(e)</strong> 𝓍 compute, 81 threads
          </figcaption>
        </figure></td>
        <td><figure id="Fig8f">
          <img src="./img/image8f.png" width="250"//>
          <figcaption>
            <strong>(f)</strong> 𝓍 update, 81 threads
          </figcaption>
        </figure></td>
	   </tr>
	   <tr>
        <td><figure id="Fig8g">
          <img src="./img/image8g.png" width="250"//>
          <figcaption>
            <strong>(g)</strong> 𝓍 copy, 201 threads
          </figcaption>
        </figure></td>
        <td><figure id="Fig8h">
          <img src="./img/image8h.png" width="250"//>
          <figcaption>
            <strong>(h)</strong> 𝓍 compute, 201 threads
          </figcaption>
        </figure></td>
        <td><figure id="Fig8i">
          <img src="./img/image8i.png" width="250"//>
          <figcaption>
            <strong>(i)</strong> 𝓍 update, 201 threads
          </figcaption>
        </figure></td>
	   </tr>
        <caption><strong>Figure 8.</strong> <span class="small-caps">Race</span> copy,
          compute, and update histograms with kernel fits</caption>
      </table>
    </section>

    <section id="Sect5.2">
      <h1>Implementation 2: Finite difference Jacobi solver</h1>
      <p>
        This second implementation performs the Jacobi relaxation on the grid
        directly using the neighboring points required by the 5-point stencil as
        opposed to explicitly forming the matrix
        <math xmlns="http://www.w3.org/1998/Math/MathML">
        <mi>A</mi>
        <mo>,</mo>
        </math>
        and in a sense implements a <em>matrix-free</em> solution. For this
        implementation, the Laplacian was discretized over a
        <math xmlns="http://www.w3.org/1998/Math/MathML">
        <mn>600</mn>
        <mo>&times;</mo>
        <mn>600</mn>
        </math>
        grid with boundary conditions set according to <a href="#Tab2">Table
          2</a>.
      </p>
      <table id="Tab2">
        <tbody>
          <tr>
            <td>0</td>
            <td>100</td>
            <td>&hellip;</td>
            <td>&hellip;</td>
            <td>100</td>
            <td>0</td>
          </tr>
          <tr>
            <td>75</td>
            <td>XXX</td>
            <td>&hellip;</td>
            <td>&hellip;</td>
            <td>XXX</td>
            <td>50</td>
          </tr>
          <tr>
            <td>&vellip;</td>
            <td>XXX</td>
            <td>&hellip;</td>
            <td>&hellip;</td>
            <td>XXX</td>
            <td>&vellip;</td>
          </tr>
          <tr>
            <td>&vellip;</td>
            <td>XXX</td>
            <td>&hellip;</td>
            <td>&hellip;</td>
            <td>XXX</td>
            <td>&vellip;</td>
          </tr>
          <tr>
            <td>75</td>
            <td>XXX</td>
            <td>&hellip;</td>
            <td>&hellip;</td>
            <td>XXX</td>
            <td>50</td>
          </tr>
          <tr>
            <td>0</td>
            <td>0</td>
            <td>&hellip;</td>
            <td>&hellip;</td>
            <td>0</td>
            <td>0</td>
          </tr>
        </tbody>
        <caption>
          <strong>Table 2.</strong> Boundary conditions for the second
          implementation of the Laplacian
        </caption>
      </table>
      <p>
        The implementation used here stems from code provided by (<a
          role="doc-biblioref" href="#HW10">Hager, &amp; Wellein, 2010</a>);
        similar code solves a three dimensional discretization of the Laplacian
        in the study featured in (<a role="doc-biblioref" href="#BBDH11">Bethune,
          et al., 2011</a>) and (<a role="doc-biblioref" href="#BBDH14">Bethune,
          et al., 2014</a>). The routine solves a heat diffusion problem, in which a
        two-dimensional heated plate has Dirichlet boundary-condition
        temperatures. Two matrices,
        <math xmlns="http://www.w3.org/1998/Math/MathML">
        <msub>
        <mi>u</mi>
        <mn>0</mn>
        </msub>
        </math>
        and
        <math xmlns="http://www.w3.org/1998/Math/MathML">
        <msub>
        <mi>u</mi>
        <mn>1</mn>
        </msub>
        <mo>,</mo>
        </math>
        store grid point values that each thread reads, e.g., from
        <math xmlns="http://www.w3.org/1998/Math/MathML">
        <msub>
        <mi>u</mi>
        <mn>1</mn>
        </msub>
        <mo>,</mo>
        </math>
        to compute newer values to write, e.g., to
        <math xmlns="http://www.w3.org/1998/Math/MathML">
        <msub>
        <mi>u</mi>
        <mn>0</mn>
        </msub>
        <mo>.</mo>
        </math>
        As the method is asynchronous, each thread independently determines
        which matrix stores its newer
        <math xmlns="http://www.w3.org/1998/Math/MathML">
        <msup>
        <mi>u</mi>
        <mrow>
        <mfenced open="(" close=")">
        <mrow>
        <mi>t</mi>
        <mo>+</mo>
        <mn>1</mn>
        </mrow>
        </mfenced>
        </mrow>
        </msup>
        <mfenced open="(" close=")">
        <mi>i</mi>
        <mi>j</mi>
        </mfenced>
        </math>
        values and older
        <math xmlns="http://www.w3.org/1998/Math/MathML">
        <msup>
        <mi>u</mi>
        <mrow>
        <mfenced open="(" close=")">
        <mi>t</mi>
        </mfenced>
        </mrow>
        </msup>
        <mfenced open="(" close=")">
        <mi>i</mi>
        <mi>j</mi>
        </mfenced>
        </math>
        values. For an
        <math xmlns="http://www.w3.org/1998/Math/MathML">
        <mi>N</mi>
        <mo>+</mo>
        <mn>2</mn>
        </math>
        by
        <math xmlns="http://www.w3.org/1998/Math/MathML">
        <mi>N</mi>
        <mo>+</mo>
        <mn>2</mn>
        </math>
        grid, each thread solves for
        <math xmlns="http://www.w3.org/1998/Math/MathML">
        <msup>
        <mi>N</mi>
        <mn>2</mn>
        </msup>
        </math>
        grid points divided by
        <math xmlns="http://www.w3.org/1998/Math/MathML">
        <mi>n</mi>
        </math>
        processing elements, such that the grid is evenly divided along the
        y-axis. When a thread copies grid point values above or below its domain
        for the computation, OpenMP<sup>&reg;</sup> locks are employed to ensure
        that data is safely captured from a single iteration. Further, locks are
        used when updating values on domain boundaries. Each thread
        <math xmlns="http://www.w3.org/1998/Math/MathML">
        <msub>
        <mi>p</mi>
        <mi>n</mi>
        </msub>
        </math>
        computes its local residual value every
        <math xmlns="http://www.w3.org/1998/Math/MathML">
        <msup>
        <mi>k</mi>
        <mi>th</mi>
        </msup>
        </math>
        iteration, which it contributes to the global residual value using an
        OpenMP<sup>&reg;</sup> atomic operation, such that it adds the local
        residual from the current iteration and subtracts the local residual
        from the previous iteration. A single thread checks for convergence with
        an atomic capture operation, and updates a shared flag variable if the
        criterion is satisfied. Pseudocode for this implementation is provided
        in <a href="#Alg4">Algorithm 4</a>, where bold upper-case text indicates
        that OpenMP<sup>&reg;</sup> locks are employed. Locks are used only with
        interior boundary rows, meaning they are unnecessary for the first and
        last rows in the domain.
      </p>
      <p>In this implementation, data was collected only for the time to
        complete an iteration. Thread counts of 10, 25, 50, 75, 100, and 150
        were used in these series of experiments. The average total iteration
        time for the varying.</p>
      <div id="Alg4">
        <pre id="Alg4t" style="display: none">
        \begin{algorithm}
        \caption{OpenMP Implementation 2}
        \begin{algorithmic}
          \STATE \textbf{Input:} Initial guess for $u^{\left(0\right)}(i,j)$, $n$ processing elements $p$
          \STATE \textbf{Output:} Solution vector $u(i,j)$
          
          \STATE Assign rows $u(i) \in u$ to each processing element, i = $[\alpha, \omega]$
          
          \FOR{\textbf{parallel} each processing element in $p_{1} \ldots p_{n}$}
            \WHILE{residual norm $>$ tolerance}
              \FOR{row index $i = \alpha \ldots \omega$}
                \IF{$i \neq 1$ AND $i \neq N$  $i = \alpha$ OR $i=\omega$}
                  \STATE \textbf{COPY} neighbor $p_{n-1}$ or $p_{n+1}$ boundary row values $u^{\left(t\right)}(i,j)$ for $u^{\left(t+1\right)}(i,j)$
                \ENDIF
                \STATE Compute $u^{\left(t + 1\right)}(i,j) = 1/4*(u^{\left(t\right)}(i+1,j) + u^{\left(t\right)}(i-1,j) + u^{\left(t\right)}(i,j+1) + u^{\left(t\right)}(i, j-1)$
                \IF{$i \neq 1$ AND $i\neq N$  $i = \alpha$ OR $i = \omega$}
                  \STATE \textbf{UPDATE} own $p_{n}$ boundary row values $u_{j}^{\left(t\right)}(i,j)$ in shared memory with $u_{j}^{\left(t + 1\right)}(i,j)$
                \ENDIF
              \ENDFOR
            \ENDWHILE
          \ENDFOR
        \end{algorithmic}
        \end{algorithm}
      </pre>
      </div>
      <script>
              var alg1 = document.getElementById( "Alg4t" ).textContent;
              pseudocode.render( alg1, document.getElementById( "Alg4" ), {
                lineNumber: true
              } );
            </script>
      <p>
        <a href="#Fig9">Figure 9</a> provides histograms and kernel fits for
        each of the thread counts. <a href="#Tab3">Table 3</a> and <a
          href="#Fig9">Figure 9</a> show that with increasing thread count, mean
        iteration time decreases, but iteration times variance increases. This
        increase in iteration time variation may result from increased
        opportunities for lock collisions with greater thread counts.
      </p>
      <table id="Tab3">
        <thead>
          <tr>
            <th>Threads</th>
            <th>Mean<br /> <math
                xmlns="http://www.w3.org/1998/Math/MathML">
            <mfenced open="(" close=")">
            <mrow>
            <msup>
            <mn>10</mn>
            <mn>-5</mn>
            </msup>
            <mi>s</mi>
            </mrow>
            </mfenced>
            </math>
            </th>
            <th>Std.<br /> <math
                xmlns="http://www.w3.org/1998/Math/MathML">
            <mfenced open="(" close=")">
            <mrow>
            <msup>
            <mn>10</mn>
            <mn>-6</mn>
            </msup>
            <mi>s</mi>
            </mrow>
            </mfenced>
            </math>
            </th>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td>10</td>
            <td>8.86</td>
            <td>3.87</td>
          </tr>
          <tr>
            <td>25</td>
            <td>3.92</td>
            <td>2.08</td>
          </tr>
          <tr>
            <td>50</td>
            <td>2.55</td>
            <td>2.34</td>
          </tr>
          <tr>
            <td>75</td>
            <td>2.53</td>
            <td>5.80</td>
          </tr>
          <tr>
            <td>100</td>
            <td>2.61</td>
            <td>5.95</td>
          </tr>
          <tr>
            <td>150</td>
            <td>2.64</td>
            <td>5.76</td>
          </tr>
        </tbody>
        <caption>
          <strong>Table 3.</strong> Mean iteration time and standard deviation
          by thread count
        </caption>
      </table>
	  
      <table id="Fig9">
       <tr>
        <td><figure id="Fig9a">
          <img src="./img/image9a.png" />
          <figcaption>
            <strong>(a)</strong> 10 threads
          </figcaption>
        </figure></td>
        <td><figure id="Fig9b">
          <img src="./img/image9b.png" />
          <figcaption>
            <strong>(b)</strong> 25 threads
          </figcaption>
        </figure></td>
        <td><figure id="Fig9c">
          <img src="./img/image9c.png" />
          <figcaption>
            <strong>(c)</strong> 50 threads
          </figcaption>
        </figure></td>
       </tr>
       <tr>
        <td><figure id="Fig9d">
          <img src="./img/image9d.png" />
          <figcaption>
            <strong>(d)</strong> 75 threads
          </figcaption>
        </figure></td>
        <td><figure id="Fig9e">
          <img src="./img/image9e.png" />
          <figcaption>
            <strong>(e)</strong> 100 threads
          </figcaption>
        </figure></td>
        <td><figure id="Fig9f">
          <img src="./img/image9f.png" />
          <figcaption>
            <strong>(f)</strong> 150 threads
          </figcaption>
        </figure></td>
       <tr>
        <caption>
          <strong>Figure 9.</strong> Iteration time histograms with kernel fits
        </caption>
      </table>
      <p>
        <a href="#Fig9">Figure 9</a> provides histograms and kernel fits for
        each of the thread counts. <a href="#Tab3">Table 3</a> and <a
          href="#Fig9">Figure 9</a> show that with increasing thread count, mean
        iteration time decreases, but iteration times variance increases. This
        increase in iteration time variation may result from increased
        opportunities for lock collisions with greater thread counts.
      </p>
      <p>
        Since this implementation is even more compute bound than the first one,
        <a href="#Tab3">Table 3</a> shows a general decrease in the time for
        each iteration as the thread count is increased. While there is no
        inflection point evident in the data presented in <a href="#Tab3">Table
          3</a>, compared to <span class="small-caps">Race</span> in <a href="#Tab1">Table
          1</a>, <a href="#Tab3">Table 3</a> still suggests that once the number of
        threads outnumbers physical cores, performance gains diminish. For
        denser matrices, or for different applications on different systems,
        these trends could change as the memory-based activities become
        relatively more expensive. The finite difference discretization of the
        Laplacian is a very sparse matrix that does not require much data
        movement.
      </p>
    </section>

    <section id="Sect5.3">
      <h1>Implementation comparison</h1>
      <p>
        The <span class="small-caps">Safe</span> variant of the first
        implementation incurs significant overhead costs for
        <math xmlns="http://www.w3.org/1998/Math/MathML">
        <mi>x</mi>
        </math>
        copy and update operations, as thread count increases, because each
        thread must copy the entire
        <math xmlns="http://www.w3.org/1998/Math/MathML">
        <mi>x</mi>
        </math>
        vector. In the second implementation, data shared between threads is
        differentiated and specific to domain location; therefore, specific
        locks may be used when copying and updating segments of the subdomain.
        Assuming an appropriate number of processing elements for a given grid,
        i.e., a thread has significantly more middle rows than boundary rows,
        copy operations, and the associated variability and costs, are minimal
        compared with compute operations. The <span class="small-caps">Race</span>
        implementation of the general solver eliminates much of the overhead
        cost from mutex locks, and convergence time is satisfactory for the
        given system. Implementation 2 is more constrained than Implementation
        1, generalizing only to finite difference discretizations of partial
        differential equations over rectangular grids. Implementation 1
        generalizes further to any sparse matrix,
        <math xmlns="http://www.w3.org/1998/Math/MathML">
        <mi>A</mi>
        <mo>,</mo>
        </math>
        with which the Jacobi method can be used. According to <a href="#Theo1">Theorem
          1</a>, convergence will occur if the spectral radius of the iteration
        matrix,
        <math xmlns="http://www.w3.org/1998/Math/MathML">
        <mi>C</mi>
        <mo>,</mo>
        </math>
        is less than 1. In the case of the Jacobi method, the iteration matrix
        is given by
      </p>
      <p>$$ C = -D^{-1}\left( L + U \right)\label{u}\tag{21} $$</p>
      <p>
        Note that in the two dimensional discretization of the Laplacian, the
        spectral radius of the Jacobian is less than l, which says that both the
        synchronous and asynchronous variants of the Jacobi algorithm will
        converge. Note <span class="small-caps">Race</span> behavior is unknown
        for different problems and HPC systems.
      </p>
      <p>The purpose of the two distinct implementations is to emphasize
        that the simulation framework proposed here can adapt to the behavior of
        different problems and platforms. The framework may be apdapted to any
        asynchronous iterative method through the process of collecting data
        representative of individual update times and using the resultant data
        to model the system in the framework.</p>
    </section>

    <section id="Sect5.4">
      <h1>Framework validation</h1>
      <p>
        To validate the performance of the simulation framework when initialized
        with appropriate distributions, a case study utilizing output from
        Implementation 1 (see <a href="#Sect5.1">Section 5.1</a> for details)
        was considered. Data was collected for a smaller problem size only in
        order to facilitate the collection of data over a large number of runs.
        Specifically, the Laplacian was discretized over a
        <math xmlns="http://www.w3.org/1998/Math/MathML">
        <mn>20</mn>
        <mo>&times;</mo>
        <mn>20</mn>
        </math>
        grid resulting in a matrix of size
        <math xmlns="http://www.w3.org/1998/Math/MathML">
        <mn>400</mn>
        <mo>&times;</mo>
        <mn>400</mn>
        <mo>.</mo>
        </math>
        Similarly to the process in <a href="#Sect5.1">Section 5.1</a>,
        distributions were fit to the output of the OpenMP<sup>&reg;</sup>
        implementation, and these distributions were used in the simulation
        framework to provide update times to the simulated processors that are
        reflective of the HPC hardware that the data was collected on. Output
        from the average of these runs is provided in <a href="#Tab4">Table
          4</a>. The leftmost column provides the number of threads that were used
        (or simulated), the middle column shows the average over multiple runs
        of the parallel implementation, and the rightmost column shows the
        average over multiple runs of the simulation generated by the simulation
        framework. In the case of this small problem, the similarity of actual
        and simulated run times helps to validate the model. Running multiple
        trials of larger problems in the framework is currently
        time-prohibitive, which is an issue that may be improved with framework
        implementation changes. Future work includes model validation for other
        problems and larger problems.
      </p>
      <table id="Tab4">
        <thead>
          <tr>
            <th>Thread Count</th>
            <th>Run Average (s)</th>
            <th>Simulation Average (s)</th>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td>11</td>
            <td>0.01</td>
            <td>0.01</td>
          </tr>
          <tr>
            <td>21</td>
            <td>0.02</td>
            <td>0.02</td>
          </tr>
          <tr>
            <td>41</td>
            <td>0.04</td>
            <td>0.04</td>
          </tr>
          <tr>
            <td>51</td>
            <td>0.04</td>
            <td>0.05</td>
          </tr>
          <tr>
            <td>81</td>
            <td>0.09</td>
            <td>0.09</td>
          </tr>
          <tr>
            <td>101</td>
            <td>0.12</td>
            <td>0.12</td>
          </tr>
          <tr>
            <td>201</td>
            <td>0.34</td>
            <td>0.35</td>
          </tr>
        </tbody>
        <caption>
          <strong>Table 4.</strong> Comparisons of run times between parallel
          executions and simulation
        </caption>
      </table>
    </section>
  </section>

  <section id="Sect6">
    <h1>Framework extension for fault-tolerance requirements</h1>
    <p>
      The modular nature of this framework allows for extra functionality to be
      easily added to the framework itself that can be used to adapt the base
      algorithm to suit a specific set of requirements. With the projected
      increase of faults (see the references in <a href="#Sect2">Section 2</a>),
      development of fault tolerant algorithms is an important endeavor. A block
      diagram showing the additional functionality dealing with fault-tolerance
      is shown in <a href="#Fig10">Figure 10</a>. The new functionality is
      achieved by passing in another function handle that performs the fault
      tolerance check and recovery work.
    </p>
    <figure id="Fig10">
      <img src="./img/image10.png" />
      <caption>
        <strong>Figure 10.</strong> Block diagram of the simulation framework
        with added support for fault tolerance mechanisms
      </caption>
    </figure>
    <p>
      The contents of the newly added <em>Fault tolerance</em> check module may
      be organized as follows: Each processor makes a call to find the global
      residual and rolls the state back to the previous known good state if the
      behavior of the residual is not as expected. See <a href="#Sect6.2">Section
        6.2</a> for more details. Note that this strategy is not being advocated for
      due to its optimality, but is being shown as an example of how to extend
      the framework for algorithm development. Techniques such as monitoring the
      progression of the component-wise residuals (e.g., (<a
        role="doc-biblioref" href="#ADQO15">Anzt, Dongarra, &amp;
        Quintana-Ortí, 2015</a>; <a role="doc-biblioref" href="#ADQO16">Anzt,
        Dongarra, &amp; Quintana-Ortí, 2016</a>)) or only rolling back portions of
      the state vector (e.g., (<a role="doc-biblioref" href="#CS17">Coleman,
        &amp; Sosonkina, 2017</a>; <a role="doc-biblioref" href="#CS18">Coleman,
        &amp; Sosonkina, 2018</a>)) would probably be more computationally
      efficient.
    </p>

    <section id="Sect6.1">
      <h1>Fault model</h1>
      <p>
        For this part of the study, faults are modeled as perturbations similar
        to several recent studies (<a role="doc-biblioref" href="#CS16b">Coleman,
          &amp; Sosonkina, 2016b</a>; <a role="doc-biblioref" href="#CSC17">Coleman,
          Sosonkina, &amp; Chow, 2017</a>; <a role="doc-biblioref" href="#SW15">Stoyanov,
          &amp; Webster, 2015</a>); the goal being producing fault tolerant
        algorithms for future computing platforms that are not too dependent on
        the precise mechanism of a fault (e.g., bit flip). Modifying the
        perturbation-based fault model described in (<a role="doc-biblioref"
          href="#CSC17">Coleman, Sosonkina, &amp; Chow, 2017</a>), a single data
        structure is targeted and a small random perturbation is injected into
        each component transiently. For example, if the targeted data structure
        is a vector
        <math xmlns="http://www.w3.org/1998/Math/MathML">
        <mi>x</mi>
        </math>
        and the maximum size of the perturbation-based fault is
        <math xmlns="http://www.w3.org/1998/Math/MathML">
        <mo>&epsilon;</mo>
        </math>
        then proceed as follows: sample a random number
        <math xmlns="http://www.w3.org/1998/Math/MathML">
        <msub>
        <mi>r</mi>
        <mi>i</mi>
        </msub>
        <mo>&isin;</mo>
        <mfenced open="(" close=")">
        <mo>-&epsilon;</mo>
        <mo>&epsilon;</mo>
        </mfenced>
        <mo>,</mo>
        </math>
        using a uniform distribution, and then set
      </p>
      <p>$$ \hat x_{i} = x_{i} + r_{i}\label{v}\tag{22} $$</p>
      <p>
        for all values of
        <math xmlns="http://www.w3.org/1998/Math/MathML">
      <mi>i</mi>
      <mo>.</mo>
      </math>
        The resultant vector
        <math xmlns="http://www.w3.org/1998/Math/MathML">
      <mo>x&#770;</mo>
      </math>
        is then perturbed away from the original vector
        <math xmlns="http://www.w3.org/1998/Math/MathML">
      <mi>x</mi>
      <mo>.</mo>
      </math>
        Other similar perturbation-based fault models have sampled the
        components
        <math xmlns="http://www.w3.org/1998/Math/MathML">
        <msub>
      <mi>r</mi>
      <mi>i</mi>
      </msub>
      </math>
        from different ranges. This can allow the creation of scenarios where
        some components are perturbed by large amounts, and some are only
        changed incrementally.
      </p>
      <p>In this study, faults are injected into the asynchronous Jacobi
        algorithm following the perturbation based methodology described above.
        Due to the relatively short execution time of the asynchronous Jacobi
        algorithm on the given test problems, a fault is induced only once
        during each run, and the fault is designated to occur at a random
        iteration number before convergence. To be precise - since "iteration"
        loses some meaning in an asynchronous iterative algorithm - the fault is
        injected on a single simulated time before the algorithm terminates. It
        is not necessary for the program to have an update scheduled on the same
        simulated time for the fault to be injected.</p>
    </section>

    <section id="Sect6.2">
      <h1>Experiments with the Fault-tolerance module</h1>
      <p>
        Similar to the earlier results in the paper, this study covers the
        solution of the linear system resulting from a two-dimensional finite
        difference discretization of the Laplacian. Before presenting simulation
        results, it is important to note that faults, as modeled here, will not
        prevent the <em>eventual</em> solution of the linear system using the
        (asynchronous) Jacobi method. Since the spectral radius of the
        associated iteration matrix is strictly less than 1, it will converge
        for any initial guess
        <math xmlns="http://www.w3.org/1998/Math/MathML">
        <msup>
        <mi>x</mi>
        <mrow>
        <mfenced open="(" close=")">
        <mn>0</mn>
        </mfenced>
        </mrow>
        </msup>
        <mo>.</mo>
        </math>
      </p>
      <p>
        Since faults are assumed to only affect the memory storing the vector
        <math xmlns="http://www.w3.org/1998/Math/MathML">
        <mi>x</mi>
        </math>
        and are assumed to occur in a transient manner, if a fault occurs on
        iteration
        <math xmlns="http://www.w3.org/1998/Math/MathML">
        <mi>F</mi>
        </math>
        then the subsequent iterate,
        <math xmlns="http://www.w3.org/1998/Math/MathML">
        <msup>
        <mi>x</mi>
        <mrow>
        <mfenced open="(" close=")">
        <mrow>
        <mi>F</mi>
        <mo>+</mo>
        <mn>1</mn>
        </mrow>
        </mfenced>
        </mrow>
        </msup>
        </math>
        can be taken to be the new starting iterate and eventual convergence is
        guaranteed due to the iteration matrix which has remained the same
        throughout the occurrence of the fault. This model can reflect the
        scenario where certain parts of the routine are designated to run on
        hardware with a higher reliability threshold, and other parts of the
        algorithm are allowed to run on hardware that may be more susceptible to
        the occurrence of a fault. This sandbox type design has been suggested
        as a possible means for providing energy efficient fault tolerance on
        future HPC environments (<a role="doc-biblioref" href="#BFHH12">Bridges,
          et al., 2012</a>; <a role="doc-biblioref" href="#HH11">Hoemmen, &amp;
          Heroux, 2011</a>; <a role="doc-biblioref" href="#SV13">Sao, &amp;
          Vuduc, 2013</a>).
      </p>
      <p>
        While eventual convergence may be guaranteed, greatly accelerated
        convergence is possible through a simple checkpointing scheme. An
        example of such a scheme (as an extension of the asynchronous Jacobi
        simulation provided by <a href="#Alg2">Algorithm 2</a>) is provided in <a
          href="#Alg5">Algorithm 5</a>.
      </p>
      <div id="Alg5">
        <pre id="Alg5t" style="display: none">
        \begin{algorithm}
        \caption{Asynchronous Jacobi simulation with checkpointing}
        \begin{algorithmic}
          \STATE \textbf{Input:} $a_{ij} \in A$; initial guess $x_{0}$; number of processing elements $p$;  input random number distribution;
                                 checkpointing tolerance $\alpha$;
                                 checkpointing frequency $\omega$
          \STATE \textbf{Input:} Solution vector $x$
          
          \STATE Assign processor update times $\tau_{1}, \tau_{2}, \ldots, \tau_{p}$  by sampling from an appropriate random number distribution

          \STATE Assign a part of $x$ to each processing element
          
          \STATE Initialize $r_{old}$ to a large value
          
          \FOR{$t = 1, 2, \ldots$, until convergence}
            \FOR{each processing element, $P_{l}$}
              \IF{$\tau_{k} = t$}
                \FOR{each element $x_{i} \in x$ assigned to $P_{l}$}
                  \STATE $x_i = \frac{-1}{a_{ii}}\left[\sum_{j\neq i} a_{ij}x_j - b_i \right]$
                \ENDFOR
                \STATE Retrieve a new update time $\tau_{k}$ by sampling from the input distribution
              \ENDIF
            \ENDFOR
            \STATE Inject a fault if appropriate
            \STATE Calculate the residual $r_{new}$ as in Eq. (18)
            
            \IF{$r_{new} > \alpha \times r_{old}$}
              \STATE $x \leftarrow x_{cp}$
            \ENDIF
            
            \IF{$\mod(t, \omega) == 0$}
              \STATE $x_{cp} \leftarrow x$
            \ENDIF
            
            \STATE Check termination conditions
          \ENDFOR
        \end{algorithmic}
        \end{algorithm}
      </pre>
      </div>
      <script>
              var alg1 = document.getElementById( "Alg5t" ).textContent;
              pseudocode.render( alg1, document.getElementById( "Alg5" ), {
                lineNumber: true
              } );
            </script>
      <p>
        Note that the asynchronous nature of the iterative method means that a
        strict check on the decrease of the residual (i.e., expecting monotonic
        decrease) is not possible. In particular, the checkpointing tolerance
        <math xmlns="http://www.w3.org/1998/Math/MathML">
        <mi>&alpha;</mi>
        </math>
        needs to be taken such that
        <math xmlns="http://www.w3.org/1998/Math/MathML">
        <mi>&alpha;</mi>
        <mo>&gt;</mo>
        <mn>1</mn>
        <mo>.</mo>
        </math>
        However, the expected manifestation of faults as rare, transient events
        allows α to be taken fairly large. Taking
        <math xmlns="http://www.w3.org/1998/Math/MathML">
        <mi>&alpha;</mi>
        </math>
        too large results in a fault having a substantial impact on the
        convergence rate of algorithm since large faults will be allowed to
        impact the algorithm with no correction. Conversely, taking α too small
        causes the algorithm to checkpoint more frequently than needed. Examples
        of the effects of a fault with different values selected for α are given
        by <a href="#Fig11">Figure 11</a>.
      </p>
      <figure id="Fig11">
        <figure id="Fig11a">
          <img src="./img/image11a.png" />
        </figure>
        <figure id="Fig11b">
          <img src="./img/image11b.png" />
        </figure>
        <figure id="Fig11c">
          <img src="./img/image11c.png" />
        </figure>
        <figure id="Fig11d">
          <img src="./img/image11d.png" />
        </figure>
        <caption>
          <strong>Figure 11.</strong> Effect of differing values of α on the
          progression of the residual
        </caption>
      </figure>
      <p>
        Note in <a href="#Fig11">Figure 11</a> that no checkpointing results in
        a delay to convergence relative to the use of checkpointing with either
        <math xmlns="http://www.w3.org/1998/Math/MathML">
        <mi>&alpha;</mi>
        <mo>=</mo>
        <mn>1</mn>
        </math>
        or
        <math xmlns="http://www.w3.org/1998/Math/MathML">
        <mi>&alpha;</mi>
        <mo>=</mo>
        <mn>10</mn>
        <mo>.</mo>
        </math>
        The size of the fault selected in this study,
        <math xmlns="http://www.w3.org/1998/Math/MathML">
        <msub>
        <mi>r</mi>
        <mi>i</mi>
        </msub>
        <mo>&isin;</mo>
        <mfenced open="(" close=")">
        <mn>-100</mn>
        <mn>100</mn>
        </mfenced>
        <mo>,</mo>
        </math>
        which may be reflective of an exponent or sign bit flip (<a
          role="doc-biblioref" href="#CS18">Coleman, &amp; Sosonkina, 2018</a>),
        results in the values
        <math xmlns="http://www.w3.org/1998/Math/MathML">
        <mi>&alpha;</mi>
        <mo>=</mo>
        <mn>1</mn>
        </math>
        and
        <math xmlns="http://www.w3.org/1998/Math/MathML">
        <mi>&alpha;</mi>
        <mo>=</mo>
        <mn>10</mn>
        </math>
        having the same performance since the error induced by the fault is
        sufficiently large that the new residual is more than
        <math xmlns="http://www.w3.org/1998/Math/MathML">
        <mi>&alpha;</mi>
        <mo>=</mo>
        <mn>10</mn>
        </math>
        times the prior residual. Faults that induce a smaller error may be
        detected by certain values of
        <math xmlns="http://www.w3.org/1998/Math/MathML">
        <mi>&alpha;</mi>
        </math>
        and not by others which would lead to differing performance.
      </p>
      <p>
        The residual progress in the plot showing the effects of using
        <math xmlns="http://www.w3.org/1998/Math/MathML">
        <mi>&alpha;</mi>
        <mo>=</mo>
        <mn>1</mn>
        </math>
        can be explained by the updates provided by certain simulated processing
        elements being rejected despite being necessary for the convergence of
        the algorithm. This can be seen in the small, momentary jumps in the
        progression of the residual visible in the other graphs. These
        rejections lead to stagnation in the progression of the algorithm and
        show why the value of
        <math xmlns="http://www.w3.org/1998/Math/MathML">
        <mi>&alpha;</mi>
        <mo>=</mo>
        <mn>1</mn>
        </math>
        should not be selected for a checkpointing scheme for an asynchronous
        iterative method.
      </p>
    </section>
  </section>

  <section id="Sect7">
    <h1>Conclusions and Future Work</h1>
    <p>This work has developed a framework that can be used to efficiently
      simulate the outcomes of asynchronous methods for future High Performance
      Computing environments. Given that asynchronous methods are notoriously
      difficult to study theoretically, their simulation is an invaluable tool
      for observing behavior and making quantitative and qualitative assertions.
      The modular and extensible nature of the framework proposed here allows
      for easy experimentation with modifications to a popular class of
      algorithms that finds uses in many areas of science and engineering.</p>
    <p>
      The work presented was designed to show the ability of the framework to
      adapt to new algorithm variants, such as those capable of handling
      algorithm recovery in the presence of transient soft faults as was shown
      by example in <a href="#Sect6.2">Section 6.2</a>.
    </p>
    <p>The simulation framework presented here is extensible and flexible
      and is able to:</p>
    <ol>
      <li>admit a variety of asynchronous methods (i.e., beyond the simple
        Jacobi algorithm)</li>
      <li>incorporate different fault models and recovery techniques for
        the development of fault tolerant algorithms, and</li>
      <li>vary hardware parameters such as thread and processor counts and
        the performance of those parameters as governed by the timing
        distributions that are supplied.</li>
    </ol>
    <p>
      In the future, the most obvious extension of the simulation framework is
      to add modules that allow it to be accurately used for experiments on
      either distributed or cloud-based computing environments. Additionally, it
      is planned to add features for optimization that could allow for the
      automation of the selection of the checkpointing tolerance as well as
      checkpointing frequency in the course of simulation. Adding the capability
      for the framework to take a range of parameters and find optimal values
      without direct input from the user could aid in the development of
      algorithms. Furthermore, the simulation framework is intended to be
      augmented with runtime simulation measurements, such those provided by
      Intel<sup>&reg;</sup> Running Average Power Limit (RAPL) interface (<a
        role="doc-biblioref" href="#Int16">Intel, 2016</a>), to obtain simulated
      application execution traces in order to model application performance and
      energy consumption.
    </p>
  </section>
  <section role="doc-acknowledgements">
    <h1>Acknowledgements</h1>
    <p>
      This work was supported in part by the Air Force Office of Scientific
      Research under the AFOSR award FA9550-12-1-0476, by the U.S. Department of
      Energy (DOE) Office of Advanced Scientific Computing Research under the
      grant DE-SC-0016564 and the Exascale Computing Project (ECP) through the
      Ames Laboratory, operated by Iowa State University under contract No.
      DE-AC00-07CH11358, by the U.S. Department of Defense High Performance
      Computing Modernization Program, through a HASI grant, the High
      Performance Computing facilities at Old Dominion University, and through
      the ILIR/IAR program at the Naval Surface Warfare Center - Dahlgren
      Division. Edmond Chow provided the MATLAB<sup>&reg;</sup> script that
      evolved into part of the proposed simulation framework.
    </p>
  </section>
  <section class="suppressInPDF" role="doc-endnotes">
    <h1>Footnotes</h1>
    <ol>
      <li id="ftn1" role="doc-endnote">Throughout the text, vector notation
        is occasionally adopted to emphasize when functions take all components
        of <math xmlns="http://www.w3.org/1998/Math/MathML">
<mi>x</mi>
</math> as opposed to a single component, such as <math
          xmlns="http://www.w3.org/1998/Math/MathML">
<msub>
<mi>x</mi>
<mn>1</mn>
</msub>
</math>.<sup><a role="doc-backlink" href="#fn_pointer_ftn1">[back]</a></sup>
      </li>
      <li id="ftn2" role="doc-endnote">Rulfo is a part of computing
        resources of the Department of Modeling, Simulation and Visualization
        Engineering at Old Dominion University.<sup><a role="doc-backlink"
          href="#fn_pointer_ftn2">[back]</a></sup>
      </li>
    </ol>
  </section>
  <section role="doc-bibliography">
    <h1>Bibliography</h1>
    <ul>
      <li id="AB05" role="doc-biblioentry">Addou, A., &amp; Benahmed, A.
        (2005). Parallel synchronous algorithm for nonlinear fixed point
        problems. <em>International Journal of Mathematics and Mathematical
          Sciences</em>. 2005(19):3175-3183. <a
        href="https://dx.doi.org/10.1155/IJMMS.2005.3175"><img
          src="../../icons/doi.png" /></a>
      </li>
      <li id="Anz12" role="doc-biblioentry">Anzt, H. (2012). <em>Asynchronous
          and multiprecision linear solvers: Scalable and fault-tolerant
          numerics for energy efficient high performance computing</em> (Doctoral
        dissertation, Karlsruher Institut für Technologie, Karlsruhe, Germany).
        <a href="https://d-nb.info/1029764689/34"><img
          src="../../icons/html.png" /></a>
      </li>
      <li id="ACD15" role="doc-biblioentry">Anzt, H., Chow, E., &amp;
        Dongarra, J. (2015). Iterative sparse triangular solves for
        preconditioning. In Träff, J., Hunold, S., &amp; Versaci, F. (eds) <em>Euro-Par
          2015: Parallel Processing. Euro-Par 2015</em> (pp. 650-661). Lecture Notes
        in Computer Science, vol 9233. Berlin, Heidelberg: Springer. <a
        href="https://dx.doi.org/10.1007/978-3-662-48096-0_50"><img
          src="../../icons/doi.png" /></a>
      </li>
      <li id="ADQO15" role="doc-biblioentry">Anzt, H., Dongarra, J., &amp;
        Quintana-Ortí, E.S. (2015). Tuning stationary iterative solvers for
        fault resilience. In <em>Proceedings of the 6th Workshop on Latest
          Advances in Scalable Algorithms for Large-Scale Systems</em> (pp. 1:1-1:8).
        New York, NY: ACM. <a href="https://dx.doi.org/10.1145/2832080.2832081"><img
          src="../../icons/doi.png" /></a>
      </li>
      <li id="ADQO16" role="doc-biblioentry">Anzt, H., Dongarra, J., &amp;
        Quintana-Ortí, E.S. (2016). Fine-grained bit-flip protection for
        relaxation methods. <em>Journal of Computational Science</em>. <a
        href="https://dx.doi.org/10.1016/j.jocs.2016.11.013"><img
          src="../../icons/doi.png" /></a>
      </li>
      <li id="ALDH12" role="doc-biblioentry">Anzt, H., Luszczek, P.,
        Dongarra, J., &amp; Heuveline, V. (2012). GPU-accelerated asynchronous
        error correction for mixed precision iterative refinement. In
        Kaklamanis, C., Papatheodorou, T., &amp; Spirakis, P.G. (eds) <em>Euro-Par
          2012 Parallel Processing</em> (pp. 908-919). Lecture Notes in Computer
        Science, vol 7484. Berlin, Heidelberg: Springer. <a
        href="https://dx.doi.org/10.1007/978-3-642-32820-6_89"><img
          src="../../icons/doi.png" /></a>
      </li>
      <li id="ABC+06" role="doc-biblioentry">Asanović, K., Bodik, R.,
        Catanzaro, B.C., Gebis, J.J., Husbands, P., Keutzer, K., Patterson, D.
        A., Plishker, W.L., Shalf, J., Williams, S.W., &amp; Yelick, K.A.
        (2006). <em>The landscape of parallel computing research: A view
          from Berkeley</em> (Report No. UCB/EECS-2006-183). Berkeley, CA: EECS
        Department, University of California. <a
        href="http://www.eecs.berkeley.edu/Pubs/TechRpts/2006/EECS-2006-183.html"><img
          src="../../icons/html.png" /></a>
      </li>
      <li id="ABC+10" role="doc-biblioentry">Ashby, S., Beckman, P., Chen,
        J., Colella, P., Collins, B., Crawford, D., Dongarra, J., Kothe, D.,
        Lusk, R., Messina, P., Mezzacappa, T., Moin, P., Norman, M. Rosner, R.,
        Sarkar, V., Siegel, A., Streitz, F., White, A., &amp; Wright, M. (2010).
        <em>The opportunities and challenges of exascale computing: Summary
          report of the advanced scientific computing advisory committee (ASCAC)
          subcommittee.</em> (Report No. Fall 2010). Washington, D.C.: U.S.
        Department of Energy Office of Science. <a
        href="https://science.energy.gov/~/media/ascr/ascac/pdf/reports/Exascale_subcommittee_report.pdf"><img
          src="../../icons/pdf.png" /></a>
      </li>
      <li id="ADG15" role="doc-biblioentry">Avron, H., Druinsky, A. &amp;
        Gupta, A. (2015). Revisiting asynchronous linear solvers: Provable
        convergence rate through randomization. <em>Journal of the ACM</em>,
        62(6):51:1-51:27. <a href="https://dx.doi.org/10.1145/2814566"><img
          src="../../icons/doi.png" /></a>
      </li>
      <li id="Bau78" role="doc-biblioentry">Baudet, G.M. (1978).
        Asynchronous iterative methods for multiprocessors. <em>Journal of
          the ACM</em>, 25(2):226-244. <a
        href="https://dx.doi.org/10.1145/322063.322067"><img
          src="../../icons/doi.png" /></a>
      </li>
      <li id="Ben07" role="doc-biblioentry">Benahmed, A. (2007). A
        convergence result for asynchronous algorithms and applications. <em>Proyecciones
          (Antofagasta)</em>, 26(2):219-236. <a
        href="https://scielo.conicyt.cl/scielo.php?script=sci_arttext&pid=S0716-09172007000200005&lng=pt&nrm=iso&tlng=en"><img
          src="../../icons/html.png" /></a>
      </li>
      <li id="BT89" role="doc-biblioentry">Bertsekas, D.P., &amp;
        Tsitsiklis, J.N. (1989). <em>Parallel and distributed computation:
          numerical methods</em>. Englewood Cliffs, NJ: Prentice-Hall.
      </li>
      <li id="BBDH11" role="doc-biblioentry">Bethune, I., Bull, J.M.,
        Dingle, N.J., &amp; Higham, N.J. (2011). <em>Investigating the
          performance of asynchronous Jacobi's method for solving systems of
          linear equations</em> (Report No. 2011.82). Manchester: The University of
        Manchester. <a href="http://eprints.maths.manchester.ac.uk/1684/"><img
          src="../../icons/html.png" /></a> <a
        href="http://eprints.maths.manchester.ac.uk/1684/1/paper.pdf"><img
          src="../../icons/pdf.png" /></a>
      </li>
      <li id="BBDH14" role="doc-biblioentry">Bethune, I., Bull, J.M.,
        Dingle, N.J., &amp; Higham, N.J. (2014). Performance analysis of
        asynchronous Jacobi's method implemented in MPI, SHMEM and OpenMP. <em>International
          Journal of High Performance Computing Applications</em>, 28(1):97-111. <a
        href="https://dx.doi.org/10.1177/1094342013493123"><img
          src="../../icons/doi.png" /></a>
      </li>
      <li id="BPC+11" role="doc-biblioentry">Boyd, S., Parikh, N., Chu, E.,
        Peleato, B., &amp; Eckstein, J. (2011). Distributed optimization and
        statistical learning via the alternating direction method of
        multipliers. <em>Foundations and Trends in Machine Learning</em>,
        3(1):1-122. <a
        href="https://web.stanford.edu/~boyd/papers/admm_distr_stats.html"><img
          src="../../icons/html.png" /></a>
      </li>
      <li id="BFHH12" role="doc-biblioentry">Bridges, P.G., Ferreira, K.
        B., Heroux, M.A., &amp; Hoemmen, M. (2012). Fault-tolerant linear
        solvers via selective reliability. <em>arXiv preprint
          arXiv:1206.1390</em>. <a href="https://arxiv.org/abs/1206.1390"><img
          src="../../icons/html.png" /></a>
      </li>
      <li id="BM02" role="doc-biblioentry">Buyya, R. &amp; Murshed, M.
        (2002). Gridsim: A toolkit for the modeling and simulation of
        distributed resource management and scheduling for grid computing. <em>Concurrency
          and Computation: Practice and Experience</em>, 14(13-15):1175-1220. <a
        href="https://dx.doi.org/10.1002/cpe.710"><img
          src="../../icons/doi.png" /></a>
      </li>
      <li id="CRB+11" role="doc-biblioentry">Calheiros, R.N., Ranjan, R.,
        Beloglazov, A., De Rose, C.A.F., &amp; Buyya, R. (2011). CloudSim: A
        toolkit for modeling and simulation of cloud computing environments and
        evaluation of resource provisioning algorithms. <em>Software:
          Practice and Experience</em>, 41(1):23-50. <a
        href="https://dx.doi.org/10.1002/spe.995"><img
          src="../../icons/doi.png" /></a>
      </li>
      <li id="CRDRB09" role="doc-biblioentry">Calheiros, R.N., Ranjan, R.,
        De Rose, C.A.F., &amp; Buyya, R. (2009). Cloudsim: A novel framework for
        modeling and simulation of cloud computing infrastructures and services.
        <em>arXiv preprint arXiv:0903.2525</em>. <a
        href="https://arxiv.org/abs/0903.2525"><img
          src="../../icons/html.png" /></a>
      </li>
      <li id="CGG+09" role="doc-biblioentry">Cappello, F., Geist, A.,
        Gropp, B., Kale, L., Kramer, B., &amp; Snir, M. (2009). Toward exascale
        resilience. <em>International Journal of High Performance Computing
          Applications</em>, 23(4):374-388. <a
        href="https://dx.doi.org/10.1177/1094342009347767"><img
          src="../../icons/doi.png" /></a>
      </li>
      <li id="CGG+14" role="doc-biblioentry">Cappello, F., Geist, A.,
        Gropp, B., Kale, L., Kramer, B., &amp; Snir, M. (2014). Toward exascale
        resilience: 2014 update. <em>Supercomputing Frontiers and
          Innovations</em>, 1(1). <a href="https://dx.doi.org/10.14529/jsfi140101"><img
          src="../../icons/doi.png" /></a>
      </li>
      <li id="Cas01" role="doc-biblioentry">Casanova, H. (2001). Simgrid: A
        toolkit for the simulation of application scheduling. In <em>Proceedings
          of the First IEEE/ACM International Symposium on Cluster Computing and
          the Grid</em> (pp. 430-437). Piscataway, NJ: IEEE Press. <a
        href="https://dx.doi.org/10.1109/CCGRID.2001.923223"><img
          src="../../icons/doi.png" /></a>
      </li>
      <li id="CLQ08" role="doc-biblioentry">Casanova, H., Legrand, A.,
        &amp; Quinson, M. (2008). Simgrid: A generic framework for large-scale
        distributed experiments. In <em>Proceedings of the Tenth
          International Conference on Computer Modeling and Simulation (UKSIM
          2008)</em> (pp. 126-131). Piscataway, NJ: IEEE Press. <a
        href="https://dx.doi.org/10.1109/UKSIM.2008.28"><img
          src="../../icons/doi.png" /></a>
      </li>
      <li id="CM69" role="doc-biblioentry">Chazan, D., &amp; Miranker, W.
        (2014). Chaotic relaxation. <em>Linear Algebra and its Applications</em>,
        2(2):199-222. <a href="https://dx.doi.org/10.1016/0024-3795(69)90028-7"><img
          src="../../icons/doi.png" /></a>
      </li>
      <li id="CC16" role="doc-biblioentry">Cheung, Y.K. &amp; Cole, R.
        (2016). A unified approach to analyzing asynchronous coordinate descent
        and tatonnement. <em>arXiv preprint arXiv:1612.09171</em>. <a
        href="https://arxiv.org/abs/1612.09171"><img
          src="../../icons/html.png" /></a>
      </li>
      <li id="CAD15" role="doc-biblioentry">Chow, E., Anzt, H., &amp;
        Dongarra, J. (2015). Asynchronous iterative algorithm for computing
        incomplete factorizations on GPUs. In Kunkel, J., &amp; Ludwig, T. (eds)
        <em>High Performance Computing. ISC High Performance 2015</em>. Lecture
        Notes in Computer Science, vol 9137. Cham: Springer. <a
        href="https://dx.doi.org/10.1007/978-3-319-20119-1_1"><img
          src="../../icons/doi.png" /></a>
      </li>
      <li id="CP15" role="doc-biblioentry">Chow, E., &amp; Patel, A.
        (2015). Fine-grained parallel incomplete LU factorization. <em>SIAM
          Journal on Scientific Computing</em>, 37(2):C169-C193. <a
        href="https://dx.doi.org/10.1137/140968896"><img
          src="../../icons/doi.png" /></a>
      </li>
      <li id="CJB+18" role="doc-biblioentry">Coleman, E.C., Jamal, A.,
        Baboulin, M., Khabou, A., &amp; Sosonkina, M. (2018). A comparison of
        soft-fault error models in the parallel preconditioned flexible GMRES.
        In Wyrzykowski, R., Dongarra, J., Deelman, E., Karczewski, K. (eds) <em>Parallel
          Processing and Applied Mathematics. PPAM 2017.</em> (pp. 36-46). Lecture
        Notes in Computer Science, vol 10777. Cham: Springer. <a
        href="https://dx.doi.org/10.1007/978-3-319-78024-5_4"><img
          src="../../icons/doi.png" /></a>
      </li>
      <li id="CS16a" role="doc-biblioentry">Coleman, E.C., &amp; Sosonkina,
        M. (2016a). A comparison and analysis of soft-fault error models using
        FGMRES. In <em>Proceedings of the 6th Annual Virginia Modeling,
          Simulation, and Analysis Center Capstone Conference</em> (pp. 135-142).
        Norfolk, VA: Virginia Modeling, Simulation, and Analysis Center.
      </li>
      <li id="CS16b" role="doc-biblioentry">Coleman, E.C., &amp; Sosonkina,
        M. (2016b). Evaluating a persistent soft fault model on preconditioned
        iterative methods. In <em>Proceedings of the 22nd Annual
          International Conference on Parallel and Distributed Processing
          Techniques and Applications</em> (pp. 98-104). Athens: The Steering
        Committee of The World Congress in Computer Science, Computer
        Engineering and Applied Computing (WorldComp). <a
        href="http://worldcomp-proceedings.com/proc/p2016/PDP3831.pdf"><img
          src="../../icons/pdf.png" /></a>
      </li>
      <li id="CS17" role="doc-biblioentry">Coleman, E.C., &amp; Sosonkina,
        M. (2017). Fault tolerance for fine-grained iterative methods. In <em>Proceedings
          of the 7th Annual Virginia Modeling, Simulation, and Analysis Center
          Capstone Conference</em>. Norfolk, VA: Virginia Modeling, Simulation, and
        Analysis Center.
      </li>
      <li id="CS18" role="doc-biblioentry">Coleman, E.C., &amp; Sosonkina,
        M. (2018). CloudSim: A toolkit for modeling and simulation of cloud
        computing environments and evaluation of resource provisioning
        algorithms. <em>Sustainable Computing: Informatics and Systems</em>, (In
        Press). <a href="https://dx.doi.org/10.1016/j.suscom.2018.01.003"><img
          src="../../icons/doi.png" /></a>
      </li>
      <li id="CSC17" role="doc-biblioentry">Coleman, E.C., Sosonkina, M.,
        &amp; Chow, E. (2017). Fault tolerant variants of the fine-grained
        parallel incomplete LU factorization. In <em>Proceedings of the
          25th High Performance Computing Symposium</em>. San Diego, CA: Society for
        Computer Simulation International. <a
        href="http://scs.org/wp-content/uploads/2017/06/50_Final_Manuscript.pdf"><img
          src="../../icons/pdf.png" /></a>
      </li>
      <li id="DHB+14" role="doc-biblioentry">Dongarra, J., Hittinger, J.,
        Bell, J., Chacon, L., Falgout, R., Heroux, M., Hovland, P., Ng, E.,
        Webster, A., &amp; Wild, S. (2014). <em>Applied mathematics
          research for exascale computing</em> (Report No. LLNL-TR-651000).
        Livermore, CA: Lawrence Livermore National Laboratory. <a
        href="https://dx.doi.org/10.2172/1149042"><img
          src="../../icons/doi.png" /></a>
      </li>
      <li id="DF05" role="doc-biblioentry">Dumitrescu, C.L., &amp; Foster,
        I. (2005). GangSim: A simulator for grid scheduling studies. In <em>CCGrid
          2005. IEEE International Symposium on Cluster Computing and the Grid</em>
        (pp. 1151-1158, vol 2). Piscataway, NJ: IEEE Press. <a
        href="https://dx.doi.org/10.1109/CCGRID.2005.1558689"><img
          src="../../icons/doi.png" /></a>
      </li>
      <li id="EHM15" role="doc-biblioentry">Elliott, J., Hoemmen, M., &amp;
        Mueller, F. (2015). A numerical soft fault model for iterative linear
        solvers. In <em>Proceedings of the 24th International Symposium on
          High-Performance Parallel and Distributed Computing</em> (pp. 271-274). New
        York, NY: ACM. <a href="https://dx.doi.org/10.1145/2749246.2749254"><img
          src="../../icons/doi.png" /></a>
      </li>
      <li id="FS00" role="doc-biblioentry">Frommer, A. &amp; Szyld, D. B.
        (2000). On asynchronous iterations. <em>Journal of Computational
          and Applied Mathematics</em>, 123(1):201-216. <a
        href="https://dx.doi.org/10.1016/S0377-0427(00)00409-X"><img
          src="../../icons/doi.png" /></a>
      </li>
      <li id="Gär99" role="doc-biblioentry">Gärtner, F.C. (1999).
        Fundamentals of fault-tolerant distributed computing in asynchronous
        environments. <em>ACM Computing Surveys</em>, 31(1):1-26. <a
        href="https://dx.doi.org/10.1145/311531.311532"><img
          src="../../icons/doi.png" /></a>
      </li>
      <li id="Gei11" role="doc-biblioentry">Geist, A. (2011). What is the
        monster in the closet?. In <em>Invited Talk at Workshop on
          Architectures I: Exascale and Beyond: Gaps in Research, Gaps in our
          Thinking</em> (vol 2).
      </li>
      <li id="Gei12" role="doc-biblioentry">Geist, A. (2012). Exascale
        monster in the closet. In <em>2012 IEEE Workshop on Silicon Errors
          in Logic-System Effects, Champaign-Urbana, IL, March</em> (pp. 27-28).
        Piscataway, NJ: IEEE Press.
      </li>
      <li id="Gei16" role="doc-biblioentry">Geist, A. (2016).
        Supercomputing's monster in the closet. <em>IEEE Spectrum</em>.
        53(3):30-35. <a href="https://doi.org/10.1109/MSPEC.2016.7420396"><img
          src="../../icons/doi.png" /></a>
      </li>
      <li id="GL09" role="doc-biblioentry">Geist, A., &amp; Lucas, R.
        (2009). Major computer science challenges at exascale. <em>International
          Journal of High Performance Computing Applications</em>. 23(4):427-436. <a
        href="https://doi.org/10.1177%2F1094342009347445"><img
          src="../../icons/doi.png" /></a>
      </li>
      <li id="GBH14" role="doc-biblioentry">Gerstenberger, R., Besta, M.,
        &amp; Hoefler, T. (2014). Enabling highly-scalable remote memory access
        programming with MPI-3 one sided. <em>Scientific Programming</em>,
        22(2):75-91. <a href="https://dx.doi.org/10.3233/SPR-140383"><img
          src="../../icons/doi.png" /></a>
      </li>
      <li id="HW10" role="doc-biblioentry">Hager, G., &amp; Wellein, G.
        (2010). <em>Introduction to high performance computing for
          scientists and engineers</em>. Boca Raton, FL: CRC Press. <a
        href="https://dx.doi.org/10.1109/TCNS.2017.2657460"><img
          src="../../icons/doi.png" /></a>
      </li>
      <li id="HH11" role="doc-biblioentry">Hoemmen, M., &amp; Heroux, M.A.
        (2011). <em>Fault-tolerant iterative methods via selective
          reliability</em>. vol 3. <a
        href="http://www.sandia.gov/%7Emaherou/docs/FTGMRES.pdf"><img
          src="../../icons/pdf.png" /></a>
      </li>
      <li id="Hon17" role="doc-biblioentry">Hong, M. (2017). A distributed,
        asynchronous and incremental algorithm for nonconvex optimization: An
        ADMM approach. <em>IEEE Transactions on Control of Network Systems</em>.
        Piscataway, NJ: IEEE Press. <a
        href="https://dx.doi.org/10.1109/TCNS.2017.2657460"><img
          src="../../icons/doi.png" /></a>
      </li>
      <li id="HD18" role="doc-biblioentry">Hook, J., &amp; Dingle, N.
        (2018). Performance analysis of asynchronous parallel Jacobi. <em>Numerical
          Algorithms</em>. 77(3):831-866. <a
        href="https://dx.doi.org/10.1007/s11075-017-0342-9"><img
          src="../../icons/doi.png" /></a>
      </li>
      <li id="Int16" role="doc-biblioentry">Intel (2016). <em>Intel<sup>&reg;</sup>
          64 and IA-32 architectures developer's manual
      </em>. Volume 3B: System Programming Guide, Part 2. <a
        href="https://www.intel.de/content/www/de/de/architecture-and-technology/64-ia-32-architectures-software-developer-vol-3b-part-2-manual.html"><img
          src="../../icons/html.png" /></a>
      </li>
      <li id="IBCH13" role="doc-biblioentry">Iutzeler, F., Bianchi, P.,
        Ciblat, P., &amp; Hachem, W. (2013). Asynchronous distributed
        optimization using a randomized alternating direction method of
        multipliers. In <em>Decision and Control (CDC), 2013 IEEE 52nd
          Annual Conference on</em> (pp. 3671-3676). Piscataway, NJ: IEEE Press. <a
        href="https://dx.doi.org/10.1109/CDC.2013.6760448"><img
          src="../../icons/doi.png" /></a>
      </li>
      <li id="OR00" role="doc-biblioentry">Ortega, J.M., &amp; Rheinboldt,
        W.C. (2000). <em>Iterative solution of nonlinear equations in
          several variables</em>. Philadelphia, PA: SIAM. <a
        href="https://dx.doi.org/10.1137/1.9780898719468"><img
          src="../../icons/doi.png" /></a>
      </li>
      <li id="Saa03" role="doc-biblioentry">Saad, Y. (2003). <em>Iterative
          methods for sparse linear systems</em>. Philadelphia, PA: SIAM. <a
        href="https://dx.doi.org/10.1137/1.9780898718003"><img
          src="../../icons/doi.png" /></a>
      </li>
      <li id="SV13" role="doc-biblioentry">Sao, P. &amp; Vuduc, R. (2013).
        Self-stabilizing iterative solvers. In <em>Proceedings of the
          Workshop on Latest Advances in Scalable Algorithms for Large-Scale
          Systems</em> (pp. 4:1-4:8). New York, NY: ACM. <a
        href="https://dx.doi.org/10.1145/2530268.2530272"><img
          src="../../icons/doi.png" /></a>
      </li>
      <li id="SWA+14" role="doc-biblioentry">Snir, M., Wisniewski, R.W.,
        Abraham, J.A., Adve, S.V., Bagchi, S., Balaji, P., Belak, J., Bose, P.,
        Cappello, F., Carlson, B., Chien, A.A., Coteus, P., Debardeleben, N.A.,
        Diniz, P.C., Engelmann, C., Erez, M., Fazzari, S., Geist, A., Gupta, R.,
        Johnson, F., Krishnamoorthy, S., Leyffer, S., Liberty, D., Mitra, S.,
        Munson, T., Schreiber, R., Stearley, J., &amp; van Hensbergen, E.
        (2014). Addressing failures in exascale computing. <em>International
          Journal of High Performance Computing Applications</em>. 28(2):129-173. <a
        href="https://dx.doi.org/10.1177/1094342014522573"><img
          src="../../icons/doi.png" /></a>
      </li>
      <li id="SN11" role="doc-biblioentry">Srivastava, K., &amp; Nedic, A.
        (2011). Distributed asynchronous constrained stochastic optimization. <em>IEEE
          Journal of Selected Topics in Signal Processing</em>. 5(4):772-790. <a
        href="https://dx.doi.org/10.1109/JSTSP.2011.2118740"><img
          src="../../icons/doi.png" /></a>
      </li>
      <li id="SW15" role="doc-biblioentry">Stoyanov, M. &amp; Webster, C.
        (2015). Numerical analysis of fixed point algorithms in the presence of
        hardware faults. <em>SIAM Journal on Scientific Computing</em>.
        37(5):C532-C553. <a href="https://dx.doi.org/10.1137/140991406"><img
          src="../../icons/doi.png" /></a>
      </li>
      <li id="Szy98" role="doc-biblioentry">Szyld, D.B. (1998). Different
        models of parallel asynchronous iterations with overlapping blocks. <em>Computational
          and Applied Mathematics</em>. 17:101-115. <a
        href="https://dx.doi.org/10.1137/140991406"><img
          src="../../icons/doi.png" /></a>
      </li>
      <li id="TBA86" role="doc-biblioentry">Tsitsiklis, J., Bertsekas, D.,
        &amp; Athans, M. (1986). Distributed asynchronous deterministic and
        stochastic gradient optimization algorithms. <em>IEEE Transactions
          on Automatic Control</em>. 31(9):803-812. <a
        href="https://dx.doi.org/10.1137/140991406"><img
          src="../../icons/doi.png" /></a>
      </li>
      <li id="VV09" role="doc-biblioentry">Venkatasubramanian, S., &amp;
        Vuduc, R.W. (2009). Tuned and wildly asynchronous stencil kernels for
        hybrid CPU/GPU systems. In <em>Proceedings of the 23rd
          International Conference on Supercomputing</em> (pp. 244-255). New York,
        NY: ACM. <a href="https://dx.doi.org/10.1145/1542275.1542312"><img
          src="../../icons/doi.png" /></a>
      </li>
      <li id="WPC16" role="doc-biblioentry">Wolfson-Pou, J., &amp; Chow, E.
        (2016). Reducing communication in distributed asynchronous iterative
        methods. <em>Procedia Computer Science</em>. 80:1906-1916. <a
        href="https://dx.doi.org/10.1016/j.procs.2016.05.501"><img
          src="../../icons/doi.png" /></a>
      </li>
      <li id="ZC10" role="doc-biblioentry">Zhong, M., &amp; Cassandras, C.
        G. (2010). Distributed asynchronous deterministic and stochastic
        gradient optimization algorithms. <em>IEEE Transactions on
          Automatic Control</em>. 55(12):2735-2750. <a
        href="https://dx.doi.org/10.1109/TAC.2010.2049518"><img
          src="../../icons/doi.png" /></a>
      </li>
    </ul>
  </section>
  </main>
</body>
</html>